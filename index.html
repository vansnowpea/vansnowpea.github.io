<!doctype html>



  


<html class="theme-next muse use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1" />






<meta property="og:type" content="website">
<meta property="og:title" content="A Song of Python and Anaconda">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="A Song of Python and Anaconda">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="A Song of Python and Anaconda">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: 'Author'
    }
  };
</script>




  <link rel="canonical" href="http://yoursite.com/"/>

  <title> A Song of Python and Anaconda </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">A Song of Python and Anaconda</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Startseite
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archiv
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/10/15/维基百科某网页表格的csv保存的分析/" itemprop="url">
                  维基百科某网页表格的csv保存的分析【Python】
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Veröffentlicht am</span>
            <time itemprop="dateCreated" datetime="2016-10-15T12:59:28+08:00" content="2016-10-15">
              2016-10-15
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="维基百科某网页表格的csv保存的分析"><a href="#维基百科某网页表格的csv保存的分析" class="headerlink" title="维基百科某网页表格的csv保存的分析"></a>维基百科某网页表格的csv保存的分析</h1><p>1、在阅读《Python网络数据采集》第五章的时候看到的案例，记录细节分析。</p>
<p>目标网址：<br><a href="http://en.wikipedia.org/wiki/Comparison_of_text_editors" target="_blank" rel="external">http://en.wikipedia.org/wiki/Comparison_of_text_editors</a></p>
<p>中的一个表格，</p>
<p>这是结果图：</p>
<p><img src="http://ocg7i7pt6.bkt.clouddn.com/blog/2016-10-15/131101232.png" alt=""></p>
<p>2、代码：</p>
<pre><code>import csv
from urllib.request import urlopen
from bs4 import BeautifulSoup

html = urlopen(&quot;http://en.wikipedia.org/wiki/Comparison_of_text_editors&quot;)
bsObj = BeautifulSoup(html, &quot;html.parser&quot;)
#The main comparison table is currently the first table on the page
table = bsObj.findAll(&quot;table&quot;,{&quot;class&quot;:&quot;wikitable&quot;})[0]
# print(table)
rows = table.findAll(&quot;tr&quot;)
# print(rows)

csvFile = open(&quot;c:\\van\\editors.csv&quot;, &apos;wt&apos;, newline=&apos;&apos;, encoding=&apos;utf-8&apos;)
writer = csv.writer(csvFile)
try:
    for row in rows:
        csvRow = []
        for cell in row.findAll([&apos;td&apos;, &apos;th&apos;]):
            csvRow.append(cell.get_text())
        writer.writerow(csvRow)
finally:
    csvFile.close()
</code></pre><p>3、上述代码的主要目的，是把对应url的表格的文字信息，提取出来，保存到csv。那么他是怎么一步步做到的？</p>
<p>首先，我们来打开url看下网站的目标内容，如下图是一个色彩鲜艳的表格：</p>
<p><img src="http://ocg7i7pt6.bkt.clouddn.com/blog/2016-10-15/115841809.png" alt=""></p>
<p>其次，我们查看下网页源代码，格式清晰的很，并且代码很长一共7910行的html，可这只是维基百科的一页而已！并且从这么多的html代码中快速的提取出我们需要的数据信息，应该怎么做呢？</p>
<p>一般来说，提取数据有2个比较通用的方法，<br>第一、无视他的源代码，我就查看目标内容的路径，可通过浏览器自带的copy xpath配合lxml提取，或者如果你习惯bs4的话，用类似方法。</p>
<p>第二，根据F12找到目标区域，比如一个表格的所在大的路径，然后由大往小的逐步提取。显然本文使用的是这个方法。当鼠标移动到</p>
<pre><code>&lt;table class=&quot;wikitable sortable jquery-tablesorter&quot; style=&quot;text-align: center; font-size: 85%; width: auto; table-layout: fixed;&quot;&gt;
</code></pre><p>这一行代码的时候，整个目标表格的颜色就变了。如下图，</p>
<p><img src="http://ocg7i7pt6.bkt.clouddn.com/blog/2016-10-15/122808452.png" alt=""></p>
<p>4、现在重点分析下这行代码：</p>
<p>table = bsObj.findAll(“table”,{“class”:”wikitable”})[0]</p>
<p>根据第三条中的分析，已经知道把对应表格中所有含有wikitable的class找出来，那么为什么要这么写？<br>此时，对照下本例python代码中，按照class搜索{“class”:”wikitable”}，实际上得到的是搜索class=”wikitable sortable”，如下的html代码（其中一条）：</p>
<pre><code>&lt;table class=&quot;wikitable sortable&quot; style=&quot;text-align: center; font-size: 85%; width: auto; table-layout: fixed;&quot;&gt;
</code></pre><p>也就是说，bs4的findall是找到了类名带有”wikitable”，就自动把”wikitable sortable”也找出来，但对照lxml的xpath来说，如果class=”wikitable”，则搜索结果为空，要写完整的class=“wikitable sortable“，另外要注意这里有一个大坑，因为F12下的class是”wikitable sortable jquery-tablesorter”,和源代码是不对应的，这会导致python里用xpath找不到内容！</p>
<p>那么为何要在代码的后面加上[0] ? 如果只用下面的代码：</p>
<pre><code>bsObj.findAll(&quot;table&quot;,{&quot;class&quot;:&quot;wikitable&quot;})
</code></pre><p>就本例使用bs4分析来说， 其实得到的是bs4.element.ResultSet ，从字面翻译，可理解成bs4的结果集，不过应该是一个列表， 而要提取里面的内容，就加上[0]，此时从bs4的角度来说，得到了一个bs4.element.Tag ， 从列表的内容提取来说，得到了列表里第一个元素的内容。</p>
<p><img src="http://ocg7i7pt6.bkt.clouddn.com/blog/2016-10-15/121538652.png" alt=""></p>
<p>5、接下来分析这一句：</p>
<pre><code>rows = table.findAll(&quot;tr&quot;)
</code></pre><p>这一行是得到表格中所有按行的内容，这包含了表格头的黑色字体。如下图： （顺便请翻到此贴底部的参考资料，学习下tr，th，td的区别。</p>
<p><img src="http://ocg7i7pt6.bkt.clouddn.com/blog/2016-10-15/125711054.png" alt=""></p>
<p>6、接下来，当开始写入csv的时候，是按行写的，包含了表格头的内容，看下代码：</p>
<pre><code>try:
        for row in rows:
            csvRow = []
            for cell in row.findAll([&apos;td&apos;, &apos;th&apos;]):
                csvRow.append(cell.get_text())
            writer.writerow(csvRow)
</code></pre><p>其中 ‘th’ 是表格头，‘td’是表格内容。</p>
<p>7、相对与上述第二种提取方法，这里详细说下第一种提取方法：</p>
<p>检测表格中黑体的表格头（以name为例）和表格内容（以’acme’为例）：</p>
<p>//*[@id=”mw-content-text”]/table[2]/thead/tr/th[1]</p>
<p>//*[@id=”mw-content-text”]/table[2]/tbody/tr[1]/th/a</p>
<p>分别得到他们的xpath地址，但格式并不统一，出现了thead和tbody。</p>
<p>不过好在表格内容的xpath都是有规律的，<br>千万要注意的是：用xpath提取表格的内容要千万小心，这是因为按照上述路径，测试结果，得到的text()返回值为空，</p>
<p>所以要修正下xpath路径，lxml的解析和网页源代码是有出入的，尤其遇到tboday和thead的时候，经过测试，在很多时候，python要把/thead和/tbody才能显示出内容，但这不是绝对的，因为我也遇到保留tbody才能提取成功的案例。</p>
<p>但还有额外的问题，因为这个表格同时有表头和表内容，而这个案例需要同时提取。而表格还有一个captain = “List of text editors”（表格的标题），也就是说，如果我们要通过直接全部提取整个表格的内容，会多出来captain的内容.</p>
<p>而如果把表头和表内容分开提取的话，他们的xpath在去掉/thead和/tbody之后的形式是这样的： 这里由于情况复杂，我先分析表头部分：</p>
<p>A)表头：</p>
<pre><code>string(//*[@id=&quot;mw-content-text&quot;]/table[2]/tr)
</code></pre><p>为何要这么写，而不用直接的text()模式呢？我们来看下</p>
<pre><code>//*[@id=&quot;mw-content-text&quot;]/table[2]/tr//text()
</code></pre><p>如果这么写，得到的结果是带有空格的，如图：</p>
<p><img src="http://ocg7i7pt6.bkt.clouddn.com/blog/2016-10-16/012705629.png" alt=""></p>
<p>可以发现，不仅多出了空格，在Cost(US$)的栏目，还分了3行，而我们需要连续的，<br>综上，我们只有通过string功能来实现把空格去掉，同时把Cost(US$)合并在一起，得到的结果将是这样的：</p>
<pre><code>Name
Creator
First public release
Latest stable version
Programming language
Cost (US$)
Software license
Open source
</code></pre><p>貌似thead部分的提取还比较顺利，可接下来tbody部分呢？ </p>
<p>B)表内容：<br>我们先看开头的2行对应的xapth地址：</p>
<pre><code># tbody
# Acme xpath:  //*[@id=&quot;mw-content-text&quot;]/table[2]/tbody/tr[1]/th/a
# AkelPad xpath: //*[@id=&quot;mw-content-text&quot;]/table[2]/tbody/tr[2]/th/a
</code></pre><p>如果去掉/tbody后， 又要把表格内容全部提取，又要去掉tbody，发现只能这么写：</p>
<pre><code>string(//*[@id=&quot;mw-content-text&quot;]/table[2])
</code></pre><p>可这样是不行的，因为他不仅把表头的内容也算进去了，还把标题captain的内容也一起搞进去了。</p>
<p>此时，又根据表内容的序号格式，尝试这么写：</p>
<pre><code>string(//*[@id=&quot;mw-content-text&quot;]/table[2]//th/a)
</code></pre><p>期望的是，得到统一的表内容，可实际返回的却是：</p>
<pre><code>Programming language
</code></pre><p>这又是什么鬼呢？<br>原来，代码识别的是满足上述格式条件的<strong>第一个</strong>/a 路径下的文字，而在表头里，从Programming language开始，他有a属性。</p>
<p>此时，又发现，既然string返回的是第一个满足条件的，那么刚修正过的表头的string表达式，其实也适合表内容的表达式啊。</p>
<p>看来，我们只好先再提高一个层级，用：</p>
<pre><code>string(//*[@id=&quot;mw-content-text&quot;]/table[2])
</code></pre><p>虽然得到的结果也包含了captain，但至少后续的内容是我们想要的，那就做下数据清理。</p>
<p>8、参考：</p>
<p><a href="http://jingyan.baidu.com/article/636f38bb1eb1aad6b8461088.html" target="_blank" rel="external">http://jingyan.baidu.com/article/636f38bb1eb1aad6b8461088.html</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/10/13/西部世界 1080P高清下载和自动提醒后续新出的/" itemprop="url">
                  西部世界 1080P高清下载和自动提醒后续新出的【Python】
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Veröffentlicht am</span>
            <time itemprop="dateCreated" datetime="2016-10-13T20:59:28+08:00" content="2016-10-13">
              2016-10-13
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="西部世界-1080P高清下载和自动提醒后续新出的"><a href="#西部世界-1080P高清下载和自动提醒后续新出的" class="headerlink" title="西部世界 1080P高清下载和自动提醒后续新出的"></a>西部世界 1080P高清下载和自动提醒后续新出的</h1><p><img src="http://ocg7i7pt6.bkt.clouddn.com/blog/2016-10-14/005536645.jpg" alt=""></p>
<p>1、主要思路是，通过高清源头的网站提供的资源，爬取后，通过迅雷实现自动下载，<br>然后后续新出的，比如下周1更新后，脚本会自动捕捉后发邮件通知，并自动下载。</p>
<p>2、代码：</p>
<pre><code># -*- coding: utf-8 -*-
# python 3.5.2
# 测试系统，Win10
# Author:Van
# 实现《西部世界》有更新后自动下载，以及邮件通知
# V1.0
# 欢迎各种改进意见
# 请把对应的帐号密码修改成自己的


# from selenium import webdriver
import requests
from lxml import etree
import time
import os
from win32com.client import Dispatch
import smtplib
from email.mime.text import MIMEText
from email.header import Header
import copy

# hints
print(&apos;请确保电脑安装了迅雷&apos;)
print(&apos;如果你用的是破解版的迅雷，请先开启再运行程序&apos;)
print()
# requests
url = &apos;http://www.btbtdy.com/btdy/dy7280.html&apos;
html = requests.get(url).content.decode(&apos;utf-8&apos;)

# lxml
selector = etree.HTML(html)
real_link = []

# to be easy, try &apos;starts-with&apos; , very useful in this case :)
HDTV = selector.xpath(&apos;//a[starts-with(@title, &quot;HDTV-1080P&quot;)]/text()&apos;)
for each in HDTV:
    print(each)


href = selector.xpath(&apos;//a[starts-with(@title, &quot;HDTV-1080P&quot;)]/@href&apos;)
print()
print(&apos;目前有 %d 集西部世界&apos; %len(href))
print()

for each in href:
    # split to get the right magnet link
    each = &apos;magnet&apos; + each.split(&apos;magnet&apos;)[-1]
    # print(each)
    real_link.append(each)

print(&apos;他们的磁链接是 :\n&apos;, real_link)
# define a temp_link in deepcopy to compare for new series
temp_link = copy.deepcopy(real_link)
print(&apos;temp_link is :&apos;, temp_link)




def addTasktoXunlei(down_url,course_infos):
    flag = False
    o = Dispatch(&quot;ThunderAgent.Agent.1&quot;)
    if down_url:
        course_path = os.getcwd()
        try:
            #AddTask(&quot;下载地址&quot;, &quot;另存文件名&quot;, &quot;保存目录&quot;,&quot;任务注释&quot;,&quot;引用地址&quot;,&quot;开始模式&quot;, &quot;只从原始地址下载&quot;,&quot;从原始地址下载线程数&quot;)
            o.AddTask(down_url, &apos;&apos;, course_path, &quot;&quot;, &quot;&quot;, -1, 0, 5)
            o.CommitTasks()
            flag = True
        except Exception:

            print(Exception.message)
            print(&quot; AddTask is fail!&quot;)
    return flag

def new_href():
    # to judge if there is a new series of WestWorld
    time.sleep(2)
    if len(real_link) &gt; len(temp_link):
        print(&apos;西部世界1080P有更新!&apos;)
        print(&apos;现在一共有 %d 集了。&apos; %len(real_link))
        return True
    else:
        return False

def send_email(htm):
    # send email to notice new WestWorld is coming
    sender = &apos;xxxxxxxx@163.com&apos;
    receiver = &apos;xxxxxxxx@qq.com,xxxxxxxx@163.com&apos;
    subject = &apos;西部世界 1080P有更新！&apos;
    smtpserver = &apos;smtp.163.com&apos;
    username = &apos;xxxxxxxx@163.com&apos;
    password = &apos;xxxxxxxx&apos;
    msg = MIMEText(htm, &apos;html&apos;, &apos;utf-8&apos;)
    msg[&apos;Subject&apos;] = Header(subject, &apos;UTF-8&apos;)
    msg[&apos;From&apos;] = sender
    msg[&apos;To&apos;] = &apos;,&apos;.join(receiver)
    smtp = smtplib.SMTP()
    smtp.connect(smtpserver)
    smtp.login(username, password)
    smtp.sendmail(sender, receiver, msg.as_string())
    smtp.quit()

def new_download():
    # only download the new WestWorld series
    if len(real_link) &gt; len(temp_link):
        # 2个地址数据的差集
        new_link = list(set(real_link).difference(set(temp_link)))
        for i in new_link:
            addTasktoXunlei(i, course_infos=None)



if __name__ == &apos;__main__&apos;:
    # download the exiting series of WestWorld
    # send_email(&apos;最新更新磁链接：&apos;+ str(real_link))
    for i in real_link:
        addTasktoXunlei(i, course_infos=None)

    # to get the later WestWorld for each hour
    while 1:
        if new_href():
            send_email(&apos;所有的下载地址（磁链接）：&apos;+ str(real_link))
            new_download()
            time.sleep(15)
            # wait for an hour
            temp_link = real_link
            print(temp_link)
            print(&apos;神剧很好看吧，亲，耐心等下一集！~！&apos;)
</code></pre><p>3、代码分析，其中用到了deepcopy，这个功能很有用，并配合了2个数组的差集，使得可以规避定时器，而让脚本直接比较temp_link的内容，而扑捉到网站有新的更新了。另外，在地址识别的时候，一开始用.xpath 没显示内容，有点奇怪，后来根据特性，使用了strats_with识别了内容。另外，原始的邮件发送函数，是一个接收人，如果要多发，则receiver的格式为list，并修改 msg[‘To’] = ‘,’.join(receiver)</p>
<p>4、邮件的作用是可以利用微信绑定来推送，相对短信，更觉方便。</p>
<p>5、感谢：<br> @陌 提供了163发送email的代码<br> @何方 提供了高清网站源<br> @其他人，交流了细节</p>
<p>6、可改进点：<br>邮件的地址内容显示的是一个列表，有待改进。</p>
<p>7、github对应仓库:</p>
<p><a href="https://github.com/vansnowpea/WestWorld-auto-download-email-xunlei-" target="_blank" rel="external">https://github.com/vansnowpea/WestWorld-auto-download-email-xunlei-</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/10/11/下载国外最新高清pdf的程序测试/" itemprop="url">
                  国外最新高清pdf寻找以及实现迅雷自动下载【Python】
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Veröffentlicht am</span>
            <time itemprop="dateCreated" datetime="2016-10-11T20:59:28+08:00" content="2016-10-11">
              2016-10-11
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="国外最新高清pdf寻找以及实现迅雷自动下载"><a href="#国外最新高清pdf寻找以及实现迅雷自动下载" class="headerlink" title="国外最新高清pdf寻找以及实现迅雷自动下载"></a>国外最新高清pdf寻找以及实现迅雷自动下载</h1><p>1、今天意外发现国外某站，提供非常近期，甚至国内亚马逊还没上市的最新高清pdf，所以测试爬虫，看是否能自动下载。</p>
<p>2、</p>
<p>《OReilly.Introduction.to.Machine.Learning.with.Python.A.Guide.for.Data.Scientists.1449369413》  </p>
<p>一开始人工下载成功， 国内要月底才上线呢。</p>
<p>3、 随后测试程序是否可自动下载，第二本书的下载遇到了问题：总提示服务器维护，但更换了ip也这样的结果，后发现是对应网盘异常了。</p>
<p>4、代码：</p>
<pre><code># -*- coding: utf-8 -*-
# python 3.5.2
# 测试系统，Win10，Firefox V46
# Author:Van
# 实现自动下载高清最新pdf的实现
# V1.0 当前只针对效果还可以的国外zippyshare网盘
# 其他的网盘还没添加进判断语句，先共享如何迅雷下载等
# 如果您有经验优化，改进此脚本，请不吝指教
# QQ群： 206241755
# 简介：因下载最新高清pdf，正好发现www.foxebook.net提供
# 但是很多的广告，特烦人，所以尝试脚本，最后因下载需求，
# 加载了迅雷，这功能的实现小牛，不过也是网络别人共享的。。

from selenium import webdriver
import requests
from lxml import etree
import re
import os
from win32com.client import Dispatch



#test name of book : SciPy and NumPy
# book_name = input(&apos;Please input the book name in English:\n&apos;)
book_name = &apos;Introduction to Machine Learning with Python&apos;
print (&apos;begin to search book(s)...&apos;)
print (&apos;---------------------------------&apos;)
# search link is :http://www.foxebook.nethttp://www.foxebook.net/search/SciPy%20and%20NumPySciPy%20and%20NumPy
PostUrl = &quot;http://www.foxebook.net/search/&quot; + book_name
# print(PostUrl)
# get the content of html
html = requests.get(PostUrl).content

# use etree selector
selector = etree.HTML(html)

# /html/body/div/div/main/div[2]/div[2]/h3/a
# /html/body/div/div/main/div[3]/div[2]/h3/a
# above is two books&apos; xpath, so the right xpath for all book is :
# /html/body/div/div/main//div[2]/h3/a
# it can be confirmed by &apos;xpath checker&apos;
total_books = selector.xpath(&quot;/html/body/div/div/main//div[2]/h3/a/text()&quot;)
# print(&apos;total books from searching are:&apos;, total_books)

num1 = 0
link_address = []
real_address = []
def find_link():
    global num1
    # find the right book, put all links in a list of : link_address

    for i in total_books:
        num1 += 1
        if re.search(book_name,i):

            print(&apos;Congrdulations, we find the book(s):\n&apos;)
            print (&apos;**********************************&apos;)
            print(i)
            print (&apos;**********************************\n&apos;)
            href = &apos;http://www.foxebook.net&apos; + selector.xpath(&apos;//*[@id=&quot;content&quot;]/div/main/div[%d]/div[2]/h3/a/@href&apos;%num1)[0]
            # print(&apos;the book link is :&apos;, href)
            # print(&apos;will downloading...&apos;)
            html_new = requests.get(href).content
            selector_new = etree.HTML(html_new)
            link_new = selector_new.xpath(&apos;//*[@id=&quot;download&quot;]/div[2]/table/tbody/tr[1]/td[2]/a/@href&apos;)[0]
            # split the next link
            link_new = &apos;http:&apos;+link_new.split(&apos;:&apos;)[-1]
            link_address.append(link_new)
    print(&apos;download link is :&apos;, link_address)
    print(&apos;\n\n&apos;)

def real_book_link():
    # print(&apos;link_address is :&apos;, link_address)
    # dynamic on zippyshare
    for j in link_address:
        # 用浏览器实现访问

        driver = webdriver.Firefox()
        driver.maximize_window()
        driver.get(j)


        try:

            # find the download button
            title_list = driver.find_element_by_xpath(&apos;//*[@id=&quot;dlbutton&quot;]&apos;)
            film_link = title_list.get_attribute(&apos;href&apos;)
            real_address.append(film_link)

        except:
            print(&apos;can not download the book&apos;)

    print(&apos;real_book_link:&apos;, real_address)
    return real_address

def addTasktoXunlei(down_url,course_infos):
    flag = False
    o = Dispatch(&quot;ThunderAgent.Agent.1&quot;)
    if down_url:
        course_path = os.getcwd()
        try:
            #AddTask(&quot;下载地址&quot;, &quot;另存文件名&quot;, &quot;保存目录&quot;,&quot;任务注释&quot;,&quot;引用地址&quot;,&quot;开始模式&quot;, &quot;只从原始地址下载&quot;,&quot;从原始地址下载线程数&quot;)
            o.AddTask(down_url, &apos;&apos;, course_path, &quot;&quot;, &quot;&quot;, -1, 0, 5)
            o.CommitTasks()
            flag = True
        except Exception:

            print(Exception.message)
            print(&quot; AddTask is fail!&quot;)
    return flag

if __name__ == &apos;__main__&apos;:
    find_link()
    real_link = real_book_link()
    for i in real_link:
        addTasktoXunlei(i, course_infos=None)
</code></pre><p><img src="http://ocg7i7pt6.bkt.clouddn.com/blog/2016-10-12/022803439.png" alt=""></p>
<p>5、第二天分析：<br>更换下载的书名为：《Introduction to Machine Learning with Python》</p>
<p>得到了2个有效的书籍目录，对比昨天的书籍名，发现提供的下载源是不同的国外网盘，而昨天的那个到今天一直打不开，而这本书的网址很快就打开了，网盘名字为： zippyshare.com </p>
<p>然后研究了下，此foxebook.net站点提供的一些网盘下载使用了多家国外网盘，并且各家的广告显示不尽相同，可靠性更是差别较大。</p>
<p>另外，发现，就SciPy and NumPy一书来说，他最后得到的地址有2个http，这应该是广告模式，而后者的http的内容是我们真实需要的，所以通过冒号：来切分a.split(‘:’)[-1]。</p>
<pre><code>In [10]: a = &apos;http://sh.st/st/7a45e8ed9f73a6a10e9a22b2d8783c44/http://www65.zippyshare.com/v/oFSWQWDk/file.html&apos;

In [11]: a
Out[11]: &apos;http://sh.st/st/7a45e8ed9f73a6a10e9a22b2d8783c44/http://www65.zippyshare.com/v/oFSWQWDk/file.html&apos;

In [12]: a.split(&apos;:&apos;)[-1]
Out[12]: &apos;//www65.zippyshare.com/v/oFSWQWDk/file.html&apos;
</code></pre><p>6、忘记说明下昨天的代码为何要用re.match （或者re.research）, 这是因为网站的关键词搜索引擎所使用的算法，我们是不知道的，但从搜索结果看，某关键词下，可能有不同的书籍，而我们是需要精确搜索，下图中实际出现了16本书，但针对SciPy and NumPy，我们要找的是第三个图对应的。因此，我们可以把显示的书名做一个match对照的循环，来实现精确匹配。而另外一方面，网站提供的书名还可能多了冒号，后面附加书名，这样的也符合我们的要求。后来发现用关键词 if xxx in yyy的方式更简便。</p>
<p><img src="http://ocg7i7pt6.bkt.clouddn.com/blog/2016-10-12/093157994.png" alt=""></p>
<p>7、昨天的代码一开始没有考虑到国外网盘下载异常失败的问题，并且有的搜索结果可能有多个网盘地址，而我只取了默认的第一个，考虑到下载的失败可能性，最好把所有下载地址都获取。所以代码需要修改。<br>由于：SciPy and NumPy 对应的网盘当机，选用：《Introduction to Machine Learning with Python》为例</p>
<p>经过对照，在最后的下载界面，是动态的，因此调用selenium+Firefox组合。最后终于得到了完整pdf队中的链接，但速度明显比较慢了，在本例中，是rar后缀的压缩包格式，里面含有pdf。</p>
<pre><code>download link is : [&apos;http://www78.zippyshare.com/v/hBU7JYZp/file.html&apos;, &apos;http://www65.zippyshare.com/v/oFSWQWDk/file.html&apos;]



content: 
book link: http://www78.zippyshare.com/d/hBU7JYZp/2248094/OReilly.Introduction.to.Machine.Learning.with.Python.A.Guide.for.Data.Scientists.1449369413.rar
content: 
book link: http://www65.zippyshare.com/d/oFSWQWDk/1124867/OReilly.Introduction.to.Machine.Learning.with.Python.1449369413_Early.Release.rar

Process finished with exit code 0
</code></pre><p>8、接下来的一个问题，怎么让程序自动下载这2个链接？群里有人推荐了一些别的软件，但是我想来想去因为以后总要面对下载速度的问题，还是选定了迅雷破解版吧，除非将来有其他更好的方案，好在有人共享了一个方案，还特别简单，不过据说只能支持http格式，BT格式的以后再想办法。</p>
<p>9：补充说明，在正文代码的第2个下载地址，是有问题的，差别在于地址点击后，前者可在浏览器或者迅雷直接下载，而后者浏览器没反映，迅雷里下载的是一个html。尽管2个链接的提取方法完全一样，但一个好使，一个异常，由于是同一本书的前后2个小版本，我也不管他了，但为了验证迅雷是否能同时下载5个（代码里设定同时下载的最大值，也是一般默认值） 我用额外的测试脚本加载了一个新的链接，是证明可同时下载的，如图：</p>
<p><img src="http://ocg7i7pt6.bkt.clouddn.com/blog/2016-10-12/203618609.png" alt=""> </p>
<p>9、参考：</p>
<p><a href="http://neue.v2ex.com/t/275703" target="_blank" rel="external">http://neue.v2ex.com/t/275703</a></p>
<p>10、github对应仓库：</p>
<p><a href="https://github.com/vansnowpea/download-pdf-with-Xunlei" target="_blank" rel="external">https://github.com/vansnowpea/download-pdf-with-Xunlei</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/10/10/用户细分精准营销--聚类/" itemprop="url">
                  用户细分精准营销--聚类　以及　机器学习典型应用【Python】
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Veröffentlicht am</span>
            <time itemprop="dateCreated" datetime="2016-10-10T13:59:28+08:00" content="2016-10-10">
              2016-10-10
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="用户细分精准营销–聚类"><a href="#用户细分精准营销–聚类" class="headerlink" title="用户细分精准营销–聚类"></a>用户细分精准营销–聚类</h1><p>1、中国移动的各个手机套餐之间的分类为例：</p>
<p><img src="http://ocg7i7pt6.bkt.clouddn.com/blog/2016-10-10/132655994.png" alt=""></p>
<ul>
<li>动感地带： 目标群：在校学生，短信需求旺盛，短信费用低。</li>
</ul>
<ul>
<li>神州大众卡：目标群：更多客户，四川移动推出：最让消费者动心的是没有月租费的日子里接听网内来电20元包完。</li>
</ul>
<ul>
<li>全球通： 目标群：全球飞的高端上午人群，不是很在乎话费，却关心品牌的增值服务，如vip候机厅等。</li>
</ul>
<ul>
<li>神州行： 目标群：普通务工人员，电话需求较多，通话费用低。</li>
</ul>
<p>那么在那个还没有智能机的年代，他们是怎么想到这些分类方法或者区分客户群的呢？回头想来客户群的区分有相当比例是由客户自己定义使用哪个套餐的。当然，以现在的技术用计算机AI可以相对容易的来进行聚类区分了。</p>
<hr>
<p>反垃圾邮件：　    朴素贝叶斯 </p>
<p>信贷风险控制：　  决策树</p>
<p>互联网广告：　    ctr预估-线性逻辑回归 </p>
<p>推荐系统：　     协同过滤 </p>
<p>自然语言处理：　  情感分析，实体识别 </p>
<p>图像识别：　     深度学习 </p>
<p>其他 </p>
<hr>
<p><img src="http://ocg7i7pt6.bkt.clouddn.com/blog/2016-10-10/135019916.png" alt=""></p>
<h1 id="一些常用的算法分类表："><a href="#一些常用的算法分类表：" class="headerlink" title="一些常用的算法分类表："></a>一些常用的算法分类表：</h1><p><img src="http://ocg7i7pt6.bkt.clouddn.com/blog/2016-10-10/143149096.png" alt=""></p>
<ul>
<li>C4.5 是决策树算法，可以解决分类和回归问题，又属于有监督算法</li>
</ul>
<ul>
<li>K-Means 是聚类算法，无监督。</li>
<li>SVM： 基于统计学，有完整理论。</li>
<li>PageRank：谷歌的算法 </li>
</ul>
<h1 id="机器学习的框架"><a href="#机器学习的框架" class="headerlink" title="机器学习的框架"></a>机器学习的框架</h1><h2 id="训练模型："><a href="#训练模型：" class="headerlink" title="训练模型："></a>训练模型：</h2><p>1、定义模型</p>
<p>2、定义损失函数</p>
<p>3、优化算法</p>
<h2 id="模型评估："><a href="#模型评估：" class="headerlink" title="模型评估："></a>模型评估：</h2><p>1、交易验证</p>
<p>2、效果评估</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/10/08/数据分析的参考书集锦/" itemprop="url">
                  数据分析的参考书集锦【Python】+【R】
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Veröffentlicht am</span>
            <time itemprop="dateCreated" datetime="2016-10-08T20:59:28+08:00" content="2016-10-08">
              2016-10-08
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="数据分析的参考书集锦，先保存过来再说。"><a href="#数据分析的参考书集锦，先保存过来再说。" class="headerlink" title="数据分析的参考书集锦，先保存过来再说。"></a>数据分析的参考书集锦，先保存过来再说。</h1><p>原文： <a href="http://bbs.pinggu.org/thread-3116701-1-1.html" target="_blank" rel="external">http://bbs.pinggu.org/thread-3116701-1-1.html</a></p>
<h1 id="入门读物："><a href="#入门读物：" class="headerlink" title="入门读物："></a>入门读物：</h1><p>深入浅出数据分析 这书挺简单的，基本的内容都涉及了，说得也比较清楚，最后谈到了 R 是大加分。难易程度：非常易。<br>啤酒与尿布 通过案例来说事情，而且是最经典的例子。难易程度：非常易。<br>数据之美 一本介绍性的书籍，每章都解决一个具体的问题，甚至还有代码，对理解数据分析的应用领域和做法非常有帮助。难易程度：易。<br>数学之美 这本书非常棒啦，入门读起来很不错！<br>      下载地址：深入浅出数据分析、啤酒与尿布、数据之美、数学之美。</p>
<p>数据分析：</p>
<p>SciPy and NumPy 这本书可以归类为数据分析书吧，因为 numpy 和 scipy 真的是非常强大啊。<br>Python for Data Analysis 作者是 Pandas 包的作者，看过他在 Scipy 会议上的演讲，实例非常强！<br>Bad Data Handbook 很好玩的书，作者的角度很不同。<br>       下载地址：SciPy and NumPy、Python for Data Analysis、Bad Data Handbook</p>
<h1 id="适合入门的教程："><a href="#适合入门的教程：" class="headerlink" title="适合入门的教程："></a>适合入门的教程：</h1><p>集体智慧编程 学习数据分析、数据挖掘、机器学习人员应该仔细阅读的第一本书。作者通过实际例子介绍了机器学习和数据挖掘中的算法，浅显易懂，还有可执行的 Python 代码。难易程度：中。<br>Machine Learning in Action 用人话把复杂难懂的机器学习算法解释清楚了，其中有零星的数学公式，但是是以解释清楚为目的的。而且有 Python 代码，大赞！目前中科院的王斌老师（微博： 王斌_ICTIR）已经翻译这本书了 机器学习实战 。这本书本身质量就很高，王老师的翻译质量也很高。难易程度：中。我带的研究生入门必看数目之一！<br>Building Machine Learning Systems with Python 虽然是英文的，但是由于写得很简单，比较理解，又有 Python 代码跟着，辅助理解。<br>数据挖掘导论 最近几年数据挖掘教材中比较好的一本书，被美国诸多大学的数据挖掘课作为教材，没有推荐 Jiawei Han 老师的那本书，因为个人觉得那本书对于初学者来说不太容易读懂。难易程度：中上。<br>Machine Learning for Hackers 也是通过实例讲解机器学习算法，用 R 实现的，可以一边学习机器学习一边学习 R。<br>       下载地址：集体智慧编程+源代码、Machine Learning in Action、Building Machine Learning Systems with Python、                            数据挖掘导论、Machine Learning for Hackers</p>
<h1 id="稍微专业些的："><a href="#稍微专业些的：" class="headerlink" title="稍微专业些的："></a>稍微专业些的：</h1><p>Introduction to Semi-Supervised Learning 半监督学习必读必看的书。<br>Learning to Rank for Information Retrieval 微软亚院刘铁岩老师关于 LTR 的著作，啥都不说了，推荐！<br>Learning to Rank for Information Retrieval and Natural Language Processing 李航老师关于 LTR 的书，也是当时他在微软亚院时候的书，可见微软亚院对 LTR 的研究之深，贡献之大。<br>推荐系统实践 这本书不用说了，研究推荐系统必须要读的书，而且是第一本要读的书。<br>Graphical Models, Exponential Families, and Variational Inference 这个是 Jordan 老爷子和他的得意门徒 Martin J Wainwright 在 Foundation of Machine Learning Research 上的创刊号，可以免费下载，比较难懂，但是一旦读通了，graphical model 的相关内容就可以踏平了。<br>Natural Language Processing with Python NLP 经典，其实主要是讲 NLTK 这个包，但是啊，NLTK 这个包几乎涵盖了 NLP 的很多内容了啊！<br>   下载地址：Introduction to Semi-Supervised Learning、Learning to Rank for Information Retrieval、                           Learning to Rank for Information Retrieval and Natural Language Proces、推荐系统实践</p>
<p>机器学习教材：<br>The Elements of Statistical Learning 这本书有对应的中文版：统计学习基础 。书中配有 R 包，非常赞！可以参照着代码学习算法。<br>统计学习方法 李航老师的扛鼎之作，强烈推荐。难易程度：难。<br>Machine Learning 去年出版的新书，作者 Kevin Murrphy 教授是机器学习领域中年少有为的代表。这书是他的集大成之作，写完之后，就去 Google 了，产学研结合，没有比这个更好的了。<br>Machine Learning 这书和上面的书不是一本！这书叫：Machine Learning: An Algorithmic Perspective 之前做过我带的研究生教材，由于配有代码，所以理解起来比较容易。<br>Pattern Recognition And Machine Learning 经典中的经典。<br>Bayesian Reasoning and Machine Learning 看名字就知道了，彻彻底底的 Bayesian 学派的书，里面的内容非常多，有一张图将机器学习中设计算法的关系总结了一下，很棒。<br>Probabilistic Graphical Models 鸿篇巨制，这书谁要是读完了告诉我一声。<br>Convex Optimization 凸优化中最好的教材，没有之一了。课程也非常棒，Stephen 老师拿着纸一步一步推到，图一点一点画，太棒了。<br>下载地址：The Elements of Statistical Learning、统计学习方法、Machine Learning: An Algorithmic Perspective、<br>                            Pattern Recognition and Machine Learning+答案、Bayesian Reasoning and Machine Learning、                                                   Probabilistic Graphical Models、Convexity and Optimization</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/10/07/下载最新电影的爬虫/" itemprop="url">
                  下载最新电影的爬虫---Xpath读取文本，链接，标题等的操作【Python】
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Veröffentlicht am</span>
            <time itemprop="dateCreated" datetime="2016-10-07T23:59:28+08:00" content="2016-10-07">
              2016-10-07
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="下载最新电影的爬虫"><a href="#下载最新电影的爬虫" class="headerlink" title="下载最新电影的爬虫"></a>下载最新电影的爬虫</h1><p>1、使用：</p>
<ul>
<li>lxml.etree</li>
<li>requests</li>
</ul>
<p>2、代码：</p>
<pre><code># -*- coding:utf-8 -*-
import requests
from lxml import etree


url = &apos;http://www.ygdy8.net/html/gndy/dyzz/index.html&apos;  #这是电影最新电影的网站
# r = requests.get(url).content
# print(r.encoding）
# &gt;&gt;&gt; ISO-8859-1
html = requests.get(url).content
# html = requests.get(url).content.decode(&apos;ISO-8859-1&apos;).encode(&apos;utf-8&apos;)

selector = etree.HTML(html)

# //*[@id=&quot;content&quot;]/div/div[1]/ol/li[1]/div/div[2]/div[2]/p[1]
# get rid of the : /tbody
biaoti = selector.xpath(&apos;//*[@id=&quot;header&quot;]/div/div[3]/div[3]/div[2]/div[2]/div[2]/ul//tr[2]/td[2]/b/a/text()&apos;)


# get rid of the : /tbody
jianjie = selector.xpath(&apos;/html/body/div/div/div[3]/div[3]/div[2]/div[2]/div[2]/ul//tr[4]/td/text()&apos;)
wangzhi = selector.xpath(&apos;//*[@id=&quot;header&quot;]/div/div[3]/div[3]/div[2]/div[2]/div[2]/ul//tr[2]/td[2]/b/a/@href&apos;)

for b,j,k in zip(biaoti,jianjie,wangzhi):
    print(b+&apos;\n&apos;+j+&apos;\n&apos;+&apos;www.ygdy8.net&apos;+k+&apos;\n&apos;)
</code></pre><p><img src="http://ocg7i7pt6.bkt.clouddn.com/blog/2016-10-08/015240484.png" alt="">    </p>
<p>3、分析：主要和上一篇的接近，但是这一篇还有读取网址等操作，注意格式上和读取内容text有差别的。<br>总结如下：</p>
<pre><code>获取到标签后我们可以获取标签中的属性值
tree.xpath(&quot;//div[@class=&apos;sec_blk mrg_b_30&apos;]/ul/li[1]/a/text()&quot;)     #获取a的文本，li标号是从1开始，而不是从0开始
tree.xpath(&quot;//div[@class=&apos;sec_blk mrg_b_30&apos;]/ul/li[1]/a/@href&quot;)   #获取a的链接地址

当然还有其他类似的xpath例子：
&quot;//input[@id=&apos;city&apos;]/@value&quot;
&quot;//div[@class=&apos;venueDetal&apos;]/p/img[@class=&apos;img&apos;]/@src&quot;
&quot;//div[@class=&apos;detail_info_title&apos;]//a[@class=&apos;hotel_star&apos;]/@title&quot;
</code></pre><p>4、参考： <a href="http://blog.chinaunix.net/uid-13869856-id-5747494.html" target="_blank" rel="external">python+lxml xpath获取数据 </a></p>
<p>5、可补充点： 自动提取直接下载链接，看是否能导入到迅雷等工具。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/10/07/东方财富网cpi数据的抓取/" itemprop="url">
                  东方财富网cpi数据的抓取【Python】
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Veröffentlicht am</span>
            <time itemprop="dateCreated" datetime="2016-10-07T22:59:28+08:00" content="2016-10-07">
              2016-10-07
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="东方财富网cpi数据的抓取"><a href="#东方财富网cpi数据的抓取" class="headerlink" title="东方财富网cpi数据的抓取"></a>东方财富网cpi数据的抓取</h1><p>1、使用：</p>
<ul>
<li>lxml.etree</li>
<li>requests</li>
</ul>
<p>2、代码：</p>
<pre><code>from lxml import etree
import requests

url = &apos;http://data.eastmoney.com/cjsj/cpi.html&apos;
content = requests.get(url).content

html = etree.HTML(content)
content1 = html.xpath(&apos;//*[@id=&quot;tb&quot;]/tr[3]/td[2]/text()&apos;)

print(content1[0].strip().replace(&apos;\n\r&apos;, &apos;&apos;))

# for txt in html.iterfind(&apos;.//*[@id=&quot;tb&quot;]/tr[3]/td&apos;):
#     print(txt.text)
</code></pre><p>3、分析：requests+lxml来分析和提取数据比较简单，可以尽可能的规避使用RE的复杂性以及可能产生的编码问题。要注意的是提取文本内容的时候要在xpath地址后面加上/text()<br>不过这个案例中，需要把xpath的/tbody去掉，这是因为有的浏览器加上去的，否则不能识别需要的文字部分。<br>另外，默认的结果因为有很多的空格和\n\r，所以需要做数据清洗，就本例，加一句content1[0].strip().replace(‘\n\r’, ‘’)即可。</p>
<p>4、没有执行的代码，表示读取2016年08月份 “全国”，“城市”，“农村”各自的cpi数据。<br>要注意的是：这属于ElementPath，一共4个种类，分别如下：</p>
<ul>
<li></li>
</ul>
<ol>
<li>iterfind()</li>
<li>findall()</li>
<li>find()</li>
<li>findtext()</li>
</ol>
<p>千万注意的是：ElementPath不能直接使用绝对路径，需要在前面加一个.符号。暂时感觉这个使用的不太多。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/10/05/tensorflow 初步学习/" itemprop="url">
                  begin to learn TensorFlow
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Veröffentlicht am</span>
            <time itemprop="dateCreated" datetime="2016-10-05T16:20:28+08:00" content="2016-10-05">
              2016-10-05
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><img src="http://ocg7i7pt6.bkt.clouddn.com/blog/2016-10-05/163346079.png" alt=""></p>
<h1 id="Introduction-begin-to-learn-Machine-Learning-and-find-Google’s-open-source-technology-TensorFlow"><a href="#Introduction-begin-to-learn-Machine-Learning-and-find-Google’s-open-source-technology-TensorFlow" class="headerlink" title="Introduction: begin to learn Machine Learning, and find Google’s open source technology: TensorFlow."></a>Introduction: begin to learn Machine Learning, and find Google’s open source technology: TensorFlow.</h1><p>1、It seems TF only support Linux by official , but I try to install also on Win 10, so try :： <a href="http://wiki.jikexueyuan.com/project/tensorflow-zh/" target="_blank" rel="external">http://wiki.jikexueyuan.com/project/tensorflow-zh/</a>：<br>though some guy reported successed on win, but I failed as can not download the big file no matter with USA IP, anyway , I record the method.</p>
<pre><code>捣鼓好了windows docker安装，参考了楼上的许多信息。
1. 安装docker
2. 点开Docker Quickstart Terminal， 打开成功后：
    docker is configured to use the default machine with IP 192.168.99.100
3. 安装tensorflow： docker run -d -p 8888:8888 -v /notebook:/notebook xblaster/tensorflow-jupyter
4. 运行tensorflow-jupyter: docker run xblaster/tensorflow-jupyter
     会提示running at: http://0.0.0.0:8888，不知道为什么会是这个IP地址，用浏览器打开不了。然后替换成docker打开时的IP，http://192.168.99.100:8888就可以打开了。
5. 运行example code，mnist参考资料：1, tensorflow官方文档；2，http://blog.csdn.net/yhl_leo/article/details/50614444 及其mnist的github：https://github.com/yhlleo/mnist，github中有input_data.py这个很重要的文件。
</code></pre><p>2、 I run it on Ubuntu and test an easy exmaple which is :</p>
<pre><code>y =0.1*x +0.3
</code></pre><p>and after 200 times of calculating , get result :</p>
<p><img src="http://ocg7i7pt6.bkt.clouddn.com/blog/2016-10-05/235616627.png" alt=""> </p>
<p>from the result : after 40 times calculating , the result closes to 0.1*x + 0.3 already , no matter how AlphaGo is so strong.</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/10/02/numpy和pandas的初步学习/" itemprop="url">
                  numpy和pandas的初步学习以及6本数据分析必读书和2份英文教程
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Veröffentlicht am</span>
            <time itemprop="dateCreated" datetime="2016-10-02T14:20:28+08:00" content="2016-10-02">
              2016-10-02
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><img src="http://ocg7i7pt6.bkt.clouddn.com/blog/2016-10-02/235644734.png" alt=""></p>
<h1 id="简介：-今天把cmder的配色研究下，找了个nice的shell版，赞一个，另外，英文是绕不过去的，最后总会发现想要的资料只有英文的。-：）"><a href="#简介：-今天把cmder的配色研究下，找了个nice的shell版，赞一个，另外，英文是绕不过去的，最后总会发现想要的资料只有英文的。-：）" class="headerlink" title="简介： 今天把cmder的配色研究下，找了个nice的shell版，赞一个，另外，英文是绕不过去的，最后总会发现想要的资料只有英文的。 ：）"></a>简介： 今天把cmder的配色研究下，找了个nice的shell版，赞一个，另外，英文是绕不过去的，最后总会发现想要的资料只有英文的。 ：）</h1><p>1、这几天抽空看了下《利用python进行数据分析》的几个章节，其中pandas部分看了2遍，熟悉了一些命令和用法，</p>
<p>2、国外一个朋友的对某个问题的建议是使用JS， 首先我是按照这个网页的心得：不过我找了一本JS的入门书，发现内容不是兴趣所在，所以暂时先记录之。</p>
<p><a href="http://kb.cnblogs.com/page/191787/" target="_blank" rel="external">http://kb.cnblogs.com/page/191787/</a></p>
<p>3、另搜资料的时候，找了一个国外的网址，介绍了数据分析几本不错的书，<br><a href="https://www.analyticsvidhya.com/blog/2014/06/books-data-scientists-or-aspiring-ones/" target="_blank" rel="external"><br>Must have books for data scientists (or aspiring ones)</a></p>
<p>4、在上述链接中，是6本书的英文介绍，貌似R的数量更多，并且有一本是R的，但有第三方给出了Python代码，（注意有的文字因粘贴丢失了超链接）：</p>
<pre><code>1. R Cookbook by Paul Teetor

This is simply the best book to start your journey with R. It contains tons of examples and practical advice on a wide range of topics like file input / output, data manipulations, merging and sorting to building a regression model. For a starter in R, this book becomes your best pal during the initial testing time.

While the book is aimed towards starters, it still remains a prominent feature of the library of any data scientist.



2. Machine Learning for Hackers by Drew Conway &amp; John Myles White

I think this book actually has a wrong title. I dropped purchasing it twice before giving it a shot (which happened only because of a recommendation from a close friend). This book is meant for data scientists and not hackers. I don’t know why the title says so. A very practical manual for learning machine learning, it comes with good visuals and you can get a copy of codes in Python (original book is based on R).



3. R graphics cookbook by Winston Chang

You can’t be a good data scientist unless you master the graphics in R! There is no better way for visualization, but to learn ggplot2. Sadly, learning ggplot2 might seem like learning a completely new language in itself. This is where this “cookbook” comes to rescue. The recipes from Winston are short, sweet and to the point. Buy this and it is bound to end up as one of the most referred book in your library.



4. Programming Collective Intelligence by Toby Segaran (popularly referred as PCI)

If there is one book you want to choose, out of this selection (for learning machine learning) – it is this one. I haven’t met a data scientist yet who has read this book and does not recommend to keep it on your bookshelf. A lot of them have re-read this book multiple times. The book was written long before data science and machine learning acquired the cult status they have today – but the topics and chapters are entirely relevant even today! Some of the topics covered in the book are collaborative filtering techniques, search engine features, Bayesian filtering and Support vector machines. If you don’t have a copy of this book – order it as soon as you finish reading this article! The book uses Python to deliver machine learning in a fascinating manner.



5. Python for Data Analysis by Wes McKinney

Written by Wes McKinney, this book teaches you everything you need about Pandas. For the starters (not sure why you are still reading this article), pandas are Python’s way to handle data structures. Except for the title of the book (which I find misleading), I like everything else about this book. It contains ample codes and examples to leave you capable of performing any operation / transformation on a dataframe in Python (using pandas).

For the advanced users, if you already know pandas, you should look at this presentation from Wes on what are the shortcomings of pandas.



6. Agile data science by Russell Jurney

A recent addition by O’Reilly, this book looks like a must read for data scientists. The focus is on using “light” tools, which are easy to use and still get the work done. This is currently on my reading list and I’ll update more details once I have read it.



These are the 6 must have books, if you are serious about being a data scientist. There are a couple of additional Python books, which you can consider – Natural Language processing with Python by Steven Bird et al and Mining the social web by Matthew A. Russell. The reason I have not kept them in the list is because you can find a lot of the information in these books easily on the web.
</code></pre><p>5、另外，还有2篇不错的英文的基于python-pandas的数据分析教程：</p>
<p><a href="https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/" target="_blank" rel="external">A Complete Tutorial to Learn Data Science with Python from Scratch</a></p>
<p>以及这个：</p>
<p><a href="https://www.analyticsvidhya.com/blog/2014/09/data-munging-python-using-pandas-baby-steps-python/" target="_blank" rel="external">Data Munging in Python (using Pandas) – Baby steps in Python</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/09/27/收到《Python for data analysis》作者的邮件回复/" itemprop="url">
                  收到《Python for data analysis》作者的邮件回复
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Veröffentlicht am</span>
            <time itemprop="dateCreated" datetime="2016-09-27T14:20:28+08:00" content="2016-09-27">
              2016-09-27
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><img src="http://ocg7i7pt6.bkt.clouddn.com/blog/2016-09-27/150628293.jpg" alt=""></p>
<h1 id="简介：-收到《Python-for-data-analysis》作者的邮件回复-：）"><a href="#简介：-收到《Python-for-data-analysis》作者的邮件回复-：）" class="headerlink" title="简介： 收到《Python for data analysis》作者的邮件回复 ：）"></a>简介： 收到《Python for data analysis》作者的邮件回复 ：）</h1><p>1、上次给他写了一个邮件，建议用Anaconda来写下一版本，虽然过了几天还是收到了回复，看样子他也意识到Anaconda更好点，也表示第二版会采用：</p>
<pre><code>try

from matplotlib.pyplot import *

and running

%matplotlib

I am creating a 2nd edition of the book, and it will use Anaconda
instead of Canopy in the instructions.

Thanks!
</code></pre><p>2、然后我去Python中文社区问了几个人，表示有兴趣翻译，由于第一版是机械工业出版社搞的中译版，发了一个email咨询是否将来打算出第二版的中译，但是还没得到回复，想了下，如果他们不翻译，就召集社区的小伙伴们来翻译吧。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel  sidebar-panel-active ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="Van" />
          <p class="site-author-name" itemprop="name">Van</p>
          <p class="site-description motion-element" itemprop="description"></p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">58</span>
              <span class="site-state-item-name">Artikel</span>
            </a>
          </div>

          

          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2016</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Van</span>
</div>

<div class="powered-by">
  Erstellt mit  <a class="theme-link" href="https://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  



  




  
  

  

  

  

</body>
</html>
