<!doctype html>



  


<html class="theme-next muse use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1" />






<meta property="og:type" content="website">
<meta property="og:title" content="天青色等烟雨">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="天青色等烟雨">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="天青色等烟雨">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: 'Author'
    }
  };
</script>




  <link rel="canonical" href="http://yoursite.com/"/>

  <title> 天青色等烟雨 </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">天青色等烟雨</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Startseite
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archiv
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/09/19/关于Xpath地址的分析小结/" itemprop="url">
                  关于Xpath地址的分析小结
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Veröffentlicht am</span>
            <time itemprop="dateCreated" datetime="2016-09-19T20:20:28+08:00" content="2016-09-19">
              2016-09-19
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="简介：-发现有时候遇到xpath的路径报错，然而使用的都是一些插件工具，但不同浏览器和不同插件得到的xpath又有所差别，故做一个测试记录"><a href="#简介：-发现有时候遇到xpath的路径报错，然而使用的都是一些插件工具，但不同浏览器和不同插件得到的xpath又有所差别，故做一个测试记录" class="headerlink" title="简介： 发现有时候遇到xpath的路径报错，然而使用的都是一些插件工具，但不同浏览器和不同插件得到的xpath又有所差别，故做一个测试记录"></a>简介： 发现有时候遇到xpath的路径报错，然而使用的都是一些插件工具，但不同浏览器和不同插件得到的xpath又有所差别，故做一个测试记录</h1><p>1、旁引： 因故装python 3 +2 的双蛇系统，是故再次要把anaconda配置到python 2 去然后搜网址的时候，看到一个评论，喜感，记录如下：</p>
<p>问：anaconda与python什么关系</p>
<p>答复：<br>python 是莽蛇，一般是陆上蛇<br>Anaconda 是美洲大水蛇。</p>
<p>是不是可以写一部： 水与火之歌  ：） </p>
<p>2、以豆瓣网电影斑块的“热门”按键为例，如图：</p>
<p><img src="http://ocg7i7pt6.bkt.clouddn.com/blog/2016-09-19/181325132" alt=""></p>
<p>在chrome里的xpath是：</p>
<pre><code>//*[@id=&quot;gaia_frm&quot;]/div[1]/div[1]/label[1]
</code></pre><p>在firefox 的firebug插件：：</p>
<pre><code>/html/body/div[3]/div[1]/div/div[2]/div[4]/div[2]/div[1]/form/div[1]/div[1]/label[1]
</code></pre><p>而firexfox的xpath checker插件则是：</p>
<pre><code>id(&apos;gaia_frm&apos;)/x:div[1]/x:div[1]/x:label[1]
</code></pre><p>又测试QQ浏览器为：</p>
<pre><code>//*[@id=&quot;gaia_frm&quot;]/div[1]/div[1]/label[1]
</code></pre><p>可以发现，谷歌和qq浏览器的格式一致，但firefox的插件，则不尽相同，但最起码，倒过来看，还是基本相同，这其实是路径开头选择差异导致的，而到路径最后基本都一样。但还要看一个标签里，采用的是什么属性定位，比如通过id，通过name等的区别。</p>
<ul>
<li><p>其中，firefox的view pather差价显示的比较特别，带有X： 字样，估计是该差价自定义的xpath格式，但如果直接复制到python里，是不识别的，因此会报错。所以，如果要采用他的格式，需要把x： 给删掉。测试就通过了。也就是要修改成：</p>
<p>  driver.find_element_by_xpath(“id(‘gaia_frm’)/div[1]/div[1]/label[1]”)</p>
</li>
</ul>
<ul>
<li><p>谷歌和qq的一样，只需要测试一个，结论直接通过：</p>
<p>  driver.find_element_by_xpath(“//*[@id=\”gaia_frm\”]/div[1]/div[1]/label[1]”)</p>
</li>
</ul>
<ul>
<li><p>firebug的，结论通过：</p>
<p>  driver.find_element_by_xpath(“/html/body/div[3]/div[1]/div/div[2]/div[4]/div[2]/div[1]/form/div[1]/div[1]/label[1]”</p>
</li>
</ul>
<p>结论：</p>
<ul>
<li>谷歌和qq浏览器自带的xpath路径分析，可通过常规需求的测试（由于有隐藏元素的一些网页功能，遇到的话依然可能失败，还有就是动态刷新技术）</li>
<li>由于我安装的firefox没看到自带的xpath分析功能，所以安装了firebug插件以及xpath checker插件，但xpath checker插件自带的格式却得不到浏览器的支持，需要特别注意。</li>
<li>xpath checker在反向验证xpath路径的时候还是有用。</li>
</ul>
<p>关于最后一点展开下，也是经验总结：</p>
<p>比如，你看别人的代码分析，因为你不确定他是使用浏览器自带的，还是自己定义的，还是插件的xpath路径，甚至可能对方根本没有任何注释，导致你看到xpath路径的时候，一头的晕，丫到底写的是啥呀？也许聪明的你会说，那可以到网页源代码查找一部分的关键词，确实可以，但也麻烦，举例来说：</p>
<pre><code>driver_item.find_element_by_xpath(&quot;//div[@class=&apos;list-wp&apos;]/a[@class=&apos;more&apos;]&quot;).click()
</code></pre><p>虽然可以知道要点击一个含有class=’more’的控件元素，但你会发现直接网页源代码木有！！ 为何？ 因为他的真实是双引号class=”more”<br>原来，因为selenium 的find_element_by_xpath(“XXX”) 命令，如果你把XXX用class=”more”直接代替，绝对的报错，为何，因为双引号冲突， 你可以反斜杠来区分，抑或用单引号，这就是原因所在。可见，即便你到网页源代码查找还是麻烦的很，更不要说，如果万一源代码里有好几个这样的查找单元了。</p>
<p>而我们用，xpath checker反向验证，可以很快的让你知道，对方分析的是什么元素，如下图：</p>
<p><img src="http://ocg7i7pt6.bkt.clouddn.com/blog/2016-09-19/185937608" alt=""></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/09/18/stocksnap 一个不错的图片网站的图片抓取/" itemprop="url">
                  stocksnap 一个不错的图片网站的图片抓取--selenium 右键保存和直接写入2个模式【python】
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Veröffentlicht am</span>
            <time itemprop="dateCreated" datetime="2016-09-18T10:59:28+08:00" content="2016-09-18">
              2016-09-18
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="参考：原文1"><a href="#参考：原文1" class="headerlink" title="参考：原文1"></a>参考：<a href="http://www.jianshu.com/p/2528edf4485c" target="_blank" rel="external">原文1</a></h1><h1 id="参考：原文2"><a href="#参考：原文2" class="headerlink" title="参考：原文2"></a>参考：<a href="http://blog.csdn.net/seanwang_25/article/details/43318907" target="_blank" rel="external">原文2</a></h1><p>1、原文1采用了scrapy方法，本文改编用selenium方法，并参考原文2首次采用模拟右键来保存图片。</p>
<p>2、网站分析：</p>
<p>首先发现鼠标拖动到图片底部区域，主页又不断加载新的图片，可判断是异步的。另单独查看2个图的xpath，发现总体格式一致，编号部分有差异：</p>
<pre><code>/html/body/div[4]/div[3]/div[2]/div/div[1]/a/img

/html/body/div[4]/div[3]/div[2]/div/div[11]/a/img
</code></pre><p>这样我们就得到了在firefox下统一的xpath为，注意是删除div[i]部分，留下2个斜杠：</p>
<pre><code>/html/body/div[4]/div[3]/div[2]/div//a/img
</code></pre><p>此时，可能觉得为啥要删除，而不是用正则式.*代替？测试那样的结果是Nan，也就是说xpath的格式和re格式不能混搭。</p>
<p>而每个jpg地址在src属性中，所以如果想查阅批量的图片地址，则：</p>
<pre><code>/html/body/div[4]/div[3]/div[2]/div//a/img/@src
</code></pre><p><img src="http://ocg7i7pt6.bkt.clouddn.com/stocksnap%20%E5%9B%BE%E7%89%87%E7%BD%91%E5%9D%80.png" alt=""></p>
<p>3、既然这么爽的得到了图片地址，剩下就是批量下载保存了：</p>
<p>这一次，先测试了用firefox模拟人工右键保存的方法，代码如下，保存那块还没整明白，也就是自动到出现图片保存的界面,根本原因是selenium无法操作操作系统级的对话框：</p>
<p>索性快速人工点保存，30个图的保存位置都是重复的，依次得到30个图：</p>
<p><img src="http://ocg7i7pt6.bkt.clouddn.com/selenium%2030pics.png" alt=""></p>
<p>4、随机打开一个图，如下，可见测试ok：<br><img src="http://ocg7i7pt6.bkt.clouddn.com/t1.png" alt=""></p>
<p>5、代码：</p>
<pre><code># -*- coding: utf-8 -*-
# python 3.5.2
# Author:vansnowpea
# stocksnap 一个不错的图片网站的图片，右键保存抓取

from selenium import webdriver
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.common.keys import Keys


print(&apos;Please wait...Firefox loading...&apos;)
print(&apos;---------------------------------&apos;)


url = &quot;https://stocksnap.io/&quot;

# 用浏览器实现访问
driver = webdriver.Firefox()
driver.maximize_window()
driver.get(url)

# 得到总的jpgs的路径集合
xpath = &quot;/html/body/div[4]/div[3]/div[2]/div//a/img&quot;

# set profile
fp = webdriver.FirefoxProfile()
fp.set_preference(&apos;browser.download.folderList&apos;, 2)
fp.set_preference(&apos;browser.download.manager.showWhenStarting&apos;, False)
fp.set_preference(&apos;browser.download.dir&apos;, &apos;./yourfolder/&apos;)
fp.set_preference(&apos;browser.helperApps.neverAsk.saveToDisk&apos;, &apos;image/jpeg&apos;)



# 保存图片,人工批量点保存，selenium无法操作操作系统级的对话框
for element in driver.find_elements_by_xpath(xpath):
    img_url = element.get_attribute(&apos;src&apos;)
    img_desc = element.get_attribute(&apos;data-desc&apos;)

    action = ActionChains(driver).move_to_element(element)
    action.context_click(element)
    action.send_keys(Keys.ARROW_DOWN)
    action.send_keys(&apos;v&apos;)
    action.perform()

print(&apos;Well done! all pictures downloaded.&apos;)
print(&apos;---------------------------------&apos;)

# driver.close()
</code></pre><p>6、如果图片数量少，人工保存下也无妨，但数量大肯定不行，所以还是用常规的自动写入数据保存的方式。另外此网站是通过ajax异步加载，当鼠标放到首页30个图的下方，也就是浏览器底部区域，他又会自动加载新的图片出来。而通过新的第一层代码的模拟鼠标下移，可以得到更多的图片。<strong>千万要注意的是，在python 3中，对数dict的关键词查找是in，比如：if n in previous:而在python 2 中是has_key,比如：if previous.has_key(n):</strong>对应代码为：</p>
<pre><code># python 3.5.2
from selenium import webdriver  
import time  
import urllib


# 爬取页面地址  
url = &quot;https://stocksnap.io/&quot;

# 目标元素的xpath  
xpath = &quot;/html/body/div[4]/div[3]/div[2]/div//a/img&quot;

# 启动Firefox浏览器  
driver = webdriver.Firefox()  

# 最大化窗口，因为每一次爬取只能看到视窗内的图片  
driver.maximize_window()  

# 记录下载过的图片地址，避免重复下载  
img_url_dic = {}  

# 浏览器打开爬取页面  
driver.get(url)  

# 模拟滚动窗口以浏览下载更多图片  
pos = 0  
m = 0 # 图片编号  
for i in range(10):  
    pos += i*500 # 每次下滚500  
    js = &quot;document.documentElement.scrollTop=%d&quot; % pos  
    driver.execute_script(js)  
    time.sleep(1)     

    for element in driver.find_elements_by_xpath(xpath):  
        img_url = element.get_attribute(&apos;src&apos;)  
        # 保存图片到指定路径  
        if img_url != None and not img_url in img_url_dic:

            img_url_dic[img_url] = &apos;&apos;  
            m += 1  
            ext = img_url.split(&apos;.&apos;)[-1]  
            filename = str(m) + &apos;.&apos; + ext  
            #保存图片数据  
            data = urllib.request.urlopen(img_url).read()
            f = open(&apos;./van/&apos; + filename, &apos;wb&apos;)
            f.write(data)  
            f.close()  
driver.close()  
</code></pre><p>7、结果展示：相对第一个方法，获取了更多的图片：<br><img src="http://ocg7i7pt6.bkt.clouddn.com/stocksnap%20ajax%20more%20pics.png" alt=""></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/09/17/基于Selenium一键写CSDN博客并做成exe文件/" itemprop="url">
                  基于Selenium一键写CSDN博客并做成exe文件【python】
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Veröffentlicht am</span>
            <time itemprop="dateCreated" datetime="2016-09-17T11:59:28+08:00" content="2016-09-17">
              2016-09-17
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="参考：原文1和原文2"><a href="#参考：原文1和原文2" class="headerlink" title="参考：原文1和原文2"></a>参考：<a href="http://www.jianshu.com/p/a81800774d91" target="_blank" rel="external">原文1</a>和<a href="http://blog.csdn.net/mrlevo520/article/details/51840217" target="_blank" rel="external">原文2</a></h1><p>1、代码，原文python2，修改成了python 3，作者信息保留，请修改CSDN帐号登录信息：</p>
<pre><code># -*- coding: utf-8 -*-
# python 3.5.2
#Author:哈士奇说喵
#CSDN--实现一键写博客

from selenium import webdriver
import time

#shift-tab多行缩进(左)
print (&apos;Please wait...Firefox loading...&apos;)
print (&apos;---------------------------------&apos;)
#reload(sys)

PostUrl = &quot;https://passport.csdn.net/account/login?from=http://my.csdn.net/my/mycsdn&quot;
driver=webdriver.Firefox()#用浏览器实现访问
#driver = webdriver.PhantomJS(executable_path=&quot;phantomjs.exe&quot;)#没用浏览器
driver.get(PostUrl)


#账号填充输入
elem_user = driver.find_element_by_id(&apos;username&apos;)
elem_psw = driver.find_element_by_id(&apos;password&apos;)

#可以自己修改登录名和账户密码，我自己的隐去了
elem_user.send_keys(&apos;41111111@qq.com&apos;)
elem_psw.send_keys(&apos;1111111&apos;)


#点击登录
#click_login = driver.find_element_by_xpath(&quot;//input[@class=&apos;logging&apos;]&quot;)
click_login = driver.find_element_by_xpath(&quot;//input[@class=&apos;logging&apos;]&quot;)
click_login.click()
print( &apos;log in...&apos;)
print (&apos;---------------------------------&apos;)
time.sleep(1)

#先点击写博客图标，不然元素隐藏

click_wbic = driver.find_element_by_xpath(&quot;//ul[@class=&apos;btns&apos;]/li[5]&quot;)
click_wbic.click()
print (&apos;jumping...&apos;)
print (&apos;---------------------------------&apos;)


click_choice = driver.find_element_by_xpath(&quot;//div[@class=&apos;wrap clearfix&apos;]/dl/dt[4]/a&quot;)
click_choice.click()#将点击操作放在内部比较好
#定位新页面元素，将handle重定位即可
driver.switch_to_window(driver.window_handles[1])#定位弹出的第一个页面，也就是当前页面
click_markdown = driver.find_element_by_xpath(&quot;//p[@class=&apos;subtit&apos;]/a&quot;)
click_markdown.click()
print (&apos;---------------------------------&apos;)
print (&apos;here we go!&apos;)
#关闭无关页面，也可以根据自己喜好保留无关页面
driver.close()#关闭第二个页面，也就是一般编辑器下的CSDN
driver.switch_to_window(driver.window_handles[0])#关闭第1个页面，也就是登录主页
driver.close()
</code></pre><p>2、代码分析：<br>一些按键的定位是使用了xpath的编写方式，以“登录”为例，右键，检查，然后找到最近的一个class，因为这个登录直接在一个class 里，如下代码：</p>
<pre><code>&lt;input class=&quot;logging&quot; accesskey=&quot;l&quot; value=&quot;登 录&quot; tabindex=&quot;6&quot; type=&quot;button&quot; data-form-sbm=&quot;1474112762583.3015&quot;&gt;
</code></pre><p>所以根据xpath的路径规则，是 //input[@class=’logging’]，然后，配合selenium的xpath选定语法，写为：</p>
<pre><code>click_login = driver.find_element_by_xpath(&quot;//input[@class=&apos;logging&apos;]&quot;)
</code></pre><p>而写博客的按键，有所小复杂，因为检查的时候，不是直接在类里，需要向上寻找一个父类，注意不是平行的类，因此代码相对复杂点，形如：</p>
<pre><code>click_wbic = driver.find_element_by_xpath(&quot;//ul[@class=&apos;btns&apos;]/li[5]&quot;)

click_choice = driver.find_element_by_xpath(&quot;//div[@class=&apos;wrap clearfix&apos;]/dl/dt[4]/a&quot;)

click_markdown = driver.find_element_by_xpath(&quot;//p[@class=&apos;subtit&apos;]/a&quot;)
</code></pre><p>3、xpath的路径分析是直接通过检查中的代码换成xpath模式，而不是右键 copy–&gt; copy xpath模式。</p>
<p>4、做成exe登录更简单些。</p>
<p>5、全文需要安装firefox，为了保证和selenium兼容，我的是46英文版。</p>
<p>6、又查阅了其他人的xpath分析方法，发现不一定要通过class来区分，如下：</p>
<pre><code>&lt;input type=&quot;text&quot; name=&quot;passwd&quot; id=&quot;passwd-id&quot; /&gt;
</code></pre><p>这段html语言，可以用以下的几种方法来定位。</p>
<pre><code>element = driver.find_element_by_id(&quot;passwd-id&quot;)
element = driver.find_element_by_name(&quot;passwd&quot;)
element = driver.find_elements_by_tag_name(&quot;input&quot;)
element = driver.find_element_by_xpath(&quot;//input[@id=&apos;passwd-id&apos;]&quot;)
</code></pre><p>而且你在用 xpath 的时候还需要注意的是，如果有多个元素匹配了 xpath，它只会返回第一个匹配的元素。如果没有找到，那么会抛出 NoSuchElementException 的异常。</p>
<p>7、由于firefox的加载比较慢，所以想通过PhantomJS来测试，但是发现他不识别既定的xpath路径，应该说是部分不识别，开头的还是识别的，中间的报错，坑爹啊。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/09/17/Firefox不能运行selenium的故障排查/" itemprop="url">
                  Firefox不能运行selenium的故障排查
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Veröffentlicht am</span>
            <time itemprop="dateCreated" datetime="2016-09-17T11:59:28+08:00" content="2016-09-17">
              2016-09-17
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="说明-Firefox中文最新版48不能运行selenium2-53-6的故障排查"><a href="#说明-Firefox中文最新版48不能运行selenium2-53-6的故障排查" class="headerlink" title="说明:Firefox中文最新版48不能运行selenium2.53.6的故障排查"></a>说明:Firefox中文最新版48不能运行selenium2.53.6的故障排查</h1><p>1、错误提示：  </p>
<pre><code>  File &quot;D:\Anaconda3\lib\site-packages\selenium\webdriver\firefox\firefox_binary.py&quot;, line 103, in _wait_until_connectable
    raise WebDriverException(&quot;Can&apos;t load the profile. Profile &quot;
selenium.common.exceptions.WebDriverException: Message: Can&apos;t load the profile. Profile Dir: %s If you specified a log_file in the FirefoxBinary constructor, check it for details.
</code></pre><p>2、排查版本： selenium已经是最新版2.53.6  firexfox也是最新中文版48</p>
<p>谷歌： </p>
<pre><code>http://stackoverflow.com/questions/37693106/selenium-2-53-not-working-on-firefox-47/37693374
</code></pre><p>上面给了一个在selenium 在2.53.0时代，运行firefox 47版本的方法，感觉麻烦，所以我优选了退回低版本的策略，如下：</p>
<p>3、评估是否firefox的48最新版还不支持？所以到firefox官网上，直接看到的最后的老版本是47， 测试安装了中文版，结果一样，继续卸载看46版本，使用的以下网址：</p>
<pre><code>https://ftp.mozilla.org/pub/firefox/releases/
https://ftp.mozilla.org/pub/firefox/releases/46.0.1/win64/en-US/
</code></pre><p>一下就成功了。</p>
<p>4、后来翻阅stackoverflow的帖子，发现有人反馈2.53.6已经支持firefox 48了。 但是我的咋就不行？ 猜测区别就是中文和英文，所以我就把46版本卸载了测试48英文版， </p>
<pre><code>https://ftp.mozilla.org/pub/firefox/releases/48.0b1/win64/en-US/
</code></pre><p>结果直接跳出firefox已停止工作。</p>
<p>5、结论：就2.53.6的selenium版本，测试firefox 46 英文版稳定运行。47和48版本失败。操作系统Win10，所以建议使用46版本的firefox。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/09/16/selenium+PhantomJS 抓取斗鱼房间信息/" itemprop="url">
                  selenium+PhantomJS 抓取斗鱼房间信息【python】
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Veröffentlicht am</span>
            <time itemprop="dateCreated" datetime="2016-09-16T12:59:28+08:00" content="2016-09-16">
              2016-09-16
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="说明原文连接【点我】"><a href="#说明原文连接【点我】" class="headerlink" title="说明原文连接【点我】"></a><a href="http://www.jianshu.com/p/520749be7377" target="_blank" rel="external">说明原文连接【点我】</a></h1><p>1、环境：selenium+PhantomJS<br>   selenium 可直接安装 ： pip install -U selenium<br>   PhantomJS： 官网下载，设置对应bin的路径为环境变量。</p>
<p>2、本文对原作的代码进行了改编，去掉了单元测试部分，因为重点是研究selenium+PhantomJS 抓取异步数据，最后把数据通过mysql导出。</p>
<p>3、代码（）：</p>
<pre><code>#coding:utf-8
# python 3.5.2


from selenium import webdriver
from bs4 import BeautifulSoup
import pymysql.cursors



class Test(object):
    def __init__(self):
        pass



    def begin_run(self):

        driver = webdriver.PhantomJS()
        driver.get(&apos;http://www.douyu.com/directory/all&apos;)
        soup = BeautifulSoup(driver.page_source, &apos;xml&apos;)
        while True:
            titles = soup.find_all(&apos;h3&apos;, {&apos;class&apos;: &apos;ellipsis&apos;})
            # soup.find_all(&apos;h3&apos;, {class_=&apos;ellipsis&apos;})
            nums = soup.find_all(&apos;span&apos;, {&apos;class&apos;: &apos;dy-num fr&apos;})
            for title, num in zip(titles, nums):
                data = {
                    &apos;房间名&apos;: title.get_text(),
                    &apos;观看数量&apos;: num.get_text()
                }
                connection = pymysql.connect(host=&apos;localhost&apos;,
                                     user=&apos;root&apos;,
                                     password=&apos;xxxx&apos;,
                                     db=&apos;douyu&apos;,
                                     charset=&apos;utf8&apos;
                                     )

                try:
                    # 创建会话指针
                    with connection.cursor() as cursor:
                        # 创建sql语句
                        sql = &apos;insert into `douyu1` (`房间名`,`观看数量`) values(%s, %s)&apos;
                        # 执行sql语句
                        cursor.execute(sql, (data[&apos;房间名&apos;], data[&apos;观看数量&apos;]))

                        # 提交
                        connection.commit()
                finally:
                    connection.close()
            if driver.page_source.find(&apos;shark-pager-disable-next&apos;) != -1:
                break
            elem = driver.find_element_by_class_name(&apos;shark-pager-next&apos;)
            elem.click()
            soup = BeautifulSoup(driver.page_source, &apos;xml&apos;)

    def start(self):
        Test().begin_run()

if __name__ == &quot;__main__&quot;:
    test = Test()
    test.start()
</code></pre><p>4、其中的坑：<br>   如果默认使用webdriver的“.”查找功能，比如很可能你突然忘记PhantomJS的拼写了，那么查阅回车后，得到的是PhantomJS，注意不是PhantomJS() ，就会导致报错如下：</p>
<pre><code>Error
Traceback (most recent call last):
  File &quot;C:\Users\Administrator\PycharmProjects\untitled\readpdf.py&quot;, line 13, in testEle
    driver.get(&apos;http://www.douyu.com/directory/all&apos;)
TypeError: get() missing 1 required positional argument: &apos;url&apos;
</code></pre><p>如果我们分别type他们的信息：</p>
<pre><code>print(type(webdriver.PhantomJS))

print(type(webdriver.PhantomJS()))
</code></pre><p>得到如下结果：</p>
<pre><code>&lt;class &apos;type&apos;&gt;

&lt;class &apos;selenium.webdriver.phantomjs.webdriver.WebDriver&apos;&gt;
</code></pre><p>也就是说，加了括号，表示得到的是selenium.webdriver.phantomjs.webdriver.WebDriver的具体对象。</p>
<p>5、测试结果：<br>～天天游戏，天天厨房～  观看数量比较威猛，</p>
<p><img src="http://ocg7i7pt6.bkt.clouddn.com/douyu%20selenium.png" alt=""></p>
<p>6、原作加入了单元测试的写法，如果有兴趣，可以看下<a href="http://blog.csdn.net/hackerain/article/details/24095117" target="_blank" rel="external">另一篇文章【点我】</a>，对unittest做了有点详细的解释说明。</p>
<p>7、总结：本文学习了 的基本使用方法，另外必须强调的是翻页方面的操作，就斗鱼来说，在总网址下，翻页时在浏览器的地址是不变的，而之前抓取的网站都是跟着翻页变更地址的，形如xxx/1  xxx/2  这样的， 那么必须重点看下源代码的以下几句：</p>
<pre><code>if driver.page_source.find(&apos;shark-pager-disable-next&apos;) != -1:
    break
elem = driver.find_element_by_class_name(&apos;shark-pager-next&apos;)
elem.click()
soup = BeautifulSoup(driver.page_source, &apos;xml&apos;)
</code></pre><p>其中if语句判断 没有下一页了 ，抓取就结束，不然，就通过模拟点击到下一页继续抓取数据。 另外，“下一页”是在名为shark-pager-next的一个类里的，所以那么写，而当到了38页，也就是最后一页后，那个“下一页”的图标就灰色了，通过查看代码，里面是在一个含有shark-pager-disable-next字样的类里</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/09/16/使用json和mongo进行果壳网异步数据的爬取/" itemprop="url">
                  使用json和mongo进行果壳网异步数据的爬取【python】
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Veröffentlicht am</span>
            <time itemprop="dateCreated" datetime="2016-09-16T10:59:28+08:00" content="2016-09-16">
              2016-09-16
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="说明原文连接【点我】"><a href="#说明原文连接【点我】" class="headerlink" title="说明原文连接【点我】"></a><a href="http://www.jianshu.com/p/6002ef3434fd" target="_blank" rel="external">说明原文连接【点我】</a></h1><p>1、在原文基础上稍作修改，增加了mysql数据库的代码，但是在数据库中的显示结果不理想，主要是其格式不确定怎么设置。</p>
<p>2、本文亮点使用json分析了异步数据，用的方法比较麻烦，但是起作用，具体为通过firefox的网络+XHR功能，鼠标逐渐往下拉，来得到get请求，查看右侧的结果，以及找到步进规律和对应的json网址，如下图：</p>
<p><img src="http://ocg7i7pt6.bkt.clouddn.com/firefox%20%E7%BD%91%E7%BB%9C%20xhr.png" alt=""></p>
<p>3、配置：需要安装mongodb,并配置,同时建议安装其图形化工具robomongo，会方便不少，， mongo代码：</p>
<pre><code>#coding:utf-8
from bs4 import BeautifulSoup
import requests
import json
import pymongo

url = &apos;http://www.guokr.com/scientific/&apos;

def dealData(url):
    client = pymongo.MongoClient(&apos;localhost&apos;, 27017)
    guoke = client[&apos;guoke&apos;]
    guokeData = guoke[&apos;guokeData&apos;]
    web_data = requests.get(url)
    datas = json.loads(web_data.text)
    print(datas.keys())
    for data in datas[&apos;result&apos;]:
        guokeData.insert_one(data)

def start():
    urls = [&apos;http://www.guokr.com/apis/minisite/article.json?retrieve_type=by_subject&amp;limit=20&amp;offset={}&amp;_=1462252453410&apos;.format(str(i)) for i in range(20, 100, 20)]
    for url in urls:
        dealData(url)

start()
</code></pre><p>4、输出结果到robomongo，如图：</p>
<p><img src="http://ocg7i7pt6.bkt.clouddn.com/robomongo%20guoke.png" alt=""></p>
<p>5、使用mysql数据库，由于不确定输出的result应该用什么格式，我就用了varchar，但是这把一个列表都放到varchar中，导致长度非常长，看起来也很不爽，仅做记录。代码：</p>
<pre><code>#coding:utf-8
from bs4 import BeautifulSoup
import requests
import json
import pymysql.cursors

url = &apos;http://www.guokr.com/scientific/&apos;

def dealData(url):
    # client = pymongo.MongoClient(&apos;localhost&apos;, 27017)
    # guoke = client[&apos;guoke&apos;]
    # guokeData = guoke[&apos;guokeData&apos;]

    web_data = requests.get(url)
    datas = json.loads(web_data.text)
    print(datas.keys())
    for data in datas[&apos;result&apos;]:
        #guokeData.insert_one(data)
        print(data)
        print (type(data))
        connection = pymysql.connect(host=&apos;localhost&apos;,
                                     user=&apos;root&apos;,
                                     password=&apos;XXXX&apos;,
                                     db=&apos;guoke&apos;,
                                     charset=&apos;utf8&apos;
                                     )

        try:
            # 创建会话指针
            with connection.cursor() as cursor:
                # 创建sql语句
                sql = &apos;insert into `guoke1` (`result`) values(%s)&apos;
                # 执行sql语句
                cursor.execute(sql, (str(data)))

                # 提交
                connection.commit()
        finally:
            connection.close()

def start():
    urls = [&apos;http://www.guokr.com/apis/minisite/article.json?retrieve_type=by_subject&amp;limit=20&amp;offset={}&amp;_=1462252453410&apos;.format(str(i)) for i in range(20, 100, 20)]
    for url in urls:

        dealData(url)

start()
</code></pre><p>6、mysql数据显示，如下图，比较杂乱：</p>
<p><img src="http://ocg7i7pt6.bkt.clouddn.com/mysql%20guoke.png" alt=""></p>
<p>7、总结：异步加载的网页爬取有所麻烦，后续会使用selenium工具来相对方便的操作， 同时，可以感觉到mongodb有其特色，</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/09/15/爬取瓜子网上海地区二手车的信息/" itemprop="url">
                  爬取瓜子网上海地区二手车的信息【python】
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Veröffentlicht am</span>
            <time itemprop="dateCreated" datetime="2016-09-15T18:59:28+08:00" content="2016-09-15">
              2016-09-15
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="说明原文连接【点我】"><a href="#说明原文连接【点我】" class="headerlink" title="说明原文连接【点我】"></a><a href="http://www.jianshu.com/p/15eb2a39ffe0" target="_blank" rel="external">说明原文连接【点我】</a></h1><p>1、在原文基础上稍作修改，目标地区为上海，更改了网址显示和价格显示部分。并增加了数据库导入，请把密码XXXX修改为你的密码。</p>
<p>2、本文亮点在网址分析处，使用了CSS path，目前在谷歌浏览器下显示为Copy selector ，这和原文的Copy CSS Path是不同的，就本文，这个分析方法完全避开了正则式，从中，也体现了bs4的强大。</p>
<p>3、代码：</p>
<pre><code># -*- coding: utf-8 -*-
# python 3.5.2

from bs4 import BeautifulSoup
import requests
import pymysql.cursors

def detailOper(url):
    web_data = requests.get(url)
    # soup = BeautifulSoup(web_data.text, &apos;lxml&apos;)
    soup = BeautifulSoup(web_data.text, &apos;html.parser&apos;)

    # body &gt; div.w &gt; div.list &gt; ul &gt; li:nth-child(1) &gt; div &gt; p.infoBox &gt; a
    # above is the css path , or copy selector in Google Chrome F12
    # we can delete &quot;body &gt; div.w &gt;&quot; as the whole site under Class = &apos;w&apos;
    # modify it for all cars info: div.list &gt; ul &gt; li &gt; div &gt; p.infoBox &gt; a
    titles = soup.select(&apos;div.list &gt; ul &gt; li &gt; div &gt; p.infoBox &gt; a&apos;)

    # get price info with F12: body &gt; div.w &gt; div.list &gt; ul &gt; li:nth-child(1) &gt; div &gt; p.priType-s &gt; span &gt; i
    # modify it for all price: div.list &gt; ul &gt; li &gt; div &gt; p.priType-s &gt; span &gt; i
    prices = soup.select(&apos;div.list &gt; ul &gt; li &gt; div &gt; p.priType-s &gt; span &gt; i&apos;)
    for title, price in zip(titles, prices):
        data = {
        &apos;车型&apos;: title.get_text(),
        &apos;网址&apos;: &apos;http://www.guazi.com&apos;+ title.get(&apos;href&apos;),
        #&apos;price&apos;:price.get_text().replace(u&apos;万&apos;, &apos;&apos;).replace(&apos; &apos;, &apos;&apos;)
        &apos;价格&apos;: price.get_text().replace(&apos;\n&apos;, &apos;&apos;).replace(&apos; &apos;, &apos;&apos;)
    }

        connection = pymysql.connect(host=&apos;localhost&apos;,
                                     user=&apos;root&apos;,
                                     password=&apos;XXXX&apos;,
                                     db=&apos;guazi&apos;,
                                     charset=&apos;utf8&apos;
                                     )

        try:
            # 创建会话指针
            with connection.cursor() as cursor:
                # 创建sql语句
                sql = &apos;insert into `guazi1` (`车型`, `价格`, `网址`) values(%s, %s, %s)&apos;
                # 执行sql语句
                cursor.execute(sql, (data[&apos;车型&apos;], data[&apos;价格&apos;],data[&apos;网址&apos;]))

                # 提交
                connection.commit()

        finally:
            connection.close()
    # print(data)

def start():
    urls = [&apos;http://www.guazi.com/sh/buy/o{}/&apos;.format(str(i)) for i in range(1, 51, 1)]
    for url in urls:
        detailOper(url)

if __name__ == &apos;__main__&apos;:
    start()
</code></pre><p>4、部分结果展示：</p>
<pre><code>{&apos;车型&apos;: &apos;东南V5菱致 2013款 1.5 手动 舒适型CNG&apos;, &apos;价格&apos;: &apos;已降价&apos;, &apos;网址&apos;: &apos;http://www.guazi.com/sh/3000216790x.htm&apos;}
{&apos;车型&apos;: &apos;福特蒙迪欧 2013款 致胜 2.3 自动 时尚型&apos;, &apos;价格&apos;: &apos;9.00万&apos;, &apos;网址&apos;: &apos;http://www.guazi.com/sh/3000237200x.htm&apos;}
{&apos;车型&apos;: &apos;斯柯达明锐 2010款 明锐 1.6 手动 逸致版&apos;, &apos;价格&apos;: &apos;3.80万&apos;, &apos;网址&apos;: &apos;http://www.guazi.com/sh/3000222224x.htm&apos;}
{&apos;车型&apos;: &apos;大众速腾 2014款 速腾 1.4TSI 手动 豪华型&apos;, &apos;价格&apos;: &apos;已降价&apos;, &apos;网址&apos;: &apos;http://www.guazi.com/sh/3000415677x.htm&apos;}
{&apos;车型&apos;: &apos;福特蒙迪欧 2011款 蒙迪欧致胜 2.3 自动 时尚型&apos;, &apos;价格&apos;: &apos;3.99万&apos;, &apos;网址&apos;: &apos;http://www.guazi.com/sh/3000218992x.htm&apos;}
{&apos;车型&apos;: &apos;大众CC2015款 CC 1.8TSI 双离合 豪华型&apos;, &apos;价格&apos;: &apos;已降价&apos;, &apos;网址&apos;: &apos;http://www.guazi.com/sh/3000431976x.htm&apos;}
{&apos;车型&apos;: &apos;大众Polo2014款 1.6 自动 舒适版&apos;, &apos;价格&apos;: &apos;19.50万&apos;, &apos;网址&apos;: &apos;http://www.guazi.com/sh/3000428176x.htm&apos;}
{&apos;车型&apos;: &apos;吉利GX7 2013款 1.8 手动 尊贵型&apos;, &apos;价格&apos;: &apos;已降价&apos;, &apos;网址&apos;: &apos;http://www.guazi.com/sh/3000215362x.htm&apos;}
{&apos;车型&apos;: &apos;标致3008 2013款 1.6THP 自动 至尚版&apos;, &apos;价格&apos;: &apos;8.10万&apos;, &apos;网址&apos;: &apos;http://www.guazi.com/sh/3000240867x.htm&apos;}
{&apos;车型&apos;: &apos;比亚迪F3 2010款 1.5 手动 新白金版 GLX-i 豪华型&apos;, &apos;价格&apos;: &apos;4.50万&apos;, &apos;网址&apos;: &apos;http://www.guazi.com/sh/3000205737x.htm&apos;}
{&apos;车型&apos;: &apos;五菱荣光S 2014款 1.2 手动 标准型7-8座&apos;, &apos;价格&apos;: &apos;9.98万&apos;, &apos;网址&apos;: &apos;http://www.guazi.com/sh/3000243019x.htm&apos;}
</code></pre><p>5、导入到了mysql中，看起来更方便。如果需要对数据进一步操作，可以在数据库中进行相关排序，如下图：<br>   <img src="http://ocg7i7pt6.bkt.clouddn.com/guazi%20shanghai.png" alt=""></p>
<p>6、本文可拓展点： 有的价格抓取是 已降价，但不知道降价后的价格，  只抓了上海地区的，如果是想全国地区的，研究了网页代码，css path都是body &gt; div.header &gt; div.hd-top.clearfix &gt; div.c2city &gt; a &gt; span  里面没有具体的城市名字，也许是用ajax的？　以后会了再更新。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/09/15/win10中python3.5.2遇到lxml安装问题的记录/" itemprop="url">
                  win10中python3.5.2遇到lxml安装问题缺少vcvarsall.bat的解决方案和记录【python】
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Veröffentlicht am</span>
            <time itemprop="dateCreated" datetime="2016-09-15T11:59:28+08:00" content="2016-09-15">
              2016-09-15
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="说明-win10中python3-5-2遇到lxml安装问题的记录"><a href="#说明-win10中python3-5-2遇到lxml安装问题的记录" class="headerlink" title="说明:win10中python3.5.2遇到lxml安装问题的记录"></a>说明:win10中python3.5.2遇到lxml安装问题的记录</h1><p>1、错误提示：  error: Unable to find vcvarsall.bat</p>
<p>2、由于在python 2.7 和win7下遇到过类似的问题，当时用安装wheel文件的方法，不过这次失败， 另外在python2.7时代，是有一个微软的支持版本的： Microsoft Visual C++ Compiler for Python 2.7,只有2.7。</p>
<p>3、该问题windows环境中，python 的 Setup需要调用一个vcvarsall.bat的文件，该文件需要安装c++编程环境才会有，经查知乎上有人给出了答案，由于python 3.5使用的SDK环境和3.4是不同的，所以需要安装VC++最新版，或者VS2015 添加VC和python相关组件， ，请参考：<a href="http://www.zhihu.com/question/26857761" target="_blank" rel="external">http://www.zhihu.com/question/26857761</a> 里面的“代代树”的回复。但是第一次安装遇到retry的失败，关闭了相关VS的进程，重新装，貌似就可以安装了，但安装完毕后，问题没有解决，依然存在啊，什么鬼？</p>
<p><img src="https://pic2.zhimg.com/583990ef6536de3763fea804fa5a44fd_b.png" alt=""></p>
<p>4、然后又发现别人推荐了<a href="https://anaconda.org/或者https://www.continuum.io/downloads的Anaconda安装解决方式，一看官网支持python最新版，这就好感爆棚了。果断下载安装，不过结果依然失败，有点闹腾。又是什么鬼？" target="_blank" rel="external">https://anaconda.org/或者https://www.continuum.io/downloads的Anaconda安装解决方式，一看官网支持python最新版，这就好感爆棚了。果断下载安装，不过结果依然失败，有点闹腾。又是什么鬼？</a><br>   原来，漏看了一句，安装后，还需要通过conda install lxml命令，开始还以为他支持python3.5.2是自带了这些安装包，理解错误，</p>
<p><img src="http://ocg7i7pt6.bkt.clouddn.com/conda%20lxml.png" alt=""></p>
<p>然后通过λ pip install -U lxml<br>Requirement already up-to-date: lxml in d:\anaconda3\lib\site-packages</p>
<p>发现是把lxml安装在d:\anaconda3\lib\site-packages<br>所以我尝试把这个lxml文件夹复制到python安装目录对应的site-packages<br>然后测试原本在pycharm提示缺少lxml库的，继续运行，跳出了需要的结果界面。这下终于搞定了。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/09/15/推荐使用requests代替urllib/" itemprop="url">
                  推荐使用requests代替urllib【python】
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Veröffentlicht am</span>
            <time itemprop="dateCreated" datetime="2016-09-15T09:59:28+08:00" content="2016-09-15">
              2016-09-15
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="说明-之前爬虫时读取网址内容用的都是urllib，这是python自带的，但如果使用复杂点的功能，还是推荐requests第三方库，这是因为这个库很优秀"><a href="#说明-之前爬虫时读取网址内容用的都是urllib，这是python自带的，但如果使用复杂点的功能，还是推荐requests第三方库，这是因为这个库很优秀" class="headerlink" title="说明:之前爬虫时读取网址内容用的都是urllib，这是python自带的，但如果使用复杂点的功能，还是推荐requests第三方库，这是因为这个库很优秀"></a>说明:之前爬虫时读取网址内容用的都是urllib，这是python自带的，但如果使用复杂点的功能，还是推荐requests第三方库，这是因为这个库很优秀</h1><p>1、比较，参考自： <a href="http://www.jb51.net/article/63711.htm：" target="_blank" rel="external">http://www.jb51.net/article/63711.htm：</a></p>
<blockquote>
<p>一个复杂一点的例子：<br>现在让我们尝试下复杂点得例子：使用GET方法获取<a href="http://foo.test/secret的资源，这次需要基本的http验证。使用上面的代码作为模板，好像我们只要把urllib2.urlopen(" target="_blank" rel="external">http://foo.test/secret的资源，这次需要基本的http验证。使用上面的代码作为模板，好像我们只要把urllib2.urlopen(</a>) 到requests.get()之间的代码换成可以发送username，password的请求就行了<br>这是urllib2的方法：</p>
<blockquote>
<blockquote>
<p>import urllib2<br>url = ‘<a href="http://example.test/secret" target="_blank" rel="external">http://example.test/secret</a>‘<br>password_manager = urllib2.HTTPPasswordMgrWithDefaultRealm()<br>password_manager.add_password(None, url, ‘dan’, ‘h0tdish’)<br>auth_handler = urllib2.HTTPBasicAuthHandler(password_manager)<br>opener = urllib2.build_opener(auth_handler)<br>urllib2.install_opener(opener)<br>response = urllib2.urlopen(url)<br>response.getcode()<br>200<br>response.read()<br>‘Welcome to the secret page!’<br>一个简单的方法中实例化了2个类，然后组建了第三个类，最后还要装载到全局的urllib2模块中，最后才调用了urlopen，那么那两个复杂的类是什么的<br>迷惑了吗，  这里所有urllib2的文档 <a href="http://docs.python.org/release/2.7/library/urllib2.html" target="_blank" rel="external">http://docs.python.org/release/2.7/library/urllib2.html</a></p>
</blockquote>
</blockquote>
<p>那Requests是怎么样解决同样的问题的呢？<br>Requests</p>
<blockquote>
<blockquote>
<p>import requests<br>url = ‘<a href="http://example.test/secret" target="_blank" rel="external">http://example.test/secret</a>‘<br>response = requests.get(url,auth=(‘dan’,’h0tdish’))<br>response.status_code<br>200<br>response.content<br>u’Welcome to the secret page!’<br>只是在调用方法的时候增加了一个auth关键字函数</p>
</blockquote>
</blockquote>
</blockquote>
<p>2、requests的documents官网：  <a href="http://www.python-requests.org/en/master/" target="_blank" rel="external">http://www.python-requests.org/en/master/</a><br>建议阅读下其documents，写的还是挺清楚的。</p>
<p>3、基本使用：<br>import requests<br>r = requests.get(‘<a href="https://api.github.com/events" target="_blank" rel="external">https://api.github.com/events</a>‘)<br>r.text<br>r.content   # 以字节的方式访问请求响应体，对于非文本请求</p>
<p>r = requests.post(‘<a href="http://httpbin.org/post" target="_blank" rel="external">http://httpbin.org/post</a>‘, data = {‘key’:’value’})</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/09/14/百度百科1000个页面数据的抓取【python】/" itemprop="url">
                  百度百科1000个页面数据的抓取【python】
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Veröffentlicht am</span>
            <time itemprop="dateCreated" datetime="2016-09-14T21:59:28+08:00" content="2016-09-14">
              2016-09-14
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="说明：以下内容学习自：http-www-imooc-com"><a href="#说明：以下内容学习自：http-www-imooc-com" class="headerlink" title="说明：以下内容学习自：http://www.imooc.com/"></a>说明：以下内容学习自：<a href="http://www.imooc.com/learn/563" target="_blank" rel="external">http://www.imooc.com/</a></h1><p>1、首先要感谢原作者非常细致的讲解，在网页分析的时候还是学到了一些很有用的技巧。后续会总结。</p>
<p>2、这次是跟着作者，使用了Eclipse + Pydev的软件调试的，相对于Pycharm，他的优点就是可以动态插入需要的类和方法，而同比Pycharm只能动态匹配类，方法就没反应了。不过在界面配色方面明显没有Pyhcarm做的好。总之，目前的配色方案不满意，但将就下吧。</p>
<p>3、原作者使用的Python 2的版本，我的版本（大部分还是照作者的）已经修改到3的版本了，。虽然感觉作者的代码，有的地方写的有点小啰嗦，不过看他视频现场操作，还是感觉细节有很多值得借鉴的地方。</p>
<pre><code># -*- coding: utf-8 -*-
# python 3.5.2

from urllib.request import urlopen
from bs4 import BeautifulSoup
from urllib.parse import urlparse
import re
import urllib


class HtmlOutputer(object):
    def __init__(self):
        self.datas = []

    def collect_data(self, data):
        if data is None:
            return
        self.datas.append(data)

    def outputer_html(self):
        fout = open(&apos;output.html&apos;, &apos;w&apos;)
        fout.write(&quot;&lt;html&gt;&quot;)
        fout.write(&quot;&lt;body&gt;&quot;)
        fout.write(&quot;&lt;table&gt;&quot;)

        # accii
        for data in self.datas:
            fout.write(&quot;&lt;tr&gt;&quot;)
            fout.write(&quot;&lt;td&gt;%s&lt;/td&gt;&quot; % data[&apos;url&apos;])
            # 原作者使用了.encode(&apos;utf-8&apos;)的代码，但在python 3 测试报错
            fout.write(&quot;&lt;td&gt;%s&lt;/td&gt;&quot; % data[&apos;title&apos;]).encode(&apos;utf-8&apos;)
            fout.write(&quot;&lt;td&gt;%s&lt;/td&gt;&quot; % data[&apos;summary&apos;]).encode(&apos;utf-8&apos;)

            fout.write(&quot;&lt;/tr&gt;&quot;)

        fout.write(&quot;&lt;/body&gt;&quot;)
        fout.write(&quot;&lt;/table&gt;&quot;)
        fout.write(&quot;&lt;/html&gt;&quot;)
        fout.close()


class HtmlParser(object):
    def _get_new_urls(self, page_url, soup):
        new_urls = set()
        # /view/123.htm
        links = soup.find_all(&apos;a&apos;, href=re.compile(r&apos;/view/\d+\.htm&apos;))
        for link in links:
            new_url = link[&apos;href&apos;]

            new_full_url = urllib.parse.urljoin(page_url, new_url)
            new_urls.add(new_full_url)
        return new_urls

    def _get_new_data(self, page_url, soup):
        # http://baike.baidu.com/view/21087.htm
        res_data = {}
        res_data[&apos;url&apos;] = page_url
        # &lt;dd class=&quot;lemmaWgt-lemmaTitle-title&quot;&gt;&lt;h1&gt;Python&lt;/h1&gt;
        title_node = soup.find(&apos;dd&apos;, class_=&quot;lemmaWgt-lemmaTitle-title&quot;).find(&quot;h1&quot;)
        res_data[&apos;title&apos;] = title_node.get_text()

        # &lt;div class=&quot;lemma-summary&quot; label-module=&quot;lemmaSummary&quot;&gt;
        summery_node = soup.find(&apos;div&apos;, class_=&quot;lemma-summary&quot;)
        res_data[&apos;summery&apos;] = summery_node.get_text()

        return res_data

    def parse(self, page_url, html_cont):
        if page_url is None or html_cont is None:
            return
        soup = BeautifulSoup(html_cont, &apos;html.parser&apos;, from_encoding=&apos;uft-8&apos;)
        new_urls = self._get_new_urls(page_url, soup)
        new_data = self._get_new_data(page_url, soup)

        return new_urls, new_data


class HtmlDownloader(object):
    def download(self, url):
        if url is None:
            return None

        response = urlopen(url)
        if response.getcode() != 200:
            return None
        return response.read()


class UrlManager(object):
    def __init__(self):
        self.new_urls = set()
        self.old_urls = set()

    def add_new_url(self, url):
        if url is None:
            return
        if url not in self.new_urls and url not in self.old_urls:
            self.new_urls.add(url)

    def add_new_urls(self, urls):
        if urls is None or len(urls) == 0:
            return
        for url in urls:
            self.add_new_url(url)

    def has_new_url(self):
        return len(self.new_urls) != 0

    def get_new_url(self):
        new_url = self.new_urls.pop()
        self.old_urls.add(new_url)
        return new_url




class SpiderMain(object):
    def __init__(self):
        # url 管理器
        self.urls = UrlManager()
        # 下载器
        self.downloader = HtmlDownloader()

        # 解析器
        self.parser = HtmlParser()

        # 输出器
        self.outputer = HtmlOutputer()

    def craw(self, root_url):
        count = 1
        # 入口url添加
        self.urls.add_new_url(root_url)

        # 如果有新的url
        while self.urls.has_new_url():
            try:

                # 获取新的url
                new_url = self.urls.get_new_url()
                print(&apos;craw %d : %s&apos; % (count, new_url))

                # 启动下载器下载页面
                html_cont = self.downloader.download(new_url)

                # 解析器来解析新的地址和数据
                new_urls, new_data = self.parser.parse(new_url, html_cont)

                # 把新的地址添加到urls
                self.urls.add_new_urls(new_urls)

                # 输出和收集数据
                self.outputer.collect_data(new_data)

                if count == 1000:
                    break
                count += 1
            except:
                print(&apos;craw failed.&apos;)

        # 输出数据
        self.outputer.outputer_html()


if __name__ == &quot;__main__&quot;:
    root_url = &quot;http://baike.baidu.com/view/21087.htm&quot;
    # 创建一个爬虫对象
    obj_spider = SpiderMain()
    # 运行爬虫函数
    obj_spider.craw(root_url)
</code></pre><p>4、原作者是的html_outputer.py 文件包含了3个函数，最后一个是把爬虫数据导出到一个.html文件里，但是按照他的源代码，最后我总是得到错误提示如下，提示整数不能用来encode，也就是爬到的百科名词有的是纯数字的，比如设想是10086 ，另外在python 2 到3 的切换使用中，也确实遇到好几在V2 下ok的代码，到了V3失败，很多是来自解码方面的异常。：</p>
<p>AttributeError: ‘int’ object has no attribute ‘encode’</p>
<p>或者当去掉utf-8输出，又遇到gbk编码的问题：<br>UnicodeEncodeError: ‘gbk’ codec can’t encode character ‘\u02c8’ in position 19: illegal multibyte sequence</p>
<p>5、总结：</p>
<ul>
<li><p>首先，原作者在一个工程下，建立一个package，然后把几个主要的类单独列出了py文件，这样的方式更为规范，此文为粘贴代码简便，统一到了一起。实际，应用第一种模式为佳。</p>
</li>
<li><p>其次，在分析html的分类入口处，比如python关键词地方的对应html代码，使用右键–&gt;检查（或F12，然后点击上面有“类”的地方，比如此文是关键词python在类lemmaWgt-lemmaTitle-title下的h1标题，然后再右键–&gt;Edit as html，就可以在一个段落里轻松复制这断html代码了。否则在F12模式下不能直接上下复制的）</p>
</li>
<li>经过比较python 2， 如果检索的关键词是数字，一样不能使用.encode(‘utf-8’)， 那么原作者的演示视频为何都没有报错？ 这应该是他当时的爬虫没有搜寻到数字关键词，而我的遇到了。 </li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel  sidebar-panel-active ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="Van" />
          <p class="site-author-name" itemprop="name">Van</p>
          <p class="site-description motion-element" itemprop="description"></p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">39</span>
              <span class="site-state-item-name">Artikel</span>
            </a>
          </div>

          

          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2016</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Van</span>
</div>

<div class="powered-by">
  Erstellt mit  <a class="theme-link" href="https://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  



  




  
  

  

  

  

</body>
</html>
