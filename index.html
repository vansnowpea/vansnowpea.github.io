<!doctype html>



  


<html class="theme-next muse use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1" />






<meta property="og:type" content="website">
<meta property="og:title" content="天青色等烟雨">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="天青色等烟雨">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="天青色等烟雨">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: 'Author'
    }
  };
</script>




  <link rel="canonical" href="http://yoursite.com/"/>

  <title> 天青色等烟雨 </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">天青色等烟雨</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/09/15/爬取瓜子网上海地区二手车的信息/" itemprop="url">
                  爬取瓜子网上海地区二手车的信息【python】
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2016-09-15T18:59:28+08:00" content="2016-09-15">
              2016-09-15
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="说明原文连接"><a href="#说明原文连接" class="headerlink" title="说明原文连接"></a><a href="http://www.jianshu.com/p/15eb2a39ffe0" target="_blank" rel="external">说明原文连接</a></h1><p>1、在原文基础上稍作修改，目标地区为上海，更改了网址显示和价格显示部分。</p>
<p>2、本文亮点在网址分析处，使用了CSS path，目前在谷歌浏览器下显示为Copy selector ，这和原文的Copy CSS Path是不同的，就本文，这个分析方法完全避开了正则式，从中，也体现了bs4的强大。</p>
<p>3、代码：</p>
<pre><code># -*- coding: utf-8 -*-
# python 3.5.2

from bs4 import BeautifulSoup
import requests

def detailOper(url):
    web_data = requests.get(url)
    # soup = BeautifulSoup(web_data.text, &apos;lxml&apos;)
    soup = BeautifulSoup(web_data.text, &apos;html.parser&apos;)

    # body &gt; div.w &gt; div.list &gt; ul &gt; li:nth-child(1) &gt; div &gt; p.infoBox &gt; a
    # above is the css path , or copy selector in Google Chrome F12
    # we can delete &quot;body &gt; div.w &gt;&quot; as the whole site under Class = &apos;w&apos;
    # modify it for all cars info: div.list &gt; ul &gt; li &gt; div &gt; p.infoBox &gt; a
    titles = soup.select(&apos;div.list &gt; ul &gt; li &gt; div &gt; p.infoBox &gt; a&apos;)

    # get price info with F12: body &gt; div.w &gt; div.list &gt; ul &gt; li:nth-child(1) &gt; div &gt; p.priType-s &gt; span &gt; i
    # modify it for all price: div.list &gt; ul &gt; li &gt; div &gt; p.priType-s &gt; span &gt; i
    prices = soup.select(&apos;div.list &gt; ul &gt; li &gt; div &gt; p.priType-s &gt; span &gt; i&apos;)
    for title, price in zip(titles, prices):
        data = {
        &apos;车型&apos;: title.get_text(),
        &apos;网址&apos;: &apos;http://www.guazi.com&apos;+ title.get(&apos;href&apos;),
        #&apos;price&apos;:price.get_text().replace(u&apos;万&apos;, &apos;&apos;).replace(&apos; &apos;, &apos;&apos;)
        &apos;价格&apos;: price.get_text().replace(&apos;\n&apos;, &apos;&apos;).replace(&apos; &apos;, &apos;&apos;)
    }
    print(data)

def start():
    urls = [&apos;http://www.guazi.com/sh/buy/o{}/&apos;.format(str(i)) for i in range(1, 51, 1)]
    for url in urls:
        detailOper(url)

if __name__ == &apos;__main__&apos;:
    start()
</code></pre><p>4、部分结果展示：</p>
<pre><code>{&apos;车型&apos;: &apos;东南V5菱致 2013款 1.5 手动 舒适型CNG&apos;, &apos;价格&apos;: &apos;已降价&apos;, &apos;网址&apos;: &apos;http://www.guazi.com/sh/3000216790x.htm&apos;}
{&apos;车型&apos;: &apos;福特蒙迪欧 2013款 致胜 2.3 自动 时尚型&apos;, &apos;价格&apos;: &apos;9.00万&apos;, &apos;网址&apos;: &apos;http://www.guazi.com/sh/3000237200x.htm&apos;}
{&apos;车型&apos;: &apos;斯柯达明锐 2010款 明锐 1.6 手动 逸致版&apos;, &apos;价格&apos;: &apos;3.80万&apos;, &apos;网址&apos;: &apos;http://www.guazi.com/sh/3000222224x.htm&apos;}
{&apos;车型&apos;: &apos;大众速腾 2014款 速腾 1.4TSI 手动 豪华型&apos;, &apos;价格&apos;: &apos;已降价&apos;, &apos;网址&apos;: &apos;http://www.guazi.com/sh/3000415677x.htm&apos;}
{&apos;车型&apos;: &apos;福特蒙迪欧 2011款 蒙迪欧致胜 2.3 自动 时尚型&apos;, &apos;价格&apos;: &apos;3.99万&apos;, &apos;网址&apos;: &apos;http://www.guazi.com/sh/3000218992x.htm&apos;}
{&apos;车型&apos;: &apos;大众CC2015款 CC 1.8TSI 双离合 豪华型&apos;, &apos;价格&apos;: &apos;已降价&apos;, &apos;网址&apos;: &apos;http://www.guazi.com/sh/3000431976x.htm&apos;}
{&apos;车型&apos;: &apos;大众Polo2014款 1.6 自动 舒适版&apos;, &apos;价格&apos;: &apos;19.50万&apos;, &apos;网址&apos;: &apos;http://www.guazi.com/sh/3000428176x.htm&apos;}
{&apos;车型&apos;: &apos;吉利GX7 2013款 1.8 手动 尊贵型&apos;, &apos;价格&apos;: &apos;已降价&apos;, &apos;网址&apos;: &apos;http://www.guazi.com/sh/3000215362x.htm&apos;}
{&apos;车型&apos;: &apos;标致3008 2013款 1.6THP 自动 至尚版&apos;, &apos;价格&apos;: &apos;8.10万&apos;, &apos;网址&apos;: &apos;http://www.guazi.com/sh/3000240867x.htm&apos;}
{&apos;车型&apos;: &apos;比亚迪F3 2010款 1.5 手动 新白金版 GLX-i 豪华型&apos;, &apos;价格&apos;: &apos;4.50万&apos;, &apos;网址&apos;: &apos;http://www.guazi.com/sh/3000205737x.htm&apos;}
{&apos;车型&apos;: &apos;五菱荣光S 2014款 1.2 手动 标准型7-8座&apos;, &apos;价格&apos;: &apos;9.98万&apos;, &apos;网址&apos;: &apos;http://www.guazi.com/sh/3000243019x.htm&apos;}
</code></pre><p>5、在原文基础上的拓展：<br>   增加了把数据通过pymysql导入到了mysql中，看起来更方便。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/09/15/win10中python3.5.2遇到lxml安装问题的记录/" itemprop="url">
                  win10中python3.5.2遇到lxml安装问题的记录【python】
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2016-09-15T11:59:28+08:00" content="2016-09-15">
              2016-09-15
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="说明-win10中python3-5-2遇到lxml安装问题的记录"><a href="#说明-win10中python3-5-2遇到lxml安装问题的记录" class="headerlink" title="说明:win10中python3.5.2遇到lxml安装问题的记录"></a>说明:win10中python3.5.2遇到lxml安装问题的记录</h1><p>1、错误提示：  error: Unable to find vcvarsall.bat</p>
<p>2、由于在python 2.7 和win7下遇到过类似的问题，当时用安装wheel文件的方法，不过这次失败， 另外在python2.7时代，是有一个微软的支持版本的： Microsoft Visual C++ Compiler for Python 2.7（只有 2.7</p>
<p>3、该问题windows环境中，python 的 Setup需要调用一个vcvarsall.bat的文件，该文件需要安装c++编程环境才会有，经查知乎上有人给出了答案，由于python 3.5使用的SDK环境和3.4是不同的，所以需要安装VC++最新版，或者VS2015 添加VC和python相关组件， ，请参考：<a href="http://www.zhihu.com/question/26857761" target="_blank" rel="external">http://www.zhihu.com/question/26857761</a> 里面的“代代树”的回复。但是第一次安装遇到retry的失败，关闭了相关VS的进程，重新装，貌似就可以安装了，但安装完毕后，问题没有解决，依然存在啊，什么鬼？</p>
<p>4、然后又发现别人推荐了<a href="https://anaconda.org/或者https://www.continuum.io/downloads的Anaconda安装解决方式，一看官网支持python最新版，这就好感爆棚了。果断下载安装，不过结果依然失败，有点闹腾。以后解决了再更新。" target="_blank" rel="external">https://anaconda.org/或者https://www.continuum.io/downloads的Anaconda安装解决方式，一看官网支持python最新版，这就好感爆棚了。果断下载安装，不过结果依然失败，有点闹腾。以后解决了再更新。</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/09/15/推荐使用requests代替urllib/" itemprop="url">
                  推荐使用requests代替urllib【python】
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2016-09-15T09:59:28+08:00" content="2016-09-15">
              2016-09-15
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="说明-之前爬虫时读取网址内容用的都是urllib，这是python自带的，但如果使用复杂点的功能，还是推荐requests第三方库，这是因为这个库很优秀"><a href="#说明-之前爬虫时读取网址内容用的都是urllib，这是python自带的，但如果使用复杂点的功能，还是推荐requests第三方库，这是因为这个库很优秀" class="headerlink" title="说明:之前爬虫时读取网址内容用的都是urllib，这是python自带的，但如果使用复杂点的功能，还是推荐requests第三方库，这是因为这个库很优秀"></a>说明:之前爬虫时读取网址内容用的都是urllib，这是python自带的，但如果使用复杂点的功能，还是推荐requests第三方库，这是因为这个库很优秀</h1><p>1、比较，参考自： <a href="http://www.jb51.net/article/63711.htm：" target="_blank" rel="external">http://www.jb51.net/article/63711.htm：</a></p>
<blockquote>
<p>一个复杂一点的例子：<br>现在让我们尝试下复杂点得例子：使用GET方法获取<a href="http://foo.test/secret的资源，这次需要基本的http验证。使用上面的代码作为模板，好像我们只要把urllib2.urlopen(" target="_blank" rel="external">http://foo.test/secret的资源，这次需要基本的http验证。使用上面的代码作为模板，好像我们只要把urllib2.urlopen(</a>) 到requests.get()之间的代码换成可以发送username，password的请求就行了<br>这是urllib2的方法：</p>
<blockquote>
<blockquote>
<p>import urllib2<br>url = ‘<a href="http://example.test/secret" target="_blank" rel="external">http://example.test/secret</a>‘<br>password_manager = urllib2.HTTPPasswordMgrWithDefaultRealm()<br>password_manager.add_password(None, url, ‘dan’, ‘h0tdish’)<br>auth_handler = urllib2.HTTPBasicAuthHandler(password_manager)<br>opener = urllib2.build_opener(auth_handler)<br>urllib2.install_opener(opener)<br>response = urllib2.urlopen(url)<br>response.getcode()<br>200<br>response.read()<br>‘Welcome to the secret page!’<br>一个简单的方法中实例化了2个类，然后组建了第三个类，最后还要装载到全局的urllib2模块中，最后才调用了urlopen，那么那两个复杂的类是什么的<br>迷惑了吗，  这里所有urllib2的文档 <a href="http://docs.python.org/release/2.7/library/urllib2.html" target="_blank" rel="external">http://docs.python.org/release/2.7/library/urllib2.html</a></p>
</blockquote>
</blockquote>
<p>那Requests是怎么样解决同样的问题的呢？<br>Requests</p>
<blockquote>
<blockquote>
<p>import requests<br>url = ‘<a href="http://example.test/secret" target="_blank" rel="external">http://example.test/secret</a>‘<br>response = requests.get(url,auth=(‘dan’,’h0tdish’))<br>response.status_code<br>200<br>response.content<br>u’Welcome to the secret page!’<br>只是在调用方法的时候增加了一个auth关键字函数</p>
</blockquote>
</blockquote>
</blockquote>
<p>2、requests的documents官网：  <a href="http://www.python-requests.org/en/master/" target="_blank" rel="external">http://www.python-requests.org/en/master/</a><br>建议阅读下其documents，写的还是挺清楚的。</p>
<p>3、基本使用：<br>import requests<br>r = requests.get(‘<a href="https://api.github.com/events" target="_blank" rel="external">https://api.github.com/events</a>‘)<br>r.text<br>r.content   # 以字节的方式访问请求响应体，对于非文本请求</p>
<p>r = requests.post(‘<a href="http://httpbin.org/post" target="_blank" rel="external">http://httpbin.org/post</a>‘, data = {‘key’:’value’})</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/09/14/百度百科1000个页面数据的抓取【python】/" itemprop="url">
                  百度百科1000个页面数据的抓取【python】
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2016-09-14T21:59:28+08:00" content="2016-09-14">
              2016-09-14
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="说明：以下内容学习自：http-www-imooc-com"><a href="#说明：以下内容学习自：http-www-imooc-com" class="headerlink" title="说明：以下内容学习自：http://www.imooc.com/"></a>说明：以下内容学习自：<a href="http://www.imooc.com/learn/563" target="_blank" rel="external">http://www.imooc.com/</a></h1><p>1、首先要感谢原作者非常细致的讲解，在网页分析的时候还是学到了一些很有用的技巧。后续会总结。</p>
<p>2、这次是跟着作者，使用了Eclipse + Pydev的软件调试的，相对于Pycharm，他的优点就是可以动态插入需要的类和方法，而同比Pycharm只能动态匹配类，方法就没反应了。不过在界面配色方面明显没有Pyhcarm做的好。总之，目前的配色方案不满意，但将就下吧。</p>
<p>3、原作者使用的Python 2的版本，我的版本（大部分还是照作者的）已经修改到3的版本了，。虽然感觉作者的代码，有的地方写的有点小啰嗦，不过看他视频现场操作，还是感觉细节有很多值得借鉴的地方。</p>
<pre><code># -*- coding: utf-8 -*-
# python 3.5.2

from urllib.request import urlopen
from bs4 import BeautifulSoup
from urllib.parse import urlparse
import re
import urllib


class HtmlOutputer(object):
    def __init__(self):
        self.datas = []

    def collect_data(self, data):
        if data is None:
            return
        self.datas.append(data)

    def outputer_html(self):
        fout = open(&apos;output.html&apos;, &apos;w&apos;)
        fout.write(&quot;&lt;html&gt;&quot;)
        fout.write(&quot;&lt;body&gt;&quot;)
        fout.write(&quot;&lt;table&gt;&quot;)

        # accii
        for data in self.datas:
            fout.write(&quot;&lt;tr&gt;&quot;)
            fout.write(&quot;&lt;td&gt;%s&lt;/td&gt;&quot; % data[&apos;url&apos;])
            # 原作者使用了.encode(&apos;utf-8&apos;)的代码，但在python 3 测试报错
            fout.write(&quot;&lt;td&gt;%s&lt;/td&gt;&quot; % data[&apos;title&apos;]).encode(&apos;utf-8&apos;)
            fout.write(&quot;&lt;td&gt;%s&lt;/td&gt;&quot; % data[&apos;summary&apos;]).encode(&apos;utf-8&apos;)

            fout.write(&quot;&lt;/tr&gt;&quot;)

        fout.write(&quot;&lt;/body&gt;&quot;)
        fout.write(&quot;&lt;/table&gt;&quot;)
        fout.write(&quot;&lt;/html&gt;&quot;)
        fout.close()


class HtmlParser(object):
    def _get_new_urls(self, page_url, soup):
        new_urls = set()
        # /view/123.htm
        links = soup.find_all(&apos;a&apos;, href=re.compile(r&apos;/view/\d+\.htm&apos;))
        for link in links:
            new_url = link[&apos;href&apos;]

            new_full_url = urllib.parse.urljoin(page_url, new_url)
            new_urls.add(new_full_url)
        return new_urls

    def _get_new_data(self, page_url, soup):
        # http://baike.baidu.com/view/21087.htm
        res_data = {}
        res_data[&apos;url&apos;] = page_url
        # &lt;dd class=&quot;lemmaWgt-lemmaTitle-title&quot;&gt;&lt;h1&gt;Python&lt;/h1&gt;
        title_node = soup.find(&apos;dd&apos;, class_=&quot;lemmaWgt-lemmaTitle-title&quot;).find(&quot;h1&quot;)
        res_data[&apos;title&apos;] = title_node.get_text()

        # &lt;div class=&quot;lemma-summary&quot; label-module=&quot;lemmaSummary&quot;&gt;
        summery_node = soup.find(&apos;div&apos;, class_=&quot;lemma-summary&quot;)
        res_data[&apos;summery&apos;] = summery_node.get_text()

        return res_data

    def parse(self, page_url, html_cont):
        if page_url is None or html_cont is None:
            return
        soup = BeautifulSoup(html_cont, &apos;html.parser&apos;, from_encoding=&apos;uft-8&apos;)
        new_urls = self._get_new_urls(page_url, soup)
        new_data = self._get_new_data(page_url, soup)

        return new_urls, new_data


class HtmlDownloader(object):
    def download(self, url):
        if url is None:
            return None

        response = urlopen(url)
        if response.getcode() != 200:
            return None
        return response.read()


class UrlManager(object):
    def __init__(self):
        self.new_urls = set()
        self.old_urls = set()

    def add_new_url(self, url):
        if url is None:
            return
        if url not in self.new_urls and url not in self.old_urls:
            self.new_urls.add(url)

    def add_new_urls(self, urls):
        if urls is None or len(urls) == 0:
            return
        for url in urls:
            self.add_new_url(url)

    def has_new_url(self):
        return len(self.new_urls) != 0

    def get_new_url(self):
        new_url = self.new_urls.pop()
        self.old_urls.add(new_url)
        return new_url




class SpiderMain(object):
    def __init__(self):
        # url 管理器
        self.urls = UrlManager()
        # 下载器
        self.downloader = HtmlDownloader()

        # 解析器
        self.parser = HtmlParser()

        # 输出器
        self.outputer = HtmlOutputer()

    def craw(self, root_url):
        count = 1
        # 入口url添加
        self.urls.add_new_url(root_url)

        # 如果有新的url
        while self.urls.has_new_url():
            try:

                # 获取新的url
                new_url = self.urls.get_new_url()
                print(&apos;craw %d : %s&apos; % (count, new_url))

                # 启动下载器下载页面
                html_cont = self.downloader.download(new_url)

                # 解析器来解析新的地址和数据
                new_urls, new_data = self.parser.parse(new_url, html_cont)

                # 把新的地址添加到urls
                self.urls.add_new_urls(new_urls)

                # 输出和收集数据
                self.outputer.collect_data(new_data)

                if count == 1000:
                    break
                count += 1
            except:
                print(&apos;craw failed.&apos;)

        # 输出数据
        self.outputer.outputer_html()


if __name__ == &quot;__main__&quot;:
    root_url = &quot;http://baike.baidu.com/view/21087.htm&quot;
    # 创建一个爬虫对象
    obj_spider = SpiderMain()
    # 运行爬虫函数
    obj_spider.craw(root_url)
</code></pre><p>4、原作者是的html_outputer.py 文件包含了3个函数，最后一个是把爬虫数据导出到一个.html文件里，但是按照他的源代码，最后我总是得到错误提示如下，提示整数不能用来encode，也就是爬到的百科名词有的是纯数字的，比如设想是10086 ，另外在python 2 到3 的切换使用中，也确实遇到好几在V2 下ok的代码，到了V3失败，很多是来自解码方面的异常。：</p>
<p>AttributeError: ‘int’ object has no attribute ‘encode’</p>
<p>或者当去掉utf-8输出，又遇到gbk编码的问题：<br>UnicodeEncodeError: ‘gbk’ codec can’t encode character ‘\u02c8’ in position 19: illegal multibyte sequence</p>
<p>5、总结：</p>
<ul>
<li><p>首先，原作者在一个工程下，建立一个package，然后把几个主要的类单独列出了py文件，这样的方式更为规范，此文为粘贴代码简便，统一到了一起。实际，应用第一种模式为佳。</p>
</li>
<li><p>其次，在分析html的分类入口处，比如python关键词地方的对应html代码，使用右键–&gt;检查（或F12，然后点击上面有“类”的地方，比如此文是关键词python在类lemmaWgt-lemmaTitle-title下的h1标题，然后再右键–&gt;Edit as html，就可以在一个段落里轻松复制这断html代码了。否则在F12模式下不能直接上下复制的）</p>
</li>
<li>经过比较python 2， 如果检索的关键词是数字，一样不能使用.encode(‘utf-8’)， 那么原作者的演示视频为何都没有报错？ 这应该是他当时的爬虫没有搜寻到数字关键词，而我的遇到了。 </li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/09/13/Ubuntu下操作Django/" itemprop="url">
                  Ubuntu下操作Django时的大杂烩【python】
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2016-09-13T23:59:28+08:00" content="2016-09-13">
              2016-09-13
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="说明：以下内容学习自：http-www-imooc-com"><a href="#说明：以下内容学习自：http-www-imooc-com" class="headerlink" title="说明：以下内容学习自：http://www.imooc.com/"></a>说明：以下内容学习自：<a href="http://www.imooc.com/video/12634" target="_blank" rel="external">http://www.imooc.com/</a></h1><p>1、首先记录一个遇到的问题，一开始vi编辑模式下按方向键会变成ABCD,ubuntu下 vi输入方向键会变成ABCD，这是ubuntu预装的是vim tiny版本，安装vim full版本即可解决。先卸载vim-tiny：</p>
<pre><code>$ sudo apt-get remove vim-common
</code></pre><p>再安装vim full：</p>
<pre><code>$ sudo apt-get install vim
</code></pre><p>2、倒腾了半天，总算把vi的命令模式和插入模式搞明白了。搜索是“？+关键词”，下一个是字母：“n”，还有很多命令，可参考这里：<a href="http://www.cnblogs.com/emanlee/archive/2011/11/10/2243930.html" target="_blank" rel="external">http://www.cnblogs.com/emanlee/archive/2011/11/10/2243930.html</a><br>, 一开始因为第一条中记录的问题，导致一直输入错误。</p>
<p>3、遇到问题：<br>  第一个电脑上，用pip isntall django后运行正常，第二个就报错 找不到django-admin 后来改用sudo apt install python-django才可以，不过貌似版本不是1.10.1的最新版，而是1.8.7的。</p>
<p>4、Show Intention Actions ： 视频教程快速插入类和函数用的Ctrl+1的快捷键？查了下pycharm是Alt+Enter，但是明显这个设置要比Ctrl+1的模式反人类。 直接在setting–&gt;Keymap 搜索Alt竟然找不到的，原来他是按照左边的名次来搜索的，Show Intention Actions，本来想给他按照视频教程的快捷键修改，但是发现可以用鼠标双击的模式来定义，并且检测没有冲突，这下甚好。但问题又来了，只能自动识别类，不能识别下面的方法，而这一点上，Eclipse+pydev倒是做的很好。</p>
<p>5、一个意外的发现： pycharm能在console调用ipython，而且带了显示函数的功能，如下图，问题是我忘记怎么把他调用出来了。：<br><img src="http://ocg7i7pt6.bkt.clouddn.com/pycharm%20ipython.png" alt=""></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/09/13/pdf的读取【python】/" itemprop="url">
                  pdf的读取【python】
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2016-09-13T23:59:28+08:00" content="2016-09-13">
              2016-09-13
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="说明：以下内容学习自：http-www-imooc-com"><a href="#说明：以下内容学习自：http-www-imooc-com" class="headerlink" title="说明：以下内容学习自：http://www.imooc.com/"></a>说明：以下内容学习自：<a href="http://www.imooc.com/video/12634" target="_blank" rel="external">http://www.imooc.com/</a></h1><p>1、使用pdfminer3k，如果直接pip安装非常卡导致失败，所以去<a href="https://pypi.python.org/pypi/pdfminer3k/" target="_blank" rel="external">https://pypi.python.org/pypi/pdfminer3k/</a> 用迅雷下载后，在对应路径，用python setup.py install 来安装。</p>
<p>2、在下载文件的docs文件夹下有一个index.html，里面有 PDFMiner API相关的说明，操作流程如下2个图：</p>
<p><img src="http://ocg7i7pt6.bkt.clouddn.com/pdf1.png" alt=""></p>
<p><img src="http://ocg7i7pt6.bkt.clouddn.com/pdf2.png" alt=""></p>
<p>3、示范代码：</p>
<pre><code>__author__ = &apos;Administrator&apos;
# python 3.5.2

from pdfminer.pdfparser import PDFParser, PDFDocument
from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter
from pdfminer.pdfdevice import PDFDevice
from pdfminer.layout import LAParams
from pdfminer.converter import PDFPageAggregator

# 获取文档对象，以二进制读的模式。
fp = open(&apos;naacl06-shinyama.pdf&apos;, &apos;rb&apos;)

# 创建一个与文档关联的parser
parser = PDFParser(fp)

# pdf文档的对象
doc = PDFDocument()

# 连接解释器和文档对象
parser.set_document(doc)
doc.set_parser(parser)

# 初始化文档
doc.initialize(&apos;&apos;)

# 创建pdf资源管理器
resource = PDFResourceManager()

# 创建参数分析器
laparam = LAParams()

# 创建一个聚合器
device = PDFPageAggregator(resource, laparams=laparam)

# 创建一个页面解释器
interpreter = PDFPageInterpreter(resource, device)

#使用文档对象从页面读取内容
for page in doc.get_pages():
    # 使用页面解释器来读取
    interpreter.process_page(page)

    # 使用聚合器来获取内容
    layout = device.get_result()

    for out in layout:
        if hasattr(out, &apos;get_text&apos;):
            print(out.get_text())
</code></pre><p>4、结果展示，读取了示范pdf的几乎所有文字，不包括一些图，表格公式等复杂的： </p>
<p><img src="http://ocg7i7pt6.bkt.clouddn.com/pdf%203%20result.png" alt=""></p>
<p>7、总结：如果需要写论文什么的，感觉还是挺有用的，作者提到了还有一些更高级的功能，不过没有展开，有点可惜，另外pdfminer3k的最后更新时间2012/07/20，貌似作者停止更新了。而一个朋友反馈：pdf解析器和修改器现在最好的应该是一个JAVA库，还有一个就是Adobe和foxit提供的API。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/09/13/把维基百科首页词条的数据导入mysql/" itemprop="url">
                  把维基百科首页词条的数据导入mysql以及读取操作【Python】
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2016-09-13T00:59:29+08:00" content="2016-09-13">
              2016-09-13
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="说明：以下内容学习自：http-www-imooc-com"><a href="#说明：以下内容学习自：http-www-imooc-com" class="headerlink" title="说明：以下内容学习自：http://www.imooc.com/"></a>说明：以下内容学习自：<a href="http://www.imooc.com/learn/712" target="_blank" rel="external">http://www.imooc.com/</a></h1><p>1、<a href="https://vansnowpea.github.io/2016/09/13/%E7%BB%B4%E5%9F%BA%E7%99%BE%E7%A7%91%E9%A6%96%E9%A1%B5%E8%AF%8D%E6%9D%A1%E7%9A%84%E7%88%AC%E5%8F%96/" target="_blank" rel="external">此文是跟着上一篇来写的</a>，之前，已经完成了对维基百科词条的数据采集工作。</p>
<p>2、本文和作者示范不同的地方是，建立数据库和表格的工具我维持使用了sqlyog软件，作者用的是哪个不确定，因为视频看不清上面的名字。</p>
<p>3、代码如下，已经有一定说明，原作者可没提供代码哦，请把password对应的密码部分，从xxxx修改成你的真实数据库密码：</p>
<pre><code>__author__ = &apos;Administrator&apos;
# coding:utf-8
# 运行在python 3.5.2

import re
from urllib.request import urlopen
from bs4 import BeautifulSoup as bs
import pymysql.cursors

# 使用urllib的urlopen方法，去打开维基百科的首页，读取，并用utf-8解码
resp = urlopen(&apos;https://en.wikipedia.org/wiki/Main_Page&apos;).read().decode(&apos;utf-8&apos;)

# 使用BeautifulSoup去解析，其中html.parser是默认的解析器
soup = bs(resp, &apos;html.parser&apos;)
# 获取所有的/wiki/开头的a标签的href属性。
listUrls = soup.find_all(&apos;a&apos;, href=re.compile(r&apos;^/wiki/&apos;))
print(listUrls)

# 输出所有的词条对应的名称和URL
for url in listUrls:
    # 排除.jpg JPG结尾的
    if not re.search(&apos;\.(jpg|JPG)$&apos;, url[&apos;href&apos;]):
        # 显示名称和网址的格式
        # string只能显示一个，而get_text()可显示标签下所有的文字。
        print(url.get_text(), &apos;&lt;----&gt;&apos;, &apos;https://en.wikipedia.org&apos;+url[&apos;href&apos;])
        # 得到数据库的连接
        connection = pymysql.connect(host=&apos;localhost&apos;,
                               user=&apos;root&apos;,
                               password=&apos;XXXX&apos;,
                               db=&apos;wikiurl&apos;,
                               charset=&apos;utf8&apos;
                               )

        try:
            # 创建会话指针
            with connection.cursor() as cursor:
                # 创建sql语句
                sql = &apos;insert into `urls` (`urlname`, `urlhref`) values(%s, %s)&apos;
                # 执行sql语句
                cursor.execute(sql, (url.get_text(), &apos;https://en.wikipedia.org&apos; + url[&apos;href&apos;]))
                # 提交
                connection.commit()

        finally:
            connection.close()
</code></pre><p>4、其中关键的步骤有：</p>
<p>A）：sqlyog中表格的制作和设置，千万注意，id的int类型，需要添加自增，否则运行程序会报错： “Field ‘id’ doesn’t have a default value</p>
<p><img src="http://ocg7i7pt6.bkt.clouddn.com/sqlyog%20wiki%20setting.png" alt=""></p>
<p>B）数据库的插入和执行命令，因为不太熟悉，对着视频敲代码都错误了几次，并且要注意以下代码<code>urls</code> (<code>urlname</code>, <code>urlhref</code>)使用的是Tab键上面的那个上点号</p>
<p>C）当程序没有报错，在sqlyog查看表格要先刷新</p>
<p>D）<a href="https://vansnowpea.github.io/2016/09/11/Python%E6%93%8D%E4%BD%9CMySQL%E6%95%B0%E6%8D%AE%E5%BA%93/" target="_blank" rel="external">在之前的帖子，是在python2.7下，通过sqlyog的import MySQLdb 来操作mysql的</a>，并且，代码中的密码对应的是passwd，而这个和直接用python 运行pymysql的对应命令password是不同的，要分清楚。 </p>
<p>5、结果图，有一丝喜悦感：<br><img src="http://ocg7i7pt6.bkt.clouddn.com/wiki%20success.png" alt=""></p>
<p>6、读取操作: 代码中，需要导入的包，和写入数据库的想同，主要的差别是读取数据需要额外的命令，比如，fetchall，fetchmany(size=n)这样的。相关读取的代码如下：</p>
<pre><code># 运行在python 3.5.2


import pymysql.cursors

connection = pymysql.connect(host=&apos;localhost&apos;,
                       user=&apos;root&apos;,
                       password=&apos;XXXX&apos;,
                       db=&apos;wikiurl&apos;,
                       charset=&apos;utf8&apos;
                       )

try:
    # 创建会话指针
    with connection.cursor() as cursor:
        # 创建需要查询数据的sql语句
        sql = &apos;select `urlname`, `urlhref` from `urls` where `id` is not null&apos;
        # 执行sql语句，计算数据库的总数量
        count = cursor.execute(sql)
        print(count)
        # 查询数据
        result = cursor.fetchmany(size=3)
        #result = cursor.fetchall()
        print(result)


finally:
    connection.close()
</code></pre><p>7、总结： 如轮是写入还是读取数据库，总流程基本为：</p>
<ul>
<li>导入巡行所需要的包，比如import pymysql.cursors</li>
<li>连接数据库，填写必要的数据库登录信息，和表格信息，编码信息等。</li>
<li>定义绘画指针和需要操作的数据库语句</li>
<li>执行相关操作，比如写入，查询等。其中写入操作还需要执行cursor.commit() 来完成提交。</li>
<li>最后，无论如何，都要在finnally中执行connection.close()来关闭数据库。</li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/09/13/维基百科首页词条的爬取/" itemprop="url">
                  维基百科首页词条的爬取【Python】
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2016-09-13T00:59:28+08:00" content="2016-09-13">
              2016-09-13
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="说明：以下内容学习自：http-www-imooc-com"><a href="#说明：以下内容学习自：http-www-imooc-com" class="headerlink" title="说明：以下内容学习自：http://www.imooc.com/"></a>说明：以下内容学习自：<a href="http://www.imooc.com/learn/712" target="_blank" rel="external">http://www.imooc.com/</a></h1><p>1、首先这个教程使用了V3的python，作者也讲到了一些功能函数，在V2不支持，经过测试，比如，urllib在V2就没有.request方法，实际上，网上反馈已经把urllib2合并到urllib中了。考虑到以后v3总会代替掉v2， 咱也升级算了。 顺便把win7也换成了win10, 貌似开机速度比win7还快。</p>
<p>2、直接上代码吧，加了中文注释，和视频教程大同小异的：之前的代码都是在sublime或者cmder里面操作，可发现换到V3以后，一个print在sublime中断报错提示，可完全一样的代码我用ipython又可以运行，好诧异。<br>所以，最后我干脆跑到pycharm去调试的。</p>
<pre><code>__author__ = &apos;Administrator&apos;
# coding:utf-8

import re
from urllib.request import urlopen
from bs4 import BeautifulSoup as bs

# 使用urllib的urlopen方法，去打开维基百科的首页，读取，并用utf-8解码
resp = urlopen(&apos;https://en.wikipedia.org/wiki/Main_Page&apos;).read().decode(&apos;utf-8&apos;)

# 使用BeautifulSoup去解析，其中html.parser是默认的解析器
soup = bs(resp, &apos;html.parser&apos;)
# 获取所有的/wiki/开头的a标签的href属性。
listUrls = soup.find_all(&apos;a&apos;, href=re.compile(r&apos;^/wiki/&apos;))
print(listUrls)

# 输出所有的词条对应的名称和URL
for url in listUrls:
    # 排除.jpg JPG结尾的
    if not re.search(&apos;\.(jpg|JPG)$&apos;, url[&apos;href&apos;]):
        # 显示名称和网址的格式
        # string只能显示一个，而get_text()可显示标签下所有的文字。
        print(url.get_text(), &apos;&lt;----&gt;&apos;, &apos;https://en.wikipedia.org&apos;+url[&apos;href&apos;])
</code></pre><p>3、win10遇到的问题：<br>   MP出错，提示awesomium_v1.6.5异常，好在MP能提示到官网提供awesomium_v1.6.6_sdk_win.exe的下载，第一次安装失败，卡机，后重启Win10修复。</p>
<p>4、方法总结：</p>
<ul>
<li>urllib相关函数得到网址和解码；</li>
<li>bs4进行解析，可使用默认的解析器或者lxml等解析器；</li>
<li>根据要求和RE规则，找到所有的地址列表</li>
<li>for循环后加工</li>
</ul>
<p>5、结果展示：网址直接可打开对应的wiki信息哦。</p>
<p><img src="http://ocg7i7pt6.bkt.clouddn.com/wiki%20citiao.png" alt=""></p>
<p>6、PS：作者提到的chrome插件postman通过翻墙下了最新版，看下载量和5星评级应该不错，不过暂时我还没体验到他的威力，以后有感悟的话，再更新。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/09/11/正则式学习小结/" itemprop="url">
                  正则式学习小结
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2016-09-11T10:59:28+08:00" content="2016-09-11">
              2016-09-11
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="说明：以下内容学习自：http-www-imooc-com"><a href="#说明：以下内容学习自：http-www-imooc-com" class="headerlink" title="说明：以下内容学习自：http://www.imooc.com/"></a>说明：以下内容学习自：<a href="http://www.imooc.com/learn/550" target="_blank" rel="external">http://www.imooc.com/</a></h1><p>由于这个视频教程已经说的比较详细，请参考原视频教程，这里为总结</p>
<p>1、简介：为何需要正则表达式，为了减少根据不同需求不断编制函数的麻烦，通过制定正则式规则，可以巧妙且有效的进行归纳总结。</p>
<p>2、Python的RE模块，RE是正则表达式的英文首字母缩写。最常用的就是re.match()函数了，其次是ma.group()，用来查阅匹配结果。</p>
<p>3、<strong>【重点】RE的语法：</strong></p>
<p><img src="http://ocg7i7pt6.bkt.clouddn.com/re%201.png" alt=""></p>
<p><img src="http://ocg7i7pt6.bkt.clouddn.com/re%202.png" alt=""></p>
<p><img src="http://ocg7i7pt6.bkt.clouddn.com/re%203.png" alt=""></p>
<p><img src="http://ocg7i7pt6.bkt.clouddn.com/re%204.png" alt=""></p>
<p>4、 RE其他模块：</p>
<ul>
<li>search（pattern, string, flags=0） 用来查找满足pattern的第一个返回值。</li>
<li>findall（pattern, string, flags=0）用来查找满足pattern的所有返回值。</li>
<li>sub（pattern, repl, string, count=0, flags=0）将字符串中匹配正则表达式的部分替代为其他值，并允许引入函数模式来进行替代。</li>
<li>split（pattern, string, maxsplit=0, flags=0）根据匹配分割字符串，返回分割字符串组成的列表。</li>
</ul>
<p>5、练习，抓取慕课网的<a href="http://www.imooc.com/course/list的图片。主要流程为：" target="_blank" rel="external">http://www.imooc.com/course/list的图片。主要流程为：</a></p>
<ol>
<li>抓取网页，</li>
<li>获取图片地址，</li>
<li>抓去图片内容并保存到本地，</li>
</ol>
<p>结果如下：<br><img src="http://ocg7i7pt6.bkt.clouddn.com/re-pics%20of%20imooc.png" alt=""></p>
<p>6、源代码：</p>
<pre><code>import re
import urllib2

test_url = &apos;http://www.imooc.com/course/list&apos;
req = urllib2.urlopen(test_url)
buf = req.read()
print buf

listurl = re.findall(r&apos;http:.+\.jpg&apos;, buf)
print listurl

i = 0
for url in listurl:
    f = open(str(i)+&apos;.jpg&apos;, &apos;w&apos;)
    req = urllib2.urlopen(url)
    buf = req.read()
    f.write(buf)
    i += 1
</code></pre><p>7、PS，再一次的，发现在Win下的结果出现乱码，而且抓的图片也是，而在Linux下是正常的。得找下原因。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/09/11/Python操作MySQL数据库/" itemprop="url">
                  Python操作MySQL数据库
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2016-09-11T10:59:28+08:00" content="2016-09-11">
              2016-09-11
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="说明：以下内容学习自：http-www-imooc-com"><a href="#说明：以下内容学习自：http-www-imooc-com" class="headerlink" title="说明：以下内容学习自：http://www.imooc.com/"></a>说明：以下内容学习自：<a href="http://www.imooc.com/learn/475" target="_blank" rel="external">http://www.imooc.com/</a></h1><p>简介：本课程讲解Python如何开发MySQL数据库程序。首先介绍Python开发数据库程序的通用接口规范，然后搭建Python开发数据库程序的开发环境，接着了解了通用接口规范中的connection、cursor两大对象之后，介绍如何增删改查数据库，最后以实例代码演示数据库程序的开发流程。</p>
<p>1、教程说到以下网址<a href="http://sourceforge.net/projects/mysql-python下载mysql-pyhon" target="_blank" rel="external">http://sourceforge.net/projects/mysql-python下载mysql-pyhon</a> 的连接器，针对python2.7的。 一开始安装失败，提示找不到python，原来此台电脑我的python 是64位的，貌似那个连接器是针对32位的，所有我又到以下网址下载了64位的连接器： <a href="http://www.codegood.com/download/11/" target="_blank" rel="external">http://www.codegood.com/download/11/</a><br>这下就能识别到python了， 但是一看版本是1.2.3的，而最新版是1.2.5的，考虑到mysql 5.6默认的msi也是32位的，我就把python 2.7.12换到了32位统一下吧。   安装成功后，在python下输入如下命令会成功，否则报错：</p>
<pre><code>&gt;&gt;&gt; import MySQLdb
&gt;&gt;&gt;
</code></pre><p>2、然后根据教程要使用sqlyog，官网卡机，只好装国内版本，我用的这个连接（Ultimate 版本: （终极是最好的））： <a href="http://www.liangchan.net/soft/download.asp?softid=6633&amp;downid=8&amp;id=6658" target="_blank" rel="external">http://www.liangchan.net/soft/download.asp?softid=6633&amp;downid=8&amp;id=6658</a></p>
<p>随后在sqlyog新建一个数据库，并设置下utf8编码，如下图：<br><img src="http://ocg7i7pt6.bkt.clouddn.com/sqlyog%201.png" alt=""></p>
<p>3、编写一个imooc.py 来建立connection连接，<strong>千万注意代码中的密码对应的是passwd，而不是全称的password哦。</strong></p>
<p><img src="http://ocg7i7pt6.bkt.clouddn.com/db%20api%20connection.png" alt=""></p>
<p>4、通过connection来创建游标对象cursor： </p>
<p><img src="http://ocg7i7pt6.bkt.clouddn.com/cursor.png" alt=""></p>
<p>5、select查询数据</p>
<p><img src="http://ocg7i7pt6.bkt.clouddn.com/select%20%E6%9F%A5%E8%AF%A2%E6%95%B0%E6%8D%AE.png" alt=""></p>
<p>6、数据库 I U D 操作：</p>
<p><img src="http://ocg7i7pt6.bkt.clouddn.com/%E6%95%B0%E6%8D%AE%E5%BA%93u%20i%20d.png" alt=""></p>
<p>7、sqlyog的表单操作，要创建2个列就要在新建的表中，增加2个属性，如下图：</p>
<p><img src="http://ocg7i7pt6.bkt.clouddn.com/sqlyog%20%E8%A1%A8%E6%A0%BC%E8%AE%BE%E7%BD%AE%E5%88%97.png" alt=""></p>
<p>8、同时推荐W3C的SQL教程，地址是： <a href="http://www.w3school.com.cn/sql/index.asp" target="_blank" rel="external">http://www.w3school.com.cn/sql/index.asp</a></p>
<p>9、刚发现<a href="http://www.jianshu.com/p/a014b26ba7ca" target="_blank" rel="external">这个帖子</a>也有写的比较详细，可参考</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel  sidebar-panel-active ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="Van" />
          <p class="site-author-name" itemprop="name">Van</p>
          <p class="site-description motion-element" itemprop="description"></p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">33</span>
              <span class="site-state-item-name">posts</span>
            </a>
          </div>

          

          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2016</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Van</span>
</div>

<div class="powered-by">
  Powered by <a class="theme-link" href="https://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  



  




  
  

  

  

  

</body>
</html>
