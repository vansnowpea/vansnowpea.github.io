<!doctype html>



  


<html class="theme-next muse use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1" />






<meta property="og:type" content="website">
<meta property="og:title" content="蛇 母 陵">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="蛇 母 陵">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="蛇 母 陵">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: 'Author'
    }
  };
</script>




  <link rel="canonical" href="http://yoursite.com/"/>

  <title> 蛇 母 陵 </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">蛇 母 陵</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Startseite
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archiv
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/09/23/豆瓣电影TOP250的爬取和作图分析/" itemprop="url">
                  豆瓣电影TOP250的爬取和作图分析【python】
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Veröffentlicht am</span>
            <time itemprop="dateCreated" datetime="2016-09-23T23:20:28+08:00" content="2016-09-23">
              2016-09-23
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="简介：-又回到豆瓣了，这是多适合爬虫的网站-：）"><a href="#简介：-又回到豆瓣了，这是多适合爬虫的网站-：）" class="headerlink" title="简介： 又回到豆瓣了，这是多适合爬虫的网站 ：）"></a>简介： 又回到豆瓣了，这是多适合爬虫的网站 ：）</h1><p>1、这次的对象是top 250 ， 网址：  <a href="https://movie.douban.com/top250" target="_blank" rel="external">https://movie.douban.com/top250</a> 目的是对这250部电影的分类做一个统计。</p>
<p>2、实际有其他人做了分析，正好看到，发现其代码很漂亮，所以研读，顺便复习检阅自己学习成果。秉承一贯的作风，做一定的细节分析。先上原文代码：</p>
<pre><code># -*- coding: utf-8 -*-
# !/usr/bin/env python

from lxml import etree
import requests
import pymysql
import matplotlib.pyplot as plt
from pylab import *
import numpy as np

# 连接mysql数据库
conn = pymysql.connect(host = &apos;localhost&apos;, user = &apos;root&apos;, passwd = &apos;54545454&apos;, db = &apos;douban&apos;, charset = &apos;utf8&apos;)
cur = conn.cursor()
cur.execute(&apos;use douban&apos;)

def get_page(i):
    url = &apos;https://movie.douban.com/top250?start={}&amp;filter=&apos;.format(i)

    html = requests.get(url).content.decode(&apos;utf-8&apos;)

    selector = etree.HTML(html)
    # //*[@id=&quot;content&quot;]/div/div[1]/ol/li[1]/div/div[2]/div[2]/p[1]
    content = selector.xpath(&apos;//div[@class=&quot;info&quot;]/div[@class=&quot;bd&quot;]/p/text()&apos;)
    print(content)

    for i in content[1::2]:
        print(str(i).strip().replace(&apos;\n\r&apos;, &apos;&apos;))
        # print(str(i).split(&apos;/&apos;))
        i = str(i).split(&apos;/&apos;)
        i = i[len(i) - 1]
        # print(&apos;zhe&apos; +ｉ)
        # print(i.strip())
        # print(i.strip().split(&apos; &apos;))
        key = i.strip().replace(&apos;\n&apos;, &apos;&apos;).split(&apos; &apos;)
        print(key)
        for i in key:
            if i not in douban.keys():
                douban[i] = 1
            else:
                douban[i] += 1

def save_mysql():
    print(douban)
    for key in douban:
        print(key)
        print(douban[key])
        if key != &apos;&apos;:
            try:
                sql = &apos;insert douban(类别, 数量) value(&apos; + &quot;\&apos;&quot; + key + &quot;\&apos;,&quot; + &quot;\&apos;&quot; + str(douban[key]) + &quot;\&apos;&quot; + &apos;);&apos;
                cur.execute(sql)
                conn.commit()
            except:
                print(&apos;插入失败&apos;)
                conn.rollback()


def pylot_show():
    sql = &apos;select * from douban;&apos;
    cur.execute(sql)
    rows = cur.fetchall()
    count = []
    category = []

    for row in rows:
        count.append(int(row[2]))
        category.append(row[1])
    print(count)
    y_pos = np.arange(len(category))
    print(y_pos)
    print(category)
    colors = np.random.rand(len(count))
    plt.barh()
    plt.barh(y_pos, count, align=&apos;center&apos;, alpha=0.4)
    plt.yticks(y_pos, category)
    for count, y_pos in zip(count, y_pos):
        plt.text(count, y_pos, count,  horizontalalignment=&apos;center&apos;, verticalalignment=&apos;center&apos;, weight=&apos;bold&apos;)
    plt.ylim(+28.0, -1.0)
    plt.title(u&apos;豆瓣电影250&apos;)
    plt.ylabel(u&apos;电影分类&apos;)
    plt.subplots_adjust(bottom = 0.15)
    plt.xlabel(u&apos;分类出现次数&apos;)
    plt.savefig(&apos;douban.png&apos;)


if __name__ == &apos;__main__&apos;:
    douban = {}
    for i in range(0, 250, 25):
        get_page(i)
    save_mysql()
    pylot_show()
    cur.close()
    conn.close()
</code></pre><p>3、首先是他用了selector = etree.HTML(html)  而我用的比较多的是bs4，应该是殊途同归。他的xpath路径是：</p>
<pre><code>content = selector.xpath(&apos;//div[@class=&quot;info&quot;]/div[@class=&quot;bd&quot;]/p/text()&apos;) 
</code></pre><p>而我用的偷懒模式：直接浏览器找到的： </p>
<pre><code>selector.xpath(&apos;//*[@id=&quot;content&quot;]/div/div[1]/ol/li[1]/div/div[2]/div[2]/p[1]/text()&apos;)
</code></pre><p>注意，默认到P[1]的路径，因为是需要读取里面的文本部分，所以加入/text()</p>
<p>4、这样就得到了第一页的影视信息的文本，不过如果用ipthon分步操作查看， 那简直是逆天的格式，如图：</p>
<p><img src="http://ocg7i7pt6.bkt.clouddn.com/blog/2016-09-23/225057825.png" alt=""><em>**</em></p>
<p>5、必须进行文字提取处理了。 </p>
<p>首先，这一大坨的，为了简单，我们用单元测试的思想，先调试第一页第一个，这里之前已经设置从：</p>
<pre><code>url = &apos;https://movie.douban.com/top250?start={}&amp;filter=&apos;.format(i)
</code></pre><p>修改为：</p>
<pre><code>url = &apos;https://movie.douban.com/top250?start={1}&amp;filter=&apos;
</code></pre><p>同时，观察整个循环输出的格式是一个列表，每个元素是一个字符串，那么第一个电影的电影信息对应的字符串是：</p>
<pre><code>&apos;\n                            导演: 弗兰克·德拉邦特 Frank Darabont\xa0\xa0\xa0主演: 蒂姆·罗宾斯 Tim Robbins /...&apos;, &apos;\n                            1994\xa0/\xa0美国\xa0/\xa0犯罪 剧情\n                        &apos;
</code></pre><p>和原始网页的信息对照，实际就是第一部分包含导演主演信息，第二部分是年份国家和剧种， 就本文，我们需要的是第二部分的内容，也就是“1994 / 美国 / 犯罪 剧情”这部分内容。然后呢，因为整个content 包含了25部电影的文本， 这些都保存在列表里，并且我们需要进一步处理的，都在对应坐标1、3、5、、这样的奇数里。因为列表第一个是从0开始的。所以我们查看了循环内的第一句为，注意这只是print，没有运行具体的代码影响原来的字符串：</p>
<pre><code>print(str(i).strip().replace(&apos;\n\r&apos;, &apos;&apos;))
</code></pre><p>这句的意思就是得到整个content列表中，下标为1、3、5。。。。的元素，并通过strip（）来进行去掉首尾的多余的空格，,随后使用replace函数，把’\n\r’替换为空字符。我们可以在ipython中验证下：</p>
<p>记录：conntent[1]：  </p>
<pre><code>aa =  &apos;\n                            1994\xa0/\xa0美国\xa0/\xa0犯罪 剧情\n                        &apos;
</code></pre><p>执行：aa.strip()，得到：</p>
<pre><code>In [19]: aa.strip()
Out[19]: &apos;1994\xa0/\xa0美国\xa0/\xa0犯罪 剧情&apos;

In [20]: aa.strip().replace(&apos;\n\r&apos;, &apos;&apos;)
Out[20]: &apos;1994\xa0/\xa0美国\xa0/\xa0犯罪 剧情&apos;
</code></pre><p>发现上述的replace函数似乎没起作用，那是这里正好没有’\n\r’，同时你可以发现，这里怎么有4个地方有\xa0 ？ 这是什么呢？ 原来是：转义字符，”\x”后接数字（两位）代表16进制数，这玩意牵涉编码的问题，打印的时候是不显示的。同时也可以感受到，如果英文较好，可以全英文写程序，可尽可能的避免这些劳什子。</p>
<p>然后循环里面的开始执行的命令都是为了去除\n和空格这些没有的东西，i = str(i).split(‘/‘)就是把去除空格和\n的字符串通过‘/’分隔在列表里。</p>
<pre><code>In [35]: aa =  &apos;\n                            1994\xa0/\xa0美国\xa0/\xa0犯罪 剧情\n                        &apos;

In [36]: aa = aa.split(&apos;/&apos;)

In [37]: aa = aa(len(aa)-1)
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-37-06f75198160a&gt; in &lt;module&gt;()
----&gt; 1 aa = aa(len(aa)-1)

TypeError: &apos;list&apos; object is not callable

In [38]: aa = aa[len(aa)-1]

In [39]: aa
Out[39]: &apos;\xa0犯罪 剧情\n
</code></pre><p>随后，通过</p>
<pre><code>In [40]: key = aa.strip().replace(&apos;\n&apos;, &apos;&apos;).split(&apos; &apos;)

In [41]: key
Out[41]: [&apos;犯罪&apos;, &apos;剧情&apos;]
</code></pre><p>发现，已经把\xa0当作空格给替换掉了，实际他显示出来就是一个空格的。然后通过空格在劈开，得到了2个元素的字符串的列表。<br>这样就清洗出了所需要的数据。</p>
<p>下面的代码紧跟了一个针对key的for循环，是用来在数据库中计数用的</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/09/22/验证码识别的测试/" itemprop="url">
                  验证码识别的测试【python】
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Veröffentlicht am</span>
            <time itemprop="dateCreated" datetime="2016-09-22T14:20:28+08:00" content="2016-09-22">
              2016-09-22
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><img src="http://ocg7i7pt6.bkt.clouddn.com/blog/2016-09-23/012758858.jpg" alt=""></p>
<h1 id="简介：-验证码识别的测试，因故需做一个自动的多用户注册的脚本"><a href="#简介：-验证码识别的测试，因故需做一个自动的多用户注册的脚本" class="headerlink" title="简介： 验证码识别的测试，因故需做一个自动的多用户注册的脚本"></a>简介： 验证码识别的测试，因故需做一个自动的多用户注册的脚本</h1><p>1、查阅相关博客，建议使用的是Python+Selenium+PIL+Tesseract ，又有人推荐了Pytesser，不过这个Pytesser的安装还挺坑的。</p>
<p>2、开始参考的是：<br><a href="http://www.th7.cn/Program/Python/201602/768304.shtml" target="_blank" rel="external">http://www.th7.cn/Program/Python/201602/768304.shtml</a></p>
<p>以及这个</p>
<p><a href="http://blog.csdn.net/lanfan_11/article/details/45558573" target="_blank" rel="external">http://blog.csdn.net/lanfan_11/article/details/45558573</a></p>
<p>3、但测试了半天，无论我按照哪个方法，最终我都不能import pytesser ， 一开始以为成功了，原来是我按照他的例子直接在pytesseract-v0.0.1的文件夹测试的脚本，那自然可以了。</p>
<p>4、毕竟第一次遇到需要这么折腾的第三方库，而且还是好几年前就停止更新了，干脆就直接把需要编写的py文件丢到这个pytesser文件夹来规避。 </p>
<p>5、后来又发现有的博客推荐了pytesseract ，看名字应该是继承了pytesser，但是保持更新的，果然顺利安装，那么就是他了。那么更新下使用的工具组合为：Python+Selenium+PIL+pytesseract+Tesseract </p>
<p>6、剩下就是如何识别验证码的问题，由于测试的网站使用的.aspx的动态图，导致每次输入url得到不同的验证码。<br>所以一不做二不休，使用了截屏计算其验证码方位后单独识别的方案。(另外一个可操作方案是使用cookie)</p>
<p>过程：</p>
<pre><code># coding:utf-8
# python 3.5.2


from selenium.webdriver.support import ui as ui
from selenium.webdriver.common.keys import Keys #需要引入keys 包
from selenium.webdriver.common.action_chains import ActionChains
from selenium import webdriver
import pytesseract
import time
from PIL import Image,ImageEnhance

num = 400
# wait = ui.WebDriverWait(browser, 10)
# 已经人工测试了一个，所以从第二2个开始
for i in range(2, num+1):

    name_cn = &apos;aaa&apos;        
    browser = webdriver.PhantomJS()
    browser.maximize_window()
    url = &quot;http://www.wangzhi.com&quot;
    browser.get(url)
    wait = ui.WebDriverWait(browser, 20)
    wait.until(lambda browser:browser.find_element_by_xpath(&quot;//*[@id=\&quot;ussheng\&quot;]&quot;))
    shs1 = browser.find_element_by_xpath(&quot;//*[@id=\&quot;ussheng\&quot;]&quot;).send_keys(&quot;shengfen&quot;)
    shs2 = browser.find_element_by_xpath(&quot;//*[@id=\&quot;uscity\&quot;]&quot;).send_keys(&quot;xxshi&quot;)
    mhq = browser.find_element_by_xpath(&quot;//*[@id=\&quot;usxian\&quot;]&quot;).send_keys(&quot;yyqu&quot;)
    nling = browser.find_element_by_xpath(&quot;//*[@id=\&quot;inscage\&quot;]&quot;).send_keys(&quot;nianling&quot;)
    dwei = browser.find_element_by_xpath(&quot;//*[@id=\&quot;worker\&quot;]&quot;).send_keys(&quot;dizhi&quot;)
    name_input = browser.find_element_by_xpath(&quot;//*[@id=\&quot;ustruename\&quot;]&quot;).send_keys(name_cn + &apos;%d&apos;%i)

    # 选择性别，下拉框操作
    sex = browser.find_element_by_xpath(&quot;//*[@id=\&quot;DropDownList1\&quot;]&quot;)
    sex.find_element_by_xpath(&quot;//*[@id=\&quot;DropDownList1\&quot;]/option[2]&quot;).click()

    # 验证码识别，思路，右键保存验证码图片，识别数字或者字母。但是保存不能执行，失败
    ###############################################################################
    # # xpath： //*[@id=&quot;ValidImage&quot;]
    # shi_bie_ma = browser.find_element_by_xpath(&quot;//*[@id=\&quot;ValidImage\&quot;]&quot;)
    # action = ActionChains(browser).move_to_element(shi_bie_ma)
    # action.context_click(shi_bie_ma)
    # action.send_keys(Keys.ARROW_DOWN)
    # action.send_keys(&apos;v&apos;)
    # action.perform()
    ###############################################################################



    # 通过下载图片后识别，发现是aspx 每次打开url都更新识别码，所以失败。看来只好一开始就截屏啦。

    # 截屏
    browser.get_screenshot_as_file(&apos;C:\\van\\image1.jpg&apos;)#比较好理解

    # 检测识别码坐标
    imgelement = browser.find_element_by_xpath(&apos;//*[@id=&quot;ValidImage&quot;]&apos;) #定位验证码
    location = imgelement.location #获取验证码x,y轴坐标

    size=imgelement.size #获取验证码的长宽
    range=(int(location[&apos;x&apos;]),int(location[&apos;y&apos;]),int(location[&apos;x&apos;]+size[&apos;width&apos;]),int(location[&apos;y&apos;]+size[&apos;height&apos;])) #写成我们需要截取的位置坐标

    im =Image.open(&apos;C:\\van\\image1.jpg&apos;)
    # 设置要裁剪的区域
    region = im.crop(range)     #此时，region是一个新的图像对象。
    #region.show()#显示的话就会被占用，所以要注释掉
    region.save(&quot;C:\\van\\image2.jpg&quot;)


     #--------------------图片增强+自动识别简单验证码-----------------------------
    #time.sleep(3)防止由于网速，可能图片还没保存好，就开始识别

    im=Image.open(&quot;C:\\van\\image2.jpg&quot;)
    imgry = im.convert(&apos;L&apos;)#图像加强，二值化
    sharpness =ImageEnhance.Contrast(imgry)#对比度增强
    sharp_img = sharpness.enhance(2.0)

    sharp_img.save(&quot;C:\\van\\image2.jpg&quot;)

    #http://www.cnblogs.com/txw1958/archive/2012/02/21/2361330.html

    #imgry.show()#这是分布测试时候用的，整个程序使用需要注释掉
    #imgry.save(&quot;E:\\image_code.jpg&quot;)
    im=Image.open(&quot;C:\\van\\image2.jpg&quot;)
    code = pytesseract.image_to_string(im)#code即为识别出的图片数字str类型
    print(code)
    #打印code观察是否识别正确

    #-------------------------------------------------------------------
    code_input = browser.find_element_by_xpath(&quot;//*[@id=\&quot;txtCheckCode\&quot;]&quot;).send_keys(code)
    # login
    browser.find_element_by_xpath(&quot;//*[@id=\&quot;Button1\&quot;]&quot;).click()
    print(&quot;fished,%d&quot;%i)
    browser.quit()
</code></pre><p>7、程序分析：</p>
<ul>
<li>虽然使用的截屏然后识别验证码的方式，也许不是最优雅，不过可可以通过元素的imgelement.location 来获取验证码x,y轴坐标，从而可以得到精确的验证码截图区域，否则人工去调试也不是不行，但需要在画图板用鼠标尽可能准确的定位。</li>
<li>增加了图片增加识别技术，加大了识别验证码的概率。</li>
<li>这段代码的不足之处是没有检测验证码识别提交后，是否失败的判断，这主要是测试的网站标的，实在做的太烂了，点了二维码不会立刻提示错误与否，而是卡机。</li>
</ul>
<p>8、总结：有较多的注释，记录了一些操作思路。学习了验证码的初级识别技术。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/09/21/IP池的建立/" itemprop="url">
                  IP池的建立
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Veröffentlicht am</span>
            <time itemprop="dateCreated" datetime="2016-09-21T22:20:28+08:00" content="2016-09-21">
              2016-09-21
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="简介：-今天测试百度api，遇到bug验证半天，最后发现是ip被墙，。所以寻找ip池方法"><a href="#简介：-今天测试百度api，遇到bug验证半天，最后发现是ip被墙，。所以寻找ip池方法" class="headerlink" title="简介： 今天测试百度api，遇到bug验证半天，最后发现是ip被墙，。所以寻找ip池方法"></a>简介： 今天测试百度api，遇到bug验证半天，最后发现是ip被墙，。所以寻找ip池方法</h1><p>1、由于已经有人写了相关的代码，因此只需要用来对照使用即可。</p>
<p>2、仍然记录一些细节，该程序的运行不是直接点main.py的run就完事了， 而是需要在cmd下额外输入命令。</p>
<p>3、 检测ok的ip会自动保存到mongodb，但中间因为软件兼容问题，可能遇到bson模块异常，记得升级mongoengine到最新。</p>
<p>4、不过抓了400个ip，只有10个返回ok，</p>
<p>参考  <a href="http://www.cnblogs.com/qiyeboy/p/5517271.html" target="_blank" rel="external">Scrapy爬取美女图片第三集 代理ip(上) (原创)
</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/09/21/bson模块丢失，原来是mongoengine坑-/" itemprop="url">
                  bson模块丢失，原来是mongoengine坑-
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Veröffentlicht am</span>
            <time itemprop="dateCreated" datetime="2016-09-21T21:20:28+08:00" content="2016-09-21">
              2016-09-21
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="简介：-运行别人的一个程序，提示缺少Bson模块，自然去安装，却发现没有名下的code模块"><a href="#简介：-运行别人的一个程序，提示缺少Bson模块，自然去安装，却发现没有名下的code模块" class="headerlink" title="简介： 运行别人的一个程序，提示缺少Bson模块，自然去安装，却发现没有名下的code模块"></a>简介： 运行别人的一个程序，提示缺少Bson模块，自然去安装，却发现没有名下的code模块</h1><p>1、原来这个模块是和mongodb相关的，当安装了bson模块，因版本问题，会引起一些文件缺失，</p>
<p>2、而mongodb官网上，也不建议外装bson：</p>
<p>到pymongo官方文档里查，第一句就是：<br>Warning Do not install the “bson” package. PyMongo comes with its own bson package; doing “pip install bson” or “easy_install bson” installs a third-party package that is incompatible with PyMongo.<br>PyMongo has no required dependencies.</p>
<p>3、还好最后安装最新的mongoengine，解决了这个问题。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/09/20/豆瓣电影的检索爬虫【python】/" itemprop="url">
                  豆瓣电影的检索爬虫【python】
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Veröffentlicht am</span>
            <time itemprop="dateCreated" datetime="2016-09-20T14:20:28+08:00" content="2016-09-20">
              2016-09-20
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="简介：-豆瓣网的电影分类比较清晰，比较了音乐板块，还是感觉电影板块用来做爬虫比较合适"><a href="#简介：-豆瓣网的电影分类比较清晰，比较了音乐板块，还是感觉电影板块用来做爬虫比较合适" class="headerlink" title="简介： 豆瓣网的电影分类比较清晰，比较了音乐板块，还是感觉电影板块用来做爬虫比较合适"></a>简介： 豆瓣网的电影分类比较清晰，比较了音乐板块，还是感觉电影板块用来做爬虫比较合适</h1><p>1、在参考他文的基础上，进行了学习分析。本文将记录和探讨细节部分。</p>
<p>2、url定义为电影板块： <a href="https://movie.douban.com/" target="_blank" rel="external">https://movie.douban.com/</a></p>
<p>3、遇到大型网站数据抓取，尤其是海外站点，因为延时造成的失败经常发生，遇到这样的情况，则建议用ui的加载until功能，在本测试案例中没有使用，因为测试结果响应较快。</p>
<p>4、因为要从大量的数据进行排序筛选，所以xpath的路径要尽可能的精确，如果直接用firebug的自带xpath地址，在排序检索，并提交内容时，可能因为路径有一个层级的差别导致失败报错。尽管很多时候，这个差别在图标点击上的效果一样。所以也是坑之一。</p>
<p>5、在运行程序前，看下内存使用率，如果较高就重启后再运行爬虫，否则由于selenium的加载浏览器比较消耗内存，很可能在此状态下，浏览器的响应延缓较大，因此，需要写好print信息，或者logging信息，以更明确程序执行点。</p>
<p>6、多选项排序的时候，建议做成gui，这是因为可以减少后续使用的重复输入。</p>
<p>7、一般来说，热评总是不缺，但是长评对于新电影是可能空缺的，为此，务必要加载try–except方式，来规避这样的情况。在本案例中，第六名的电影，正好没有长评，如果没有try except，则程序会在第六条就跳出终止了。</p>
<p>8、功能实现，为简单，只按照豆瓣电影默认的按照热门—-按热门排序，爬取了前20的电影的名字，网址，热评和长评信息，</p>
<p>9、代码如下：</p>
<pre><code># coding:utf-8
# 改编，精简版
# python 3.5.2
# 豆瓣网排名抓取和影评抓取V1.0

import time
from selenium import webdriver
import selenium.webdriver.support.ui as ui

url = &quot;https://movie.douban.com/&quot;
browser = webdriver.Firefox()
browser1 = webdriver.PhantomJS()
browser.get(url)

SUMRESOURCES = 0

# 热门
browser.find_element_by_xpath(&quot;//*[@id=\&quot;gaia_frm\&quot;]/div[1]/div[1]/label[1]&quot;).click()

# 按热度排序
browser.find_element_by_xpath(&quot;//*[@id=\&quot;gaia_frm\&quot;]/div[3]/div[1]/label[1]&quot;).click()

# 加载更多
browser.find_element_by_xpath(&quot;//*[@id=\&quot;gaia\&quot;]/div[4]/a&quot;).click()
time.sleep(2)

def get_title():

    global SUMRESOURCES
    # 定义抓top 20的电影
    num = 20
    # 定义抓长评
    long = 1
    for i in range(1, num+1):
            try:
                # 抓第i个图的基本信息
                title_list = browser.find_element_by_xpath(&quot;//*[@id=\&quot;gaia\&quot;]/div[4]/div/a[%d]&quot;%i)
                print(&apos;------------------NO--&apos;+ &apos;&apos;+str(SUMRESOURCES+1) +&apos;--------------------&apos;)
                print()
                print(&apos;电影名字:&apos;, title_list.text)
                film_link = title_list.get_attribute(&apos;href&apos;)
                print(&apos;电影链接:&apos;, film_link)
                SUMRESOURCES += 1
                get_detail(film_link,long)
            except:
                print(&apos;不能抓到影视信息&apos;)


def get_detail(url,long=0):
    browser1.get(url)

    # 简介内容
    jian_jie = browser1.find_element_by_xpath(&quot;/html/body/div[3]/div[1]/div/div[1]/div[3]/div/span[1]&quot;).text
    print(&apos;电影简介：&apos;, jian_jie)

    # 热门评论
    browser1.find_element_by_xpath(&quot;//*[@id=\&quot;hot-comments-tab\&quot;]&quot;).click()
    for i in range(1, 5):
        try:

            re_ping = browser1.find_element_by_xpath(&quot;//*[@id=\&quot;hot-comments\&quot;]/div[%d]/div/p&quot;%i).text
            print(&apos;-----&apos;+&apos;热评:&apos;+&apos;------&apos;)
            print(u&apos;最新热评:&apos;+ re_ping)
        except:
            print(&apos;抓取热评失败&apos;)



    if long == 1:
        try:
            # 点开下拉三角，展开长评
            # 使用对应img里自带的class
            browser1.find_element_by_xpath(&quot;//img[@class=&apos;bn-arrow&apos;]&quot;).click()
            time.sleep(1)

            long_get = browser1.find_element_by_xpath(&quot;//div[@class=&apos;review-bd&apos;]/div[2]/div&quot;)

            if long_get.text.encode(&apos;utf-8&apos;)==&apos;提示: 这篇影评可能有剧透&apos;:

                print(&apos;发现恶心的剧透！将跳过！&apos;)
                long_ping = browser1.find_element_by_xpath(&quot;//div[@class=&apos;review-bd&apos;]/div[2]/div[2]&quot;)
            else:
                long_ping = long_get
            print(&apos;----------------------------------------&apos;+&apos;长评:&apos;+&apos;----------------------------------------&apos;)
            print(&apos;长评:&apos;, long_ping.text)
        except:
            print(&apos;抓取长评失败&apos;)



if __name__==&quot;__main__&quot;:
    get_title()
</code></pre><p>10、参考： <a href="http://www.jianshu.com/p/bb4f2f7c62ed" target="_blank" rel="external">Python自定义豆瓣电影种类，排行，点评的爬取与存储（进阶上</a></p>
<p>11、前20个热门影评带长评，看了下很长,只保留了1和6的电影信息，有兴趣的就看看，没兴趣就忽略：</p>
<p>D:\Anaconda3\python.exe C:/Users/Administrator/PycharmProjects/untitled1/test/blog.py<br>——————NO–1——————–</p>
<p>电影名字: 釜山行 8.3<br>电影链接: <a href="https://movie.douban.com/subject/25986180/?tag=%E7%83%AD%E9%97%A8&amp;from=gaia" target="_blank" rel="external">https://movie.douban.com/subject/25986180/?tag=%E7%83%AD%E9%97%A8&amp;from=gaia</a><br>电影简介： 证券公司基金管理人石宇（孔侑 饰）光鲜精干，却也是个重利轻义之徒。妻子为此与之决裂，女儿秀安（金秀安 饰）则对如此自私的父亲越来越失望，决定前往釜山和母亲生活。在秀安生日这天，石宇抽出时间陪伴女儿登上开往釜山的特快列车。而与此同时，城市四处出现了极为可疑的暴动事件。政府极力洗白无法掩盖丧尸肆虐的事实，即便懵然无知的列车乘客也因为不速之客的到来而堕入恐慌绝望的地狱中。开车的刹那，一名感染者冲入车厢，而她很快尸变并对目光所及之处的健康人展开血腥屠杀。未过多久，丧尸便呈几何数爆发性地增长。石宇被迫和幸存者的乘客们在逼仄的空间中奋力求生。<br>通往釜山的遥遥旅途布满杀机，危难时刻每个幸存者的人性也承受巨大的考验……<br>—–热评:——<br>最新热评:一切灾难皆人性，唉，最后那一枪要是开了，就是神作了。<br>—–热评:——<br>最新热评:套路很深，煽情很猛！据说这部电影耗资100亿韩元，其中90亿用于几百名群演的霹雳舞教学费用。哦，对了，还要唱好歌，关键时候保命就靠他了。<br>—–热评:——<br>最新热评:僵尸进攻的部分比较过瘾，火车拖着一尾巴僵尸的部分视觉效果很有趣；人文部分十分韩剧，社会讽刺过于直接而显得无趣了，男主那洗衣液CF般的闪回、洗手间哭泣都太cheesy，其实马东锡才是英雄，只是被塑造得不壮烈罢了；镜头感、节奏感so so，演员们演得不够害怕，有的部分特别明显。拜托，是僵尸也！<br>—–热评:——<br>最新热评:比预想的好，《僵尸世界大战》的僵尸模式，虽然不可避免的出现了很多套路与恶意煽情的东西，但是，惊悚的氛围把握还是挺准确的<br>—————————————-长评:—————————————-<br>长评: 我记得好久以前，在我上次回国以前吧，悉尼的电影行业某朋友跟我提过她的老板投了一个韩国僵尸片，我当时就觉得投这个干嘛啦韩国拍的僵尸片必须没什么好看的嘛，毕竟“僵尸片是欧美的类型片”这个印象已经根深蒂固了。<br>后来《釜山行》出来，被各种好评，我挺惊讶。前两天看了个微博Po的推荐终于忍不住，今天约了朋友去看掉了。看完以后到现在三个小时了，我还感觉闷闷的，一面觉得“真好看呀”一面为我们中国电影被韩国电影抛在后面的、越来越遥远的距离感到焦虑。<br>《釜山行》就是一个典型的僵尸片，和我们看过的所有僵尸电影拥有完全一样的套路，有封闭空间，有感染有逃命，有对抗有小聪明，有煽情有生死，这里有的《行尸走肉》呀《僵尸肖恩》啊它们都有过。但是《釜山行》是真正意义上的属于亚洲的僵尸片，它充满了身为亚洲人会各种共鸣而欧美人绝对想不到用的元素与梗：亚洲社会独一份儿的伦理道德、人情世故、因果报应……全都被调进了故事里。<br>看一个僵尸片到最后不止被圆满地吓了一圈，还感动得五体投地，哭得稀里哗啦。我好几次以为自己扛过了哭点，没想到最后还是败给了一段父女的对话。眼泪止也止不住。<br>这个片子在你以为充满希望的时候突然就绝望了，又在你终于绝望以后留了一点希望。<br>剧本是工工整整的好莱坞商业大片的模式，第几分钟出现几个事件几个转折几个主要角色几对人物关系—-全部是教科书的模版，以最快速度发展故事，让每一分钟的情节都饱满生动（用饺子比喻的话就是塞满了肉馅），丝毫不用一点闲话家常来浪费镜头时间。<br>知道如何利用一个满是缺点的主角的成长与改变让观众代入角色（少年漫画的套路），知道怎么运用人气角色的命运来影响观众情绪，知道留一个情节上的“关键道具”（儿歌）并在正确的位置使用了三次……两个小时里十几个人物全部立住了，观众能清晰地分辨他们记住他们并对他们拥有不同的解读—-就人设这一点，多成功啊。从好莱坞学到的，已经完全变成自己的了。</p>
<p>几年前上学的时候，我的澳洲电影老师专门开了一节课讲韩国电影的崛起与风格，讲朴赞郁讲《老男孩》，当年让我一个对“韩国电影”四个字充满鄙视的无知少女差点跪着出了教室。我那时候觉得中韩两国电影的差距至少有10年那么多吧。<br>如今，这些年过去，我隐约觉得中国电影多少也有了一丢丢进步，结果看完《釜山行》，人家他妈的又一下子甩开了我们十多年。</p>
<p>怎么追啊。继续追吧。心累累的，哎。</p>
<p>——————NO–6——————–</p>
<p>电影名字: 我们这种叛徒 6.3<br>电影链接: <a href="https://movie.douban.com/subject/10461676/?tag=%E7%83%AD%E9%97%A8&amp;from=gaia" target="_blank" rel="external">https://movie.douban.com/subject/10461676/?tag=%E7%83%AD%E9%97%A8&amp;from=gaia</a><br>电影简介： 牛津大学导师佩里和女友嘉儿在怡人的安提瓜岛上享受美妙的假期时，偶然结识了俄国富豪迪马——一场精彩的网球比赛让佩里和迪马的人生轨迹有了交集。谁料，迪马竟是俄国犯罪组织的洗钱专家，组织内部斗争的残酷令迪马萌生去意，于是他希望通过佩里向英国情报部门传达寻求政治庇护的请求……<br>自此，佩里和嘉儿渐渐脱离了正常的生活轨道，一系列政治阴谋、间谍行动扑面而来，就此开启了一场惊心动魄的跨国逃亡之旅：从法国巴黎到瑞士阿尔卑斯山，再到伦敦城里黑暗的走廊，哪里才是安全港湾？谁才是真正的叛徒？<br>—–热评:——<br>最新热评:点解会拣你？因为当时没有其他人啊。没得拣的情况下可以得到的最好结果。伊万最近几年都不行。<br>—–热评:——<br>最新热评:一万同志从角色设定到演技都毫无吸引力啊，四哥她爹演技还是有功底的（里面这女儿真是坑爹没商量），全部注意力都被Brody的fashion show吸引去了，玳瑁镜、围巾、风衣、三件套、居家服简直英国范儿到不行。剧情一般般啦，勒卡雷叔叔的水平应该不是这个level的吧……<br>—–热评:——<br>最新热评:比老婆收入低形象倒是挺符合，但是真的不适合长发。剧情没兴趣看。<br>—–热评:——<br>最新热评:3.5/5 摄影大加分！想金盆洗手奈何身不由己。Dima人物塑造得不错。<br><strong>抓取长评失败</strong></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/09/20/163和qq邮箱的自动登录--selenium【python】/" itemprop="url">
                  163和qq邮箱的自动登录--selenium【python】
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Veröffentlicht am</span>
            <time itemprop="dateCreated" datetime="2016-09-20T10:20:28+08:00" content="2016-09-20">
              2016-09-20
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="简介：-之前直接测试mail-q63-com-以及-mail-qq-com的selenium登录都受阻，网站采用了较强的反爬技术，如动态id等，故用规避方法登陆"><a href="#简介：-之前直接测试mail-q63-com-以及-mail-qq-com的selenium登录都受阻，网站采用了较强的反爬技术，如动态id等，故用规避方法登陆" class="headerlink" title="简介： 之前直接测试mail.q63.com 以及 mail.qq.com的selenium登录都受阻，网站采用了较强的反爬技术，如动态id等，故用规避方法登陆"></a>简介： 之前直接测试mail.q63.com 以及 mail.qq.com的selenium登录都受阻，网站采用了较强的反爬技术，如动态id等，故用规避方法登陆</h1><p>1、在参考他文的基础上，做了代码重构。QQ邮箱在右上角有“基本版”登录，没有做加强反爬处理，而163的，则是通过www.163.com去找邮箱，发现接口是email.163.com，注意多了一个字母e，是较早的登录接口。</p>
<p>2、代码(请把你的帐号密码代替xxxx)，增加了自动判断输入是否正确：</p>
<pre><code># coding:utf-8
# python 3.5.2
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
import selenium.webdriver.support.ui as ui
import time

# 选择QQ还是163邮箱
email_type = input(&quot;1-QQ免费邮箱\n2-163免费邮箱\n请输入对应数字：&quot;)

def run():
    global email_type
    if email_type != str(1) or email_type != str(2):
        print (&quot;输入错误，请重新输入 :)&quot;)
        email_type = input(&quot;1-QQ免费邮箱\n2-163免费邮箱\n请输入对应数字：&quot;)

    if email_type == str(1):
        url = &quot;https://ui.ptlogin2.qq.com/cgi-bin/login?style=9&amp;appid=522005705&amp;daid=4&amp;s_url=https%3A%2F%2Fw.mail.qq.com%2Fcgi-bin%2Flogin%3Fvt%3Dpassport%26vm%3Dwsk%26delegate_url%3D%26f%3Dxhtml%26target%3D&amp;hln_css=http%3A%2F%2Fmail.qq.com%2Fzh_CN%2Fhtmledition%2Fimages%2Flogo%2Fqqmail%2Fqqmail_logo_default_200h.png&amp;low_login=1&amp;hln_autologin=%E8%AE%B0%E4%BD%8F%E7%99%BB%E5%BD%95%E7%8A%B6%E6%80%81&amp;pt_no_onekey=1&quot;
        driver = webdriver.Firefox()
        driver.get(url)
        time.sleep(2)
        driver.find_element_by_xpath(&quot;/html/body/div[1]/div[2]/div[4]/ul/li[1]/input&quot;).send_keys(&quot;xxxx&quot;)
        driver.find_element_by_xpath(&quot;/html/body/div[1]/div[2]/div[4]/ul/li[2]/input&quot;).send_keys(&quot;xxxx&quot;)
        driver.find_element_by_xpath(&quot;//html/body/div[1]/div[2]/div[4]/div[2]&quot;).click()
        print(&apos;--------------Log In------------&apos;)
        time.sleep(1)


    elif email_type == str(2):
        url = &quot;http://email.163.com/&quot;

        driver = webdriver.Firefox()
        driver.get(url)
        time.sleep(2)
        driver.find_element_by_xpath(&quot;/html/body/div[1]/div[1]/div/div[2]/div[2]/div/form/div[1]/label&quot;).send_keys(&quot;xxxx&quot;)
        driver.find_element_by_xpath(&quot;/html/body/div[1]/div[1]/div/div[2]/div[2]/div/form/div[2]/label&quot;).send_keys(&quot;xxxx&quot;)
        driver.find_element_by_xpath(&quot;/html/body/div[1]/div[1]/div/div[2]/div[2]/div/form/div[2]/button&quot;).click()
        print(&apos;--------------Log In------------&apos;)
        time.sleep(1)


run()
</code></pre><p>3、pycharm输出：</p>
<p>1-QQ免费邮箱<br>2-163免费邮箱<br>请输入对应数字：4<br>输入错误，请重新输入 :)<br>1-QQ免费邮箱<br>2-163免费邮箱<br>请输入对应数字：2<br>————–Log In————</p>
<p>Process finished with exit code 0</p>
<p>4、参考：</p>
<p>  <a href="http://www.jianshu.com/p/a78b6bb95543" target="_blank" rel="external">【伪】解决动态id元素Selenium无法捕捉自动登录问题</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/09/19/关于Xpath地址的分析小结/" itemprop="url">
                  关于Xpath地址的分析小结
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Veröffentlicht am</span>
            <time itemprop="dateCreated" datetime="2016-09-19T20:20:28+08:00" content="2016-09-19">
              2016-09-19
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="简介：-发现有时候遇到xpath的路径报错，然而使用的都是一些插件工具，但不同浏览器和不同插件得到的xpath又有所差别，故做一个测试记录"><a href="#简介：-发现有时候遇到xpath的路径报错，然而使用的都是一些插件工具，但不同浏览器和不同插件得到的xpath又有所差别，故做一个测试记录" class="headerlink" title="简介： 发现有时候遇到xpath的路径报错，然而使用的都是一些插件工具，但不同浏览器和不同插件得到的xpath又有所差别，故做一个测试记录"></a>简介： 发现有时候遇到xpath的路径报错，然而使用的都是一些插件工具，但不同浏览器和不同插件得到的xpath又有所差别，故做一个测试记录</h1><p>1、旁引： 因故装python 3 +2 的双蛇系统，是故再次要把anaconda配置到python 2 去然后搜网址的时候，看到一个评论，喜感，记录如下：</p>
<p>问：anaconda与python什么关系</p>
<p>答复：<br>python 是莽蛇，一般是陆上蛇<br>Anaconda 是美洲大水蛇。</p>
<p>好吧：如果英文不好的，可以用简友0han写的翻译小程序来看下：</p>
<p>===============================</p>
<h1 id="有道词典-命令行版v1-1-by-0han"><a href="#有道词典-命令行版v1-1-by-0han" class="headerlink" title="=有道词典 命令行版v1.1 by 0han="></a>=有道词典 命令行版v1.1 by 0han=</h1><p>输入’q’可退出程式</p>
<p>请输入词语：python</p>
<p>翻译:n. 巨蟒；大蟒</p>
<p>请输入词语：anaconda</p>
<p>翻译:n. 水蟒；蟒蛇</p>
<p>请输入词语：</p>
<p><strong><br>是不是可以写一部： 水与火之歌  ：） </strong></p>
<p>2、以豆瓣网电影板块的“热门”按键为例，如图：</p>
<p><img src="http://ocg7i7pt6.bkt.clouddn.com/blog/2016-09-19/181325132" alt=""></p>
<p>在chrome里的xpath是：</p>
<pre><code>//*[@id=&quot;gaia_frm&quot;]/div[1]/div[1]/label[1]
</code></pre><p>在firefox 的firebug插件：：</p>
<pre><code>/html/body/div[3]/div[1]/div/div[2]/div[4]/div[2]/div[1]/form/div[1]/div[1]/label[1]
</code></pre><p>而firexfox的xpath checker插件则是：</p>
<pre><code>id(&apos;gaia_frm&apos;)/x:div[1]/x:div[1]/x:label[1]
</code></pre><p>又测试QQ浏览器为：</p>
<pre><code>//*[@id=&quot;gaia_frm&quot;]/div[1]/div[1]/label[1]
</code></pre><p>可以发现，谷歌和qq浏览器的格式一致，但firefox的插件，则不尽相同，但最起码，倒过来看，还是基本相同，这其实是路径开头选择差异导致的，而到路径最后基本都一样，这可以理解为绝对路径和相对路径，前者最开始一个斜杠，后者2个斜杠。那即便后面就肯定一样？但还要看“后面”的层级定义，即便到达最后一级的标签，采用的是什么属性定位，比如通过id，通过name等的区别。</p>
<ul>
<li><p>其中，firefox的xpath checker插件显示的比较特别，带有X： 字样，估计是该插件自定义的xpath格式，但如果直接复制到python里，是不识别的，因此会报错。所以，如果要采用他的格式，需要把x： 给删掉。测试就通过了。也就是要修改成：</p>
<p>  driver.find_element_by_xpath(“id(‘gaia_frm’)/div[1]/div[1]/label[1]”)</p>
</li>
</ul>
<ul>
<li><p>谷歌和qq的一样，只需要测试一个，结论直接通过：</p>
<p>  driver.find_element_by_xpath(“//*[@id=\”gaia_frm\”]/div[1]/div[1]/label[1]”)</p>
</li>
</ul>
<ul>
<li><p>firebug的，结论通过：</p>
<p>  driver.find_element_by_xpath(“/html/body/div[3]/div[1]/div/div[2]/div[4]/div[2]/div[1]/form/div[1]/div[1]/label[1]”</p>
</li>
</ul>
<p>结论：</p>
<ul>
<li>谷歌和qq浏览器自带的xpath路径分析，可通过常规需求的测试（由于有隐藏元素的一些网页功能，遇到的话依然可能失败，还有就是动态刷新技术）</li>
<li>由于我安装的firefox没看到自带的xpath分析功能，所以安装了firebug插件以及xpath checker插件，但xpath checker插件自带的格式却得不到浏览器的支持，需要特别注意。</li>
<li>xpath checker在反向验证xpath路径的时候还是有用。</li>
</ul>
<p>关于最后一点展开下，也是经验总结：</p>
<p>比如，你看别人的代码分析，因为你不确定他是使用浏览器自带的，还是自己定义的，还是插件的xpath路径，甚至可能对方根本没有任何注释，导致你看到xpath路径的时候，一头的晕，丫到底写的是啥呀？也许聪明的你会说，那可以到网页源代码查找一部分的关键词，确实可以，但也麻烦，举例来说：</p>
<pre><code>driver_item.find_element_by_xpath(&quot;//div[@class=&apos;list-wp&apos;]/a[@class=&apos;more&apos;]&quot;).click()
</code></pre><p>虽然可以知道要点击一个含有class=’more’的控件元素，但你会发现直接网页源代码木有！！ 为何？ 因为他的真实是双引号class=”more”<br>原来，因为selenium 的find_element_by_xpath(“XXX”) 命令，如果你把XXX用class=”more”直接代替，绝对的报错，为何，因为双引号冲突， 你可以反斜杠来区分，抑或用单引号，这就是原因所在。可见，即便你到网页源代码查找还是麻烦的很，更不要说，如果万一源代码里有好几个这样的查找单元了。</p>
<p>而我们用，xpath checker反向验证，可以很快的让你知道，对方分析的是什么元素，如下图：</p>
<p><img src="http://ocg7i7pt6.bkt.clouddn.com/blog/2016-09-19/185937608" alt=""></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/09/18/stocksnap 一个不错的图片网站的图片抓取/" itemprop="url">
                  stocksnap 一个不错的图片网站的图片抓取--selenium 右键保存和直接写入2个模式【python】
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Veröffentlicht am</span>
            <time itemprop="dateCreated" datetime="2016-09-18T10:59:28+08:00" content="2016-09-18">
              2016-09-18
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="参考：原文1"><a href="#参考：原文1" class="headerlink" title="参考：原文1"></a>参考：<a href="http://www.jianshu.com/p/2528edf4485c" target="_blank" rel="external">原文1</a></h1><h1 id="参考：原文2"><a href="#参考：原文2" class="headerlink" title="参考：原文2"></a>参考：<a href="http://blog.csdn.net/seanwang_25/article/details/43318907" target="_blank" rel="external">原文2</a></h1><p>1、原文1采用了scrapy方法，本文改编用selenium方法，并参考原文2首次采用模拟右键来保存图片。</p>
<p>2、网站分析：</p>
<p>首先发现鼠标拖动到图片底部区域，主页又不断加载新的图片，可判断是异步的。另单独查看2个图的xpath，发现总体格式一致，编号部分有差异：</p>
<pre><code>/html/body/div[4]/div[3]/div[2]/div/div[1]/a/img

/html/body/div[4]/div[3]/div[2]/div/div[11]/a/img
</code></pre><p>这样我们就得到了在firefox下统一的xpath为，注意是删除div[i]部分，留下2个斜杠：</p>
<pre><code>/html/body/div[4]/div[3]/div[2]/div//a/img
</code></pre><p>此时，可能觉得为啥要删除，而不是用正则式.*代替？测试那样的结果是Nan，也就是说xpath的格式和re格式不能混搭。</p>
<p>而每个jpg地址在src属性中，所以如果想查阅批量的图片地址，则：</p>
<pre><code>/html/body/div[4]/div[3]/div[2]/div//a/img/@src
</code></pre><p><img src="http://ocg7i7pt6.bkt.clouddn.com/stocksnap%20%E5%9B%BE%E7%89%87%E7%BD%91%E5%9D%80.png" alt=""></p>
<p>3、既然这么爽的得到了图片地址，剩下就是批量下载保存了：</p>
<p>这一次，先测试了用firefox模拟人工右键保存的方法，代码如下，保存那块还没整明白，也就是自动到出现图片保存的界面,根本原因是selenium无法操作操作系统级的对话框：</p>
<p>索性快速人工点保存，30个图的保存位置都是重复的，依次得到30个图：</p>
<p><img src="http://ocg7i7pt6.bkt.clouddn.com/selenium%2030pics.png" alt=""></p>
<p>4、随机打开一个图，如下，可见测试ok：<br><img src="http://ocg7i7pt6.bkt.clouddn.com/t1.png" alt=""></p>
<p>5、代码：</p>
<pre><code># -*- coding: utf-8 -*-
# python 3.5.2
# Author:vansnowpea
# stocksnap 一个不错的图片网站的图片，右键保存抓取

from selenium import webdriver
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.common.keys import Keys


print(&apos;Please wait...Firefox loading...&apos;)
print(&apos;---------------------------------&apos;)


url = &quot;https://stocksnap.io/&quot;

# 用浏览器实现访问
driver = webdriver.Firefox()
driver.maximize_window()
driver.get(url)

# 得到总的jpgs的路径集合
xpath = &quot;/html/body/div[4]/div[3]/div[2]/div//a/img&quot;

# set profile
fp = webdriver.FirefoxProfile()
fp.set_preference(&apos;browser.download.folderList&apos;, 2)
fp.set_preference(&apos;browser.download.manager.showWhenStarting&apos;, False)
fp.set_preference(&apos;browser.download.dir&apos;, &apos;./yourfolder/&apos;)
fp.set_preference(&apos;browser.helperApps.neverAsk.saveToDisk&apos;, &apos;image/jpeg&apos;)



# 保存图片,人工批量点保存，selenium无法操作操作系统级的对话框
for element in driver.find_elements_by_xpath(xpath):
    img_url = element.get_attribute(&apos;src&apos;)
    img_desc = element.get_attribute(&apos;data-desc&apos;)

    action = ActionChains(driver).move_to_element(element)
    action.context_click(element)
    action.send_keys(Keys.ARROW_DOWN)
    action.send_keys(&apos;v&apos;)
    action.perform()

print(&apos;Well done! all pictures downloaded.&apos;)
print(&apos;---------------------------------&apos;)

# driver.close()
</code></pre><p>6、如果图片数量少，人工保存下也无妨，但数量大肯定不行，所以还是用常规的自动写入数据保存的方式。另外此网站是通过ajax异步加载，当鼠标放到首页30个图的下方，也就是浏览器底部区域，他又会自动加载新的图片出来。而通过新的第一层代码的模拟鼠标下移，可以得到更多的图片。<strong>千万要注意的是，在python 3中，对数dict的关键词查找是in，比如：if n in previous:而在python 2 中是has_key,比如：if previous.has_key(n):</strong>对应代码为：</p>
<pre><code># python 3.5.2
from selenium import webdriver  
import time  
import urllib


# 爬取页面地址  
url = &quot;https://stocksnap.io/&quot;

# 目标元素的xpath  
xpath = &quot;/html/body/div[4]/div[3]/div[2]/div//a/img&quot;

# 启动Firefox浏览器  
driver = webdriver.Firefox()  

# 最大化窗口，因为每一次爬取只能看到视窗内的图片  
driver.maximize_window()  

# 记录下载过的图片地址，避免重复下载  
img_url_dic = {}  

# 浏览器打开爬取页面  
driver.get(url)  

# 模拟滚动窗口以浏览下载更多图片  
pos = 0  
m = 0 # 图片编号  
for i in range(10):  
    pos += i*500 # 每次下滚500  
    js = &quot;document.documentElement.scrollTop=%d&quot; % pos  
    driver.execute_script(js)  
    time.sleep(1)     

    for element in driver.find_elements_by_xpath(xpath):  
        img_url = element.get_attribute(&apos;src&apos;)  
        # 保存图片到指定路径  
        if img_url != None and not img_url in img_url_dic:

            img_url_dic[img_url] = &apos;&apos;  
            m += 1  
            ext = img_url.split(&apos;.&apos;)[-1]  
            filename = str(m) + &apos;.&apos; + ext  
            #保存图片数据  
            data = urllib.request.urlopen(img_url).read()
            f = open(&apos;./van/&apos; + filename, &apos;wb&apos;)
            f.write(data)  
            f.close()  
driver.close()  
</code></pre><p>7、结果展示：相对第一个方法，获取了更多的图片：<br><img src="http://ocg7i7pt6.bkt.clouddn.com/stocksnap%20ajax%20more%20pics.png" alt=""></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/09/17/基于Selenium一键写CSDN博客并做成exe文件/" itemprop="url">
                  基于Selenium一键写CSDN博客并做成exe文件【python】
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Veröffentlicht am</span>
            <time itemprop="dateCreated" datetime="2016-09-17T11:59:28+08:00" content="2016-09-17">
              2016-09-17
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="参考：原文1和原文2"><a href="#参考：原文1和原文2" class="headerlink" title="参考：原文1和原文2"></a>参考：<a href="http://www.jianshu.com/p/a81800774d91" target="_blank" rel="external">原文1</a>和<a href="http://blog.csdn.net/mrlevo520/article/details/51840217" target="_blank" rel="external">原文2</a></h1><p>1、代码，原文python2，修改成了python 3，作者信息保留，请修改CSDN帐号登录信息：</p>
<pre><code># -*- coding: utf-8 -*-
# python 3.5.2
#Author:哈士奇说喵
#CSDN--实现一键写博客

from selenium import webdriver
import time

#shift-tab多行缩进(左)
print (&apos;Please wait...Firefox loading...&apos;)
print (&apos;---------------------------------&apos;)
#reload(sys)

PostUrl = &quot;https://passport.csdn.net/account/login?from=http://my.csdn.net/my/mycsdn&quot;
driver=webdriver.Firefox()#用浏览器实现访问
#driver = webdriver.PhantomJS(executable_path=&quot;phantomjs.exe&quot;)#没用浏览器
driver.get(PostUrl)


#账号填充输入
elem_user = driver.find_element_by_id(&apos;username&apos;)
elem_psw = driver.find_element_by_id(&apos;password&apos;)

#可以自己修改登录名和账户密码，我自己的隐去了
elem_user.send_keys(&apos;41111111@qq.com&apos;)
elem_psw.send_keys(&apos;1111111&apos;)


#点击登录
#click_login = driver.find_element_by_xpath(&quot;//input[@class=&apos;logging&apos;]&quot;)
click_login = driver.find_element_by_xpath(&quot;//input[@class=&apos;logging&apos;]&quot;)
click_login.click()
print( &apos;log in...&apos;)
print (&apos;---------------------------------&apos;)
time.sleep(1)

#先点击写博客图标，不然元素隐藏

click_wbic = driver.find_element_by_xpath(&quot;//ul[@class=&apos;btns&apos;]/li[5]&quot;)
click_wbic.click()
print (&apos;jumping...&apos;)
print (&apos;---------------------------------&apos;)


click_choice = driver.find_element_by_xpath(&quot;//div[@class=&apos;wrap clearfix&apos;]/dl/dt[4]/a&quot;)
click_choice.click()#将点击操作放在内部比较好
#定位新页面元素，将handle重定位即可
driver.switch_to_window(driver.window_handles[1])#定位弹出的第一个页面，也就是当前页面
click_markdown = driver.find_element_by_xpath(&quot;//p[@class=&apos;subtit&apos;]/a&quot;)
click_markdown.click()
print (&apos;---------------------------------&apos;)
print (&apos;here we go!&apos;)
#关闭无关页面，也可以根据自己喜好保留无关页面
driver.close()#关闭第二个页面，也就是一般编辑器下的CSDN
driver.switch_to_window(driver.window_handles[0])#关闭第1个页面，也就是登录主页
driver.close()
</code></pre><p>2、代码分析：<br>一些按键的定位是使用了xpath的编写方式，以“登录”为例，右键，检查，然后找到最近的一个class，因为这个登录直接在一个class 里，如下代码：</p>
<pre><code>&lt;input class=&quot;logging&quot; accesskey=&quot;l&quot; value=&quot;登 录&quot; tabindex=&quot;6&quot; type=&quot;button&quot; data-form-sbm=&quot;1474112762583.3015&quot;&gt;
</code></pre><p>所以根据xpath的路径规则，是 //input[@class=’logging’]，然后，配合selenium的xpath选定语法，写为：</p>
<pre><code>click_login = driver.find_element_by_xpath(&quot;//input[@class=&apos;logging&apos;]&quot;)
</code></pre><p>而写博客的按键，有所小复杂，因为检查的时候，不是直接在类里，需要向上寻找一个父类，注意不是平行的类，因此代码相对复杂点，形如：</p>
<pre><code>click_wbic = driver.find_element_by_xpath(&quot;//ul[@class=&apos;btns&apos;]/li[5]&quot;)

click_choice = driver.find_element_by_xpath(&quot;//div[@class=&apos;wrap clearfix&apos;]/dl/dt[4]/a&quot;)

click_markdown = driver.find_element_by_xpath(&quot;//p[@class=&apos;subtit&apos;]/a&quot;)
</code></pre><p>3、xpath的路径分析是直接通过检查中的代码换成xpath模式，而不是右键 copy–&gt; copy xpath模式。</p>
<p>4、做成exe登录更简单些。</p>
<p>5、全文需要安装firefox，为了保证和selenium兼容，我的是46英文版。</p>
<p>6、又查阅了其他人的xpath分析方法，发现不一定要通过class来区分，如下：</p>
<pre><code>&lt;input type=&quot;text&quot; name=&quot;passwd&quot; id=&quot;passwd-id&quot; /&gt;
</code></pre><p>这段html语言，可以用以下的几种方法来定位。</p>
<pre><code>element = driver.find_element_by_id(&quot;passwd-id&quot;)
element = driver.find_element_by_name(&quot;passwd&quot;)
element = driver.find_elements_by_tag_name(&quot;input&quot;)
element = driver.find_element_by_xpath(&quot;//input[@id=&apos;passwd-id&apos;]&quot;)
</code></pre><p>而且你在用 xpath 的时候还需要注意的是，如果有多个元素匹配了 xpath，它只会返回第一个匹配的元素。如果没有找到，那么会抛出 NoSuchElementException 的异常。</p>
<p>7、由于firefox的加载比较慢，所以想通过PhantomJS来测试，但是发现他不识别既定的xpath路径，应该说是部分不识别，开头的还是识别的，中间的报错，坑爹啊。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/09/17/Firefox不能运行selenium的故障排查/" itemprop="url">
                  Firefox不能运行selenium的故障排查
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Veröffentlicht am</span>
            <time itemprop="dateCreated" datetime="2016-09-17T11:59:28+08:00" content="2016-09-17">
              2016-09-17
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="说明-Firefox中文最新版48不能运行selenium2-53-6的故障排查"><a href="#说明-Firefox中文最新版48不能运行selenium2-53-6的故障排查" class="headerlink" title="说明:Firefox中文最新版48不能运行selenium2.53.6的故障排查"></a>说明:Firefox中文最新版48不能运行selenium2.53.6的故障排查</h1><p>1、错误提示：  </p>
<pre><code>  File &quot;D:\Anaconda3\lib\site-packages\selenium\webdriver\firefox\firefox_binary.py&quot;, line 103, in _wait_until_connectable
    raise WebDriverException(&quot;Can&apos;t load the profile. Profile &quot;
selenium.common.exceptions.WebDriverException: Message: Can&apos;t load the profile. Profile Dir: %s If you specified a log_file in the FirefoxBinary constructor, check it for details.
</code></pre><p>2、排查版本： selenium已经是最新版2.53.6  firexfox也是最新中文版48</p>
<p>谷歌： </p>
<pre><code>http://stackoverflow.com/questions/37693106/selenium-2-53-not-working-on-firefox-47/37693374
</code></pre><p>上面给了一个在selenium 在2.53.0时代，运行firefox 47版本的方法，感觉麻烦，所以我优选了退回低版本的策略，如下：</p>
<p>3、评估是否firefox的48最新版还不支持？所以到firefox官网上，直接看到的最后的老版本是47， 测试安装了中文版，结果一样，继续卸载看46版本，使用的以下网址：</p>
<pre><code>https://ftp.mozilla.org/pub/firefox/releases/
https://ftp.mozilla.org/pub/firefox/releases/46.0.1/win64/en-US/
</code></pre><p>一下就成功了。</p>
<p>4、后来翻阅stackoverflow的帖子，发现有人反馈2.53.6已经支持firefox 48了。 但是我的咋就不行？ 猜测区别就是中文和英文，所以我就把46版本卸载了测试48英文版， </p>
<pre><code>https://ftp.mozilla.org/pub/firefox/releases/48.0b1/win64/en-US/
</code></pre><p>结果直接跳出firefox已停止工作。</p>
<p>5、结论：就2.53.6的selenium版本，测试firefox 46 英文版稳定运行。47和48版本失败。操作系统Win10，所以建议使用46版本的firefox。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel  sidebar-panel-active ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="Van" />
          <p class="site-author-name" itemprop="name">Van</p>
          <p class="site-description motion-element" itemprop="description"></p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">45</span>
              <span class="site-state-item-name">Artikel</span>
            </a>
          </div>

          

          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2016</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Van</span>
</div>

<div class="powered-by">
  Erstellt mit  <a class="theme-link" href="https://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  



  




  
  

  

  

  

</body>
</html>
