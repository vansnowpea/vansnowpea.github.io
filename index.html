<!doctype html>



  


<html class="theme-next muse use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1" />






<meta property="og:type" content="website">
<meta property="og:title" content="A Song of Python and Anaconda">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="A Song of Python and Anaconda">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="A Song of Python and Anaconda">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: 'Author'
    }
  };
</script>




  <link rel="canonical" href="http://yoursite.com/"/>

  <title> A Song of Python and Anaconda </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">A Song of Python and Anaconda</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/10/21/如何优雅的建立自己的SS5实现轻松快速翻墙/" itemprop="url">
                  阿里云ECS建立微信公众号----以itchatmp为例【python】
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2016-10-21T11:59:28+08:00" content="2016-10-21">
              2016-10-21
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="阿里云ECS建立微信公众号—-以itchatmp为例"><a href="#阿里云ECS建立微信公众号—-以itchatmp为例" class="headerlink" title="阿里云ECS建立微信公众号—-以itchatmp为例"></a>阿里云ECS建立微信公众号—-以itchatmp为例</h1><p>1、由于近期发现itchat开源项目要升级到itchatmp , 所以考虑下把自己的阿里云ECS利用起来，不然有点浪费了。</p>
<p>2、首先熟悉下flask的hello world实现,把这段代码放到ECS上：</p>
<pre><code>#encoding:utf8
from flask import Flask, render_template

app = Flask(__name__)
@app.route(&apos;/&apos;)
def hello_world():
    content = &apos;Hello World!&apos;
    return render_template(&apos;hello.html&apos;, content = content)

if __name__ == &apos;__main__&apos;:
    app.run(&apos;0.0.0.0&apos;)
</code></pre><p>其中：@app.route(‘/‘) 是定义的路由，表示对应网址的根目录。<br>然后在本机输入ECS的ip加上端口5000，应该就能看到hello world了。</p>
<p>3、 </p>
<p>flask的 中文文档资料： <a href="http://dormousehole.readthedocs.io/en/latest/" target="_blank" rel="external">http://dormousehole.readthedocs.io/en/latest/</a></p>
<p> flask的基础视频参考： <a href="http://www.jikexueyuan.com/course/943.html" target="_blank" rel="external">http://www.jikexueyuan.com/course/943.html</a></p>
<p>4、 以itchatmp为例：</p>
<p>首先根据itchatmp的github网址： <a href="https://github.com/littlecodersh/itchatmp" target="_blank" rel="external">https://github.com/littlecodersh/itchatmp</a></p>
<p>下载到本地，然后</p>
<pre><code>pip install itchatmp
</code></pre><p>在这个过程中，很可能安装失败，是因为墙的因素，建议开启翻墙工具</p>
<p>在安装好之后，把对应的示范脚本，根据你的微信订阅号的相关参数进行修改：</p>
<pre><code>import itchatmp

itchatmp.update_config(itchatmp.WechatConfig(
    token=&apos;xxxxxxxxxx&apos;,
    appId = &apos;xxxxxxxxxx&apos;,
    appSecret = &apos;xxxxxxxxxx&apos;))

@itchatmp.msg_register(itchatmp.content.TEXT)
def text_reply(msg):
    return msg[&apos;content&apos;]

itchatmp.run()
</code></pre><p>5、然后到微信订阅号的后台，把url部分，修改成你的ECS的ip。</p>
<p>6、运行一个形如weixinchat.py的脚本，把上述代码复制，并运行。一切顺利的话，你的订阅号就工作了，此时，他的默认功能是，当你输入信息后，订阅号自动回复相同内容。</p>
<p>7、本文案例是在Win10，Win NT 2008下都测试通过，</p>
<p>8、如果你需要一些Linux相关的参考资料，如下：</p>
<ul>
<li><a href="https://shanguangyu.com/articles/wechat-ECS/" target="_blank" rel="external">阿里云ECS搭建微信公众平台</a></li>
<li><a href="http://jayveehe.github.io/2015/01/26/nginx-flask/" target="_blank" rel="external">微信公众平台接入初探</a></li>
<li><a href="http://www.oschina.net/translate/serving-flask-with-nginx-on-ubuntu?cmp" target="_blank" rel="external">在 Ubuntu 上使用 Nginx 部署 Flask 应用</a></li>
<li><a href="http://www.cnblogs.com/weishun/p/weixin-publish-developing.html" target="_blank" rel="external">微信公众平台开发(免费云BAE+高效优雅的Python+网站开放的API)</a></li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/10/21/阿里云建立微信公众号的相关资料/" itemprop="url">
                  阿里云ECS建立微信公众号----以itchatmp为例【python】
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2016-10-21T11:59:28+08:00" content="2016-10-21">
              2016-10-21
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="阿里云ECS建立微信公众号—-以itchatmp为例"><a href="#阿里云ECS建立微信公众号—-以itchatmp为例" class="headerlink" title="阿里云ECS建立微信公众号—-以itchatmp为例"></a>阿里云ECS建立微信公众号—-以itchatmp为例</h1><p>1、由于近期发现itchat开源项目要升级到itchatmp , 所以考虑下把自己的阿里云ECS利用起来，不然有点浪费了。</p>
<p>2、首先熟悉下flask的hello world实现,把这段代码放到ECS上：</p>
<pre><code>#encoding:utf8
from flask import Flask, render_template

app = Flask(__name__)
@app.route(&apos;/&apos;)
def hello_world():
    content = &apos;Hello World!&apos;
    return render_template(&apos;hello.html&apos;, content = content)

if __name__ == &apos;__main__&apos;:
    app.run(&apos;0.0.0.0&apos;)
</code></pre><p>其中：@app.route(‘/‘) 是定义的路由，表示对应网址的根目录。<br>然后在本机输入ECS的ip加上端口5000，应该就能看到hello world了。</p>
<p>3、 </p>
<p>flask的 中文文档资料： <a href="http://dormousehole.readthedocs.io/en/latest/" target="_blank" rel="external">http://dormousehole.readthedocs.io/en/latest/</a></p>
<p> flask的基础视频参考： <a href="http://www.jikexueyuan.com/course/943.html" target="_blank" rel="external">http://www.jikexueyuan.com/course/943.html</a></p>
<p>4、 以itchatmp为例：</p>
<p>首先根据itchatmp的github网址： <a href="https://github.com/littlecodersh/itchatmp" target="_blank" rel="external">https://github.com/littlecodersh/itchatmp</a></p>
<p>下载到本地，然后</p>
<pre><code>pip install itchatmp
</code></pre><p>在这个过程中，很可能安装失败，是因为墙的因素，建议开启翻墙工具</p>
<p>在安装好之后，把对应的示范脚本，根据你的微信订阅号的相关参数进行修改：</p>
<pre><code>import itchatmp

itchatmp.update_config(itchatmp.WechatConfig(
    token=&apos;xxxxxxxxxx&apos;,
    appId = &apos;xxxxxxxxxx&apos;,
    appSecret = &apos;xxxxxxxxxx&apos;))

@itchatmp.msg_register(itchatmp.content.TEXT)
def text_reply(msg):
    return msg[&apos;content&apos;]

itchatmp.run()
</code></pre><p>5、然后到微信订阅号的后台，把url部分，修改成你的ECS的ip。</p>
<p>6、运行一个形如weixinchat.py的脚本，把上述代码复制，并运行。一切顺利的话，你的订阅号就工作了，此时，他的默认功能是，当你输入信息后，订阅号自动回复相同内容。</p>
<p>7、本文案例是在Win10，Win NT 2008下都测试通过，</p>
<p>8、如果你需要一些Linux相关的参考资料，如下：</p>
<ul>
<li><a href="https://shanguangyu.com/articles/wechat-ECS/" target="_blank" rel="external">阿里云ECS搭建微信公众平台</a></li>
<li><a href="http://jayveehe.github.io/2015/01/26/nginx-flask/" target="_blank" rel="external">微信公众平台接入初探</a></li>
<li><a href="http://www.oschina.net/translate/serving-flask-with-nginx-on-ubuntu?cmp" target="_blank" rel="external">在 Ubuntu 上使用 Nginx 部署 Flask 应用</a></li>
<li><a href="http://www.cnblogs.com/weishun/p/weixin-publish-developing.html" target="_blank" rel="external">微信公众平台开发(免费云BAE+高效优雅的Python+网站开放的API)</a></li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/10/20/测试Daocloud/" itemprop="url">
                  测试Daocloud
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2016-10-20T16:59:28+08:00" content="2016-10-20">
              2016-10-20
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="测试Daocloud"><a href="#测试Daocloud" class="headerlink" title="测试Daocloud"></a>测试Daocloud</h1><p>1、话说现在新浪SAE每天开始收10个豆，加上他主推SVN管理，且不支持同时SVN+Git的管理方法。<br>这使得带来一定的不方便，所以决定找一个代替SAE的。正好看到别人的帖子，所以决定测试下Daocloud</p>
<p>2、案例： </p>
<p><a href="http://vansnowpea-van-2048.daoapp.io/" target="_blank" rel="external">http://vansnowpea-van-2048.daoapp.io/</a></p>
<p>3、代码,转自：<a href="https://my.oschina.net/u/923087/blog/286050" target="_blank" rel="external">https://my.oschina.net/u/923087/blog/286050</a>：</p>
<pre><code># -*- coding: utf-8 -*-
&quot;&quot;&quot;
Created on Tue Jul  1 14:15:39 2014

@author: kelvin
&quot;&quot;&quot;

import random

class game2048:
    totalScore = 0
    v = [[2, 8, 8, 2],
         [4, 2, 4, 8],
         [2, 4, 2, 0],
         [4, 2, 4, 0]]
    &apos;&apos;&apos;
    v = [[0, 0, 0, 0],
         [0, 0, 0, 0],
         [0, 0, 0, 0],
         [0, 0, 0, 0]]
    &apos;&apos;&apos;
    def __init__(self):
        for i in range(4):
            self.v[i] = [random.choice([0,0,0,2,2,4]) for x in range(4)]


    def display(self):
        print(&apos;{0:4} {1:4} {2:4} {3:4}&apos;.format(self.v[0][0], self.v[0][1], self.v[0][2], self.v[0][3]))
        print(&apos;{0:4} {1:4} {2:4} {3:4}&apos;.format(self.v[1][0], self.v[1][1], self.v[1][2], self.v[1][3]))
        print(&apos;{0:4} {1:4} {2:4} {3:4}&apos;.format(self.v[2][0], self.v[2][1], self.v[2][2], self.v[2][3]))
        print(&apos;{0:4} {1:4} {2:4} {3:4}&apos;.format(self.v[3][0], self.v[3][1], self.v[3][2], self.v[3][3]))
        print(&apos;得分为:{0:4}&apos;.format(self.totalScore))
        print(&apos;游戏是否结束:{0:4}&apos;.format(self.isOver()))
    #重新排列
    def align(self,vList, direction):
        for i in range(vList.count(0)):
            vList.remove(0)
        zeros = [0 for x in range(4-len(vList))]
        if direction == &apos;left&apos;:
            vList.extend(zeros)
        else:
            vList[:0] = zeros
    #将相同的元素相加，返回新增积分
    def addSame(self,vList, direction):
        increment=0
        if direction == &apos;left&apos;:
            for i in [0,1,2]:
                if vList[i]==vList[i+1] and vList[i+1]!=0:
                    vList[i] *= 2
                    vList[i+1] = 0
                    increment += vList[i]
        else:
            for i in [3,2,1]:
                if vList[i]==vList[i-1] and vList[i-1]!=0:
                    vList[i] *= 2
                    vList[i-1] = 0
                    increment += vList[i]
        return increment
    #处理行和方向,返回新增积分
    def handle(self, vList, direction):
        self.align(vList, direction)
        increment = self.addSame(vList, direction)
        self.align(vList, direction)
        self.totalScore += increment #直接加到总值
        return increment
    #判断游戏是否结束
    def judge(self):

        if self.isOver():
            print(&apos;你输了，游戏结束!&apos;)
            return False
        else:
            if self.totalScore &gt;= 2048:
                print(&apos;你赢了，游戏结束！但是你还可以继续玩。&apos;)
            return True
    #判断游戏是否真正结束
    def isOver(self):
        N = self.calcCharNumber(0)
        if N!=0:
            return False
        else:
            for row in range(4):
                flag = self.isListOver(self.v[row])
                if flag==False:
                    return False    
            for col in range(4):
                # 将矩阵中一列复制到一个列表中然后处理
                vList = [self.v[row][col] for row in range(4)]
                flag = self.isListOver(vList)
                if flag==False:
                    return False
        return True

    #判断一个列表是否还可以合并
    def isListOver(self, vList):
        for i in [0,1,2]:
            if vList[i]==vList[i+1] and vList[i+1]!=0:
                return False
        return True
    def calcCharNumber(self, char):
        n = 0
        for q in self.v:
            n += q.count(char)
        return n
    def addElement(self):
        # 统计空白区域数目 N
        N = self.calcCharNumber(0)
        if N!=0:
            # 按2和4出现的几率为3/1来产生随机数2和4
            num = random.choice([2, 2, 2, 4]) 
            # 产生随机数k，上一步产生的2或4将被填到第k个空白区域
            k = random.randrange(1, N+1)    #k的范围为[1,N]
            n = 0
            for i in range(4):
                for j in range(4):
                    if self.v[i][j] == 0:
                        n += 1
                        if n == k:
                            self.v[i][j] = num
                            return


    def moveLeft(self):
        self.moveHorizontal(&apos;left&apos;)
    def moveRight(self):
        self.moveHorizontal(&apos;right&apos;)
    def moveHorizontal(self, direction):
        for row in range(4):
            self.handle(self.v[row], direction)

    def moveUp(self):
        self.moveVertical(&apos;left&apos;)
    def moveDown(self):
        self.moveVertical(&apos;right&apos;)
    def moveVertical(self, direction):
        for col in range(4):
            # 将矩阵中一列复制到一个列表中然后处理
            vList = [self.v[row][col] for row in range(4)]
            self.handle(vList, direction)
            # 从处理后的列表中的数字覆盖原来矩阵中的值
            for row in range(4):
                self.v[row][col] = vList[row]

    #主要的处理函数
    def operation(self):
        op = input(&apos;operator:&apos;)
        if op in [&apos;a&apos;, &apos;A&apos;]:    # 向左移动
            self.moveLeft()
            self.addElement()
        elif op in [&apos;d&apos;, &apos;D&apos;]:  # 向右移动
            self.moveRight()
            self.addElement()
        elif op in [&apos;w&apos;, &apos;W&apos;]:  # 向上移动
            self.moveUp()
            self.addElement()
        elif op in [&apos;s&apos;, &apos;S&apos;]:  # 向下移动
            self.moveDown()
            self.addElement()
        else:
            print(&apos;错误的输入。请输入 [W, S, A, D] 或者是其小写&apos;)    

#开始
print(&apos;输入：W(上移) S(下移) A(左移) D(右移), press &lt;CR&gt;.&apos;)
g =game2048()
flag = True
while True:
    g.display()
    flag = g.judge()
    g.operation()
    flag = g.judge()
</code></pre>
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/10/20/从零开始 Python 微信公众号开发【转帖】/" itemprop="url">
                  从零开始 Python 微信公众号开发【转帖】
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2016-10-20T14:59:28+08:00" content="2016-10-20">
              2016-10-20
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="从零开始-Python-微信公众号开发【转帖】"><a href="#从零开始-Python-微信公众号开发【转帖】" class="headerlink" title="从零开始 Python 微信公众号开发【转帖】"></a>从零开始 Python 微信公众号开发【转帖】</h1><p>1、虽然在8月底的时候我也有总结一篇基于SAE的订阅号的实现的日志。不过也就是简单实现而已。</p>
<p>2、今天偶然看到小段的总结文档，写的比较详细，忍不住转发下，以下是原文地址：</p>
<p><a href="https://zhuanlan.zhihu.com/p/21354943" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/21354943
</a></p>
<p>3、要强调下，如果使用SAE新浪云，是需要实名认证的。</p>
<p>4、最近也从github或搜索发现了一些不错的微信的sdk，包括但不限于：</p>
<ul>
<li><a href="https://github.com/littlecodersh/ItChat" target="_blank" rel="external">https://github.com/littlecodersh/ItChat</a></li>
<li><a href="http://wechat-python-sdk.com/" target="_blank" rel="external">http://wechat-python-sdk.com/</a></li>
</ul>
<p>5、然后今天发生了喜感的一幕，当和little聊起之前曾给他发过email请教过细节等，他回复说，当时正好有人写了email，才想起做一个这样的sdk。  </p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/10/17/数据分析和一个工具OpenRefine/" itemprop="url">
                  数据分析和一个工具OpenRefine【Python】
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2016-10-17T12:59:28+08:00" content="2016-10-17">
              2016-10-17
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="数据分析和一个工具OpenRefine"><a href="#数据分析和一个工具OpenRefine" class="headerlink" title="数据分析和一个工具OpenRefine"></a>数据分析和一个工具OpenRefine</h1><p>1、在阅读《Python网络数据采集》第七章的时候看到的案例，记录细节分析。</p>
<p><img src="http://ocg7i7pt6.bkt.clouddn.com//blog/20161017/232231872.png" alt=""></p>
<p>2、来简单体会下他的作用，根据书上的例子，我选定了之前程序得到的csv文件，导入后，的界面如下图： </p>
<p><img src="http://ocg7i7pt6.bkt.clouddn.com//blog/20161018/001139678.png" alt=""></p>
<p>可以看到Programming language，的栏目，这里要从77行中，筛选出同时有3种语言技能的，先在上面的下拉三角点开，使用text filter，然后配合RE表达式：  .+,.+,.+  输入到左侧，即可：</p>
<p><img src="http://ocg7i7pt6.bkt.clouddn.com//blog/20161018/001523164.png" alt=""></p>
<p>3、此软件关于正则表达式的使用，可参考此网址：</p>
<p><a href="https://github.com/OpenRefine/OpenRefine/wiki/General-Refine-Expression-Language" target="_blank" rel="external">https://github.com/OpenRefine/OpenRefine/wiki/General-Refine-Expression-Language</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/10/15/维基百科某网页表格的csv保存的分析/" itemprop="url">
                  维基百科某网页表格的csv保存的分析【Python】
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2016-10-15T12:59:28+08:00" content="2016-10-15">
              2016-10-15
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="维基百科某网页表格的csv保存的分析"><a href="#维基百科某网页表格的csv保存的分析" class="headerlink" title="维基百科某网页表格的csv保存的分析"></a>维基百科某网页表格的csv保存的分析</h1><p>1、在阅读《Python网络数据采集》第五章的时候看到的案例，记录细节分析。</p>
<p>目标网址：<br><a href="http://en.wikipedia.org/wiki/Comparison_of_text_editors" target="_blank" rel="external">http://en.wikipedia.org/wiki/Comparison_of_text_editors</a></p>
<p>中的一个表格，</p>
<p>这是结果图：</p>
<p><img src="http://ocg7i7pt6.bkt.clouddn.com/blog/2016-10-15/131101232.png" alt=""></p>
<p>2、代码：</p>
<pre><code>import csv
from urllib.request import urlopen
from bs4 import BeautifulSoup

html = urlopen(&quot;http://en.wikipedia.org/wiki/Comparison_of_text_editors&quot;)
bsObj = BeautifulSoup(html, &quot;html.parser&quot;)
#The main comparison table is currently the first table on the page
table = bsObj.findAll(&quot;table&quot;,{&quot;class&quot;:&quot;wikitable&quot;})[0]
# print(table)
rows = table.findAll(&quot;tr&quot;)
# print(rows)

csvFile = open(&quot;c:\\van\\editors.csv&quot;, &apos;wt&apos;, newline=&apos;&apos;, encoding=&apos;utf-8&apos;)
writer = csv.writer(csvFile)
try:
    for row in rows:
        csvRow = []
        for cell in row.findAll([&apos;td&apos;, &apos;th&apos;]):
            csvRow.append(cell.get_text())
        writer.writerow(csvRow)
finally:
    csvFile.close()
</code></pre><p>3、上述代码的主要目的，是把对应url的表格的文字信息，提取出来，保存到csv。那么他是怎么一步步做到的？</p>
<p>首先，我们来打开url看下网站的目标内容，如下图是一个色彩鲜艳的表格：</p>
<p><img src="http://ocg7i7pt6.bkt.clouddn.com/blog/2016-10-15/115841809.png" alt=""></p>
<p>其次，我们查看下网页源代码，格式清晰的很，并且代码很长一共7910行的html，可这只是维基百科的一页而已！并且从这么多的html代码中快速的提取出我们需要的数据信息，应该怎么做呢？</p>
<p>一般来说，提取数据有2个比较通用的方法，<br>第一、无视他的源代码，我就查看目标内容的路径，可通过浏览器自带的copy xpath配合lxml提取，或者如果你习惯bs4的话，用类似方法。</p>
<p>第二，根据F12找到目标区域，比如一个表格的所在大的路径，然后由大往小的逐步提取。显然本文使用的是这个方法。当鼠标移动到</p>
<pre><code>&lt;table class=&quot;wikitable sortable jquery-tablesorter&quot; style=&quot;text-align: center; font-size: 85%; width: auto; table-layout: fixed;&quot;&gt;
</code></pre><p>这一行代码的时候，整个目标表格的颜色就变了。如下图，</p>
<p><img src="http://ocg7i7pt6.bkt.clouddn.com/blog/2016-10-15/122808452.png" alt=""></p>
<p>4、现在重点分析下这行代码：</p>
<p>table = bsObj.findAll(“table”,{“class”:”wikitable”})[0]</p>
<p>根据第三条中的分析，已经知道把对应表格中所有含有wikitable的class找出来，那么为什么要这么写？<br>此时，对照下本例python代码中，按照class搜索{“class”:”wikitable”}，实际上得到的是搜索class=”wikitable sortable”，如下的html代码（其中一条）：</p>
<pre><code>&lt;table class=&quot;wikitable sortable&quot; style=&quot;text-align: center; font-size: 85%; width: auto; table-layout: fixed;&quot;&gt;
</code></pre><p>也就是说，bs4的findall是找到了类名带有”wikitable”，就自动把”wikitable sortable”也找出来，但对照lxml的xpath来说，如果class=”wikitable”，则搜索结果为空，要写完整的class=“wikitable sortable“，另外要注意这里有一个大坑，因为F12下的class是”wikitable sortable jquery-tablesorter”,和源代码是不对应的，这会导致python里用xpath找不到内容！</p>
<p>那么为何要在代码的后面加上[0] ? 如果只用下面的代码：</p>
<pre><code>bsObj.findAll(&quot;table&quot;,{&quot;class&quot;:&quot;wikitable&quot;})
</code></pre><p>就本例使用bs4分析来说， 其实得到的是bs4.element.ResultSet ，从字面翻译，可理解成bs4的结果集，不过应该是一个列表， 而要提取里面的内容，就加上[0]，此时从bs4的角度来说，得到了一个bs4.element.Tag ， 从列表的内容提取来说，得到了列表里第一个元素的内容。</p>
<p><img src="http://ocg7i7pt6.bkt.clouddn.com/blog/2016-10-15/121538652.png" alt=""></p>
<p>5、接下来分析这一句：</p>
<pre><code>rows = table.findAll(&quot;tr&quot;)
</code></pre><p>这一行是得到表格中所有按行的内容，这包含了表格头的黑色字体。如下图： （顺便请翻到此贴底部的参考资料，学习下tr，th，td的区别。</p>
<p><img src="http://ocg7i7pt6.bkt.clouddn.com/blog/2016-10-15/125711054.png" alt=""></p>
<p>6、接下来，当开始写入csv的时候，是按行写的，包含了表格头的内容，看下代码：</p>
<pre><code>try:
        for row in rows:
            csvRow = []
            for cell in row.findAll([&apos;td&apos;, &apos;th&apos;]):
                csvRow.append(cell.get_text())
            writer.writerow(csvRow)
</code></pre><p>其中 ‘th’ 是表格头，‘td’是表格内容。</p>
<p>7、相对与上述第二种提取方法，这里详细说下第一种提取方法：</p>
<p>检测表格中黑体的表格头（以name为例）和表格内容（以’acme’为例）：</p>
<p>//*[@id=”mw-content-text”]/table[2]/thead/tr/th[1]</p>
<p>//*[@id=”mw-content-text”]/table[2]/tbody/tr[1]/th/a</p>
<p>分别得到他们的xpath地址，但格式并不统一，出现了thead和tbody。</p>
<p>不过好在表格内容的xpath都是有规律的，<br>千万要注意的是：用xpath提取表格的内容要千万小心，这是因为按照上述路径，测试结果，得到的text()返回值为空，</p>
<p>所以要修正下xpath路径，lxml的解析和网页源代码是有出入的，尤其遇到tboday和thead的时候，经过测试，在很多时候，python要把/thead和/tbody才能显示出内容，但这不是绝对的，因为我也遇到保留tbody才能提取成功的案例。</p>
<p>但还有额外的问题，因为这个表格同时有表头和表内容，而这个案例需要同时提取。而表格还有一个captain = “List of text editors”（表格的标题），也就是说，如果我们要通过直接全部提取整个表格的内容，会多出来captain的内容.</p>
<p>而如果把表头和表内容分开提取的话，他们的xpath在去掉/thead和/tbody之后的形式是这样的： 这里由于情况复杂，我先分析表头部分：</p>
<p>A)表头：</p>
<pre><code>string(//*[@id=&quot;mw-content-text&quot;]/table[2]/tr)
</code></pre><p>为何要这么写，而不用直接的text()模式呢？我们来看下</p>
<pre><code>//*[@id=&quot;mw-content-text&quot;]/table[2]/tr//text()
</code></pre><p>如果这么写，得到的结果是带有空格的，如图：</p>
<p><img src="http://ocg7i7pt6.bkt.clouddn.com/blog/2016-10-16/012705629.png" alt=""></p>
<p>可以发现，不仅多出了空格，在Cost(US$)的栏目，还分了3行，而我们需要连续的，<br>综上，我们只有通过string功能来实现把空格去掉，同时把Cost(US$)合并在一起，得到的结果将是这样的：</p>
<pre><code>Name
Creator
First public release
Latest stable version
Programming language
Cost (US$)
Software license
Open source
</code></pre><p>貌似thead部分的提取还比较顺利，可接下来tbody部分呢？ </p>
<p>B)表内容：<br>我们先看开头的2行对应的xapth地址：</p>
<pre><code># tbody
# Acme xpath:  //*[@id=&quot;mw-content-text&quot;]/table[2]/tbody/tr[1]/th/a
# AkelPad xpath: //*[@id=&quot;mw-content-text&quot;]/table[2]/tbody/tr[2]/th/a
</code></pre><p>如果去掉/tbody后， 又要把表格内容全部提取，又要去掉tbody，发现只能这么写：</p>
<pre><code>string(//*[@id=&quot;mw-content-text&quot;]/table[2])
</code></pre><p>可这样是不行的，因为他不仅把表头的内容也算进去了，还把标题captain的内容也一起搞进去了。</p>
<p>此时，又根据表内容的序号格式，尝试这么写：</p>
<pre><code>string(//*[@id=&quot;mw-content-text&quot;]/table[2]//th/a)
</code></pre><p>期望的是，得到统一的表内容，可实际返回的却是：</p>
<pre><code>Programming language
</code></pre><p>这又是什么鬼呢？<br>原来，代码识别的是满足上述格式条件的<strong>第一个</strong>/a 路径下的文字，而在表头里，从Programming language开始，他有a属性。</p>
<p>此时，又发现，既然string返回的是第一个满足条件的，那么刚修正过的表头的string表达式，其实也适合表内容的表达式啊。看来，我们只好先再提高一个层级，用：</p>
<pre><code>string(//*[@id=&quot;mw-content-text&quot;]/table[2])
</code></pre><p>貌似进入了xpath分析的死循环，做下数据清理应该也是一个路子。</p>
<p>8、上面分析了一大堆，结果不满意，那么lxml的有没有类似bs4的findall功能呢？</p>
<pre><code>∙ iterfind() iterates over all Elements that match the path expression
∙ findall() returns a list of matching Elements
∙ find() efficiently returns only the first match
∙ findtext() returns the .text content of the first match
</code></pre><p>不过当用：</p>
<pre><code>table = selector.findall(&apos;.//*[@id=&quot;mw-content-text&quot;]/table[2]/tr&apos;)
</code></pre><p>得到的结果显示的都是element的list，不显示里面的文字，提取的命令貌似在官网上也没找到，并且尝试：print(each.text) 完全空结果，这就有点坑了。</p>
<p>9、从第8点的分析来看，使用bs4在提取表格的时候，优势还是较大的，因为其返回的是列表形式的html内容，使得具体的提取方便，而lxml的返回的是一个看不到内容的<element table="" at="" 0x3933fa8="">，并且官网上的案例似乎也不太明了。以下是尝试的lxml提取脚本：</element></p>
<pre><code>import requests
from lxml import etree    

html = urlopen(&quot;http://en.wikipedia.org/wiki/Comparison_of_text_editors&quot;)Comparison_of_text_editors&quot;)
selector = etree.HTML(html)
#The main comparison table is currently the first table on the page
table = selector.findall(&apos;.//*[@id=&quot;mw-content-text&quot;]/table[2]/tr&apos;)
content = []
for i in range(len(table)):
    text = selector.xpath(&apos;string(//*[@id=&quot;mw-content-text&quot;]/table[2]/tr[%d + 1])&apos;%i)
    print(text)
    print(len(text))
    content.append(text)
print(content)
</code></pre><p>虽然通过lxml.etree 的findall 以及 xpath配合得到了表格里的需要的文字，但是，这样的格式，要再插入csv却非常的麻烦，这是因为我们要按行插入，而返回的列表如下面的格式（下面显示的是列表第一项）：</p>
<pre><code>[&apos;\nName\nCreator\nFirst public release\nLatest stable version\nProgramming language\nCost (US$)\nSoftware license\nOpen source\n&apos;, 
</code></pre><p>如果打印出来则是一行行的文字形式，则形如：</p>
<pre><code>Name
Creator
First public release
Latest stable version
Programming language
Cost (US$)
Software license
Open source


Acme
Rob Pike
1993
Plan 9 and Inferno
C
Free
LPL (OSI approved)
Yes
</code></pre><p>要想得到bs4的那种csv的表格形式 ，还需要额外增加很多的数据清洗代码，看上去简单，实际很复杂，比如当你想通过len()的时候计算第一项有几个单词，按我们需求是从Name到Open source 的8个单词对应csv的8个列，但python是按照字符串去统计的，返回的每个列表元素的长度肯定不同的。而如果想把\n都替换掉，则前后所需单词后紧挨在一起了。总之最后反而得不偿失，尤其是如果希望使用wt的csv写入格式的话。希望以后能找到更好的办法来解决lxml这样的表格提取问题。</p>
<pre><code>In [4]: a
Out[4]: &apos;\nName\nCreator\nFirst public release\nLatest stable version\nProgramming language\nCost (US$)\nSoftware license\nOpen source\n&apos;

In [5]: a.replace(&apos;\n&apos;, &apos;&apos;)
Out[5]: &apos;NameCreatorFirst public releaseLatest stable versionProgramming languageCost (US$)Software licenseOpen source&apos;

In [6]: length = len(a)

In [7]: length
Out[7]: 118
</code></pre><p>10、参考：</p>
<p><a href="http://jingyan.baidu.com/article/636f38bb1eb1aad6b8461088.html" target="_blank" rel="external">http://jingyan.baidu.com/article/636f38bb1eb1aad6b8461088.html</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/10/13/西部世界 1080P高清下载和自动提醒后续新出的/" itemprop="url">
                  西部世界 1080P高清下载和自动提醒后续新出的【Python】
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2016-10-13T20:59:28+08:00" content="2016-10-13">
              2016-10-13
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="西部世界-1080P高清下载和自动提醒后续新出的"><a href="#西部世界-1080P高清下载和自动提醒后续新出的" class="headerlink" title="西部世界 1080P高清下载和自动提醒后续新出的"></a>西部世界 1080P高清下载和自动提醒后续新出的</h1><p><img src="http://ocg7i7pt6.bkt.clouddn.com/blog/2016-10-14/005536645.jpg" alt=""></p>
<p>1、主要思路是，通过高清源头的网站提供的资源，爬取后，通过迅雷实现自动下载，<br>然后后续新出的，比如下周1更新后，脚本会自动捕捉后发邮件通知，并自动下载。</p>
<p>2、代码：</p>
<pre><code># -*- coding: utf-8 -*-
# python 3.5.2
# 测试系统，Win10
# Author:Van
# 实现《西部世界》有更新后自动下载，以及邮件通知
# V1.0
# 欢迎各种改进意见
# 请把对应的帐号密码修改成自己的


# from selenium import webdriver
import requests
from lxml import etree
import time
import os
from win32com.client import Dispatch
import smtplib
from email.mime.text import MIMEText
from email.header import Header
import copy

# hints
print(&apos;请确保电脑安装了迅雷&apos;)
print(&apos;如果你用的是破解版的迅雷，请先开启再运行程序&apos;)
print()
# requests
url = &apos;http://www.btbtdy.com/btdy/dy7280.html&apos;
html = requests.get(url).content.decode(&apos;utf-8&apos;)

# lxml
selector = etree.HTML(html)
real_link = []

# to be easy, try &apos;starts-with&apos; , very useful in this case :)
HDTV = selector.xpath(&apos;//a[starts-with(@title, &quot;HDTV-1080P&quot;)]/text()&apos;)
for each in HDTV:
    print(each)


href = selector.xpath(&apos;//a[starts-with(@title, &quot;HDTV-1080P&quot;)]/@href&apos;)
print()
print(&apos;目前有 %d 集西部世界&apos; %len(href))
print()

for each in href:
    # split to get the right magnet link
    each = &apos;magnet&apos; + each.split(&apos;magnet&apos;)[-1]
    # print(each)
    real_link.append(each)

print(&apos;他们的磁链接是 :\n&apos;, real_link)
# define a temp_link in deepcopy to compare for new series
temp_link = copy.deepcopy(real_link)
print(&apos;temp_link is :&apos;, temp_link)




def addTasktoXunlei(down_url,course_infos):
    flag = False
    o = Dispatch(&quot;ThunderAgent.Agent.1&quot;)
    if down_url:
        course_path = os.getcwd()
        try:
            #AddTask(&quot;下载地址&quot;, &quot;另存文件名&quot;, &quot;保存目录&quot;,&quot;任务注释&quot;,&quot;引用地址&quot;,&quot;开始模式&quot;, &quot;只从原始地址下载&quot;,&quot;从原始地址下载线程数&quot;)
            o.AddTask(down_url, &apos;&apos;, course_path, &quot;&quot;, &quot;&quot;, -1, 0, 5)
            o.CommitTasks()
            flag = True
        except Exception:

            print(Exception.message)
            print(&quot; AddTask is fail!&quot;)
    return flag

def new_href():
    # to judge if there is a new series of WestWorld
    time.sleep(2)
    if len(real_link) &gt; len(temp_link):
        print(&apos;西部世界1080P有更新!&apos;)
        print(&apos;现在一共有 %d 集了。&apos; %len(real_link))
        return True
    else:
        return False

def send_email(htm):
    # send email to notice new WestWorld is coming
    sender = &apos;xxxxxxxx@163.com&apos;
    receiver = &apos;xxxxxxxx@qq.com,xxxxxxxx@163.com&apos;
    subject = &apos;西部世界 1080P有更新！&apos;
    smtpserver = &apos;smtp.163.com&apos;
    username = &apos;xxxxxxxx@163.com&apos;
    password = &apos;xxxxxxxx&apos;
    msg = MIMEText(htm, &apos;html&apos;, &apos;utf-8&apos;)
    msg[&apos;Subject&apos;] = Header(subject, &apos;UTF-8&apos;)
    msg[&apos;From&apos;] = sender
    msg[&apos;To&apos;] = &apos;,&apos;.join(receiver)
    smtp = smtplib.SMTP()
    smtp.connect(smtpserver)
    smtp.login(username, password)
    smtp.sendmail(sender, receiver, msg.as_string())
    smtp.quit()

def new_download():
    # only download the new WestWorld series
    if len(real_link) &gt; len(temp_link):
        # 2个地址数据的差集
        new_link = list(set(real_link).difference(set(temp_link)))
        for i in new_link:
            addTasktoXunlei(i, course_infos=None)



if __name__ == &apos;__main__&apos;:
    # download the exiting series of WestWorld
    # send_email(&apos;最新更新磁链接：&apos;+ str(real_link))
    for i in real_link:
        addTasktoXunlei(i, course_infos=None)

    # to get the later WestWorld for each hour
    while 1:
        if new_href():
            send_email(&apos;所有的下载地址（磁链接）：&apos;+ str(real_link))
            new_download()
            time.sleep(15)
            # wait for an hour
            temp_link = real_link
            print(temp_link)
            print(&apos;神剧很好看吧，亲，耐心等下一集！~！&apos;)
</code></pre><p>3、代码分析，其中用到了deepcopy，这个功能很有用，并配合了2个数组的差集，使得可以规避定时器，而让脚本直接比较temp_link的内容，而扑捉到网站有新的更新了。另外，在地址识别的时候，一开始用.xpath 没显示内容，有点奇怪，后来根据特性，使用了strats_with识别了内容。另外，原始的邮件发送函数，是一个接收人，如果要多发，则receiver的格式为list，并修改 msg[‘To’] = ‘,’.join(receiver)</p>
<p>4、邮件的作用是可以利用微信绑定来推送，相对短信，更觉方便。</p>
<p>5、感谢：<br> @陌 提供了163发送email的代码<br> @何方 提供了高清网站源<br> @其他人，交流了细节</p>
<p>6、可改进点：<br>邮件的地址内容显示的是一个列表，有待改进。</p>
<p>7、github对应仓库:</p>
<p><a href="https://github.com/vansnowpea/WestWorld-auto-download-email-xunlei-" target="_blank" rel="external">https://github.com/vansnowpea/WestWorld-auto-download-email-xunlei-</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/10/11/下载国外最新高清pdf的程序测试/" itemprop="url">
                  国外最新高清pdf寻找以及实现迅雷自动下载【Python】
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2016-10-11T20:59:28+08:00" content="2016-10-11">
              2016-10-11
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="国外最新高清pdf寻找以及实现迅雷自动下载"><a href="#国外最新高清pdf寻找以及实现迅雷自动下载" class="headerlink" title="国外最新高清pdf寻找以及实现迅雷自动下载"></a>国外最新高清pdf寻找以及实现迅雷自动下载</h1><p>1、今天意外发现国外某站，提供非常近期，甚至国内亚马逊还没上市的最新高清pdf，所以测试爬虫，看是否能自动下载。</p>
<p>2、</p>
<p>《OReilly.Introduction.to.Machine.Learning.with.Python.A.Guide.for.Data.Scientists.1449369413》  </p>
<p>一开始人工下载成功， 国内要月底才上线呢。</p>
<p>3、 随后测试程序是否可自动下载，第二本书的下载遇到了问题：总提示服务器维护，但更换了ip也这样的结果，后发现是对应网盘异常了。</p>
<p>4、代码：</p>
<pre><code># -*- coding: utf-8 -*-
# python 3.5.2
# 测试系统，Win10，Firefox V46
# Author:Van
# 实现自动下载高清最新pdf的实现
# V1.0 当前只针对效果还可以的国外zippyshare网盘
# 其他的网盘还没添加进判断语句，先共享如何迅雷下载等
# 如果您有经验优化，改进此脚本，请不吝指教
# QQ群： 206241755
# 简介：因下载最新高清pdf，正好发现www.foxebook.net提供
# 但是很多的广告，特烦人，所以尝试脚本，最后因下载需求，
# 加载了迅雷，这功能的实现小牛，不过也是网络别人共享的。。

from selenium import webdriver
import requests
from lxml import etree
import re
import os
from win32com.client import Dispatch



#test name of book : SciPy and NumPy
# book_name = input(&apos;Please input the book name in English:\n&apos;)
book_name = &apos;Introduction to Machine Learning with Python&apos;
print (&apos;begin to search book(s)...&apos;)
print (&apos;---------------------------------&apos;)
# search link is :http://www.foxebook.nethttp://www.foxebook.net/search/SciPy%20and%20NumPySciPy%20and%20NumPy
PostUrl = &quot;http://www.foxebook.net/search/&quot; + book_name
# print(PostUrl)
# get the content of html
html = requests.get(PostUrl).content

# use etree selector
selector = etree.HTML(html)

# /html/body/div/div/main/div[2]/div[2]/h3/a
# /html/body/div/div/main/div[3]/div[2]/h3/a
# above is two books&apos; xpath, so the right xpath for all book is :
# /html/body/div/div/main//div[2]/h3/a
# it can be confirmed by &apos;xpath checker&apos;
total_books = selector.xpath(&quot;/html/body/div/div/main//div[2]/h3/a/text()&quot;)
# print(&apos;total books from searching are:&apos;, total_books)

num1 = 0
link_address = []
real_address = []
def find_link():
    global num1
    # find the right book, put all links in a list of : link_address

    for i in total_books:
        num1 += 1
        if re.search(book_name,i):

            print(&apos;Congrdulations, we find the book(s):\n&apos;)
            print (&apos;**********************************&apos;)
            print(i)
            print (&apos;**********************************\n&apos;)
            href = &apos;http://www.foxebook.net&apos; + selector.xpath(&apos;//*[@id=&quot;content&quot;]/div/main/div[%d]/div[2]/h3/a/@href&apos;%num1)[0]
            # print(&apos;the book link is :&apos;, href)
            # print(&apos;will downloading...&apos;)
            html_new = requests.get(href).content
            selector_new = etree.HTML(html_new)
            link_new = selector_new.xpath(&apos;//*[@id=&quot;download&quot;]/div[2]/table/tbody/tr[1]/td[2]/a/@href&apos;)[0]
            # split the next link
            link_new = &apos;http:&apos;+link_new.split(&apos;:&apos;)[-1]
            link_address.append(link_new)
    print(&apos;download link is :&apos;, link_address)
    print(&apos;\n\n&apos;)

def real_book_link():
    # print(&apos;link_address is :&apos;, link_address)
    # dynamic on zippyshare
    for j in link_address:
        # 用浏览器实现访问

        driver = webdriver.Firefox()
        driver.maximize_window()
        driver.get(j)


        try:

            # find the download button
            title_list = driver.find_element_by_xpath(&apos;//*[@id=&quot;dlbutton&quot;]&apos;)
            film_link = title_list.get_attribute(&apos;href&apos;)
            real_address.append(film_link)

        except:
            print(&apos;can not download the book&apos;)

    print(&apos;real_book_link:&apos;, real_address)
    return real_address

def addTasktoXunlei(down_url,course_infos):
    flag = False
    o = Dispatch(&quot;ThunderAgent.Agent.1&quot;)
    if down_url:
        course_path = os.getcwd()
        try:
            #AddTask(&quot;下载地址&quot;, &quot;另存文件名&quot;, &quot;保存目录&quot;,&quot;任务注释&quot;,&quot;引用地址&quot;,&quot;开始模式&quot;, &quot;只从原始地址下载&quot;,&quot;从原始地址下载线程数&quot;)
            o.AddTask(down_url, &apos;&apos;, course_path, &quot;&quot;, &quot;&quot;, -1, 0, 5)
            o.CommitTasks()
            flag = True
        except Exception:

            print(Exception.message)
            print(&quot; AddTask is fail!&quot;)
    return flag

if __name__ == &apos;__main__&apos;:
    find_link()
    real_link = real_book_link()
    for i in real_link:
        addTasktoXunlei(i, course_infos=None)
</code></pre><p><img src="http://ocg7i7pt6.bkt.clouddn.com/blog/2016-10-12/022803439.png" alt=""></p>
<p>5、第二天分析：<br>更换下载的书名为：《Introduction to Machine Learning with Python》</p>
<p>得到了2个有效的书籍目录，对比昨天的书籍名，发现提供的下载源是不同的国外网盘，而昨天的那个到今天一直打不开，而这本书的网址很快就打开了，网盘名字为： zippyshare.com </p>
<p>然后研究了下，此foxebook.net站点提供的一些网盘下载使用了多家国外网盘，并且各家的广告显示不尽相同，可靠性更是差别较大。</p>
<p>另外，发现，就SciPy and NumPy一书来说，他最后得到的地址有2个http，这应该是广告模式，而后者的http的内容是我们真实需要的，所以通过冒号：来切分a.split(‘:’)[-1]。</p>
<pre><code>In [10]: a = &apos;http://sh.st/st/7a45e8ed9f73a6a10e9a22b2d8783c44/http://www65.zippyshare.com/v/oFSWQWDk/file.html&apos;

In [11]: a
Out[11]: &apos;http://sh.st/st/7a45e8ed9f73a6a10e9a22b2d8783c44/http://www65.zippyshare.com/v/oFSWQWDk/file.html&apos;

In [12]: a.split(&apos;:&apos;)[-1]
Out[12]: &apos;//www65.zippyshare.com/v/oFSWQWDk/file.html&apos;
</code></pre><p>6、忘记说明下昨天的代码为何要用re.match （或者re.research）, 这是因为网站的关键词搜索引擎所使用的算法，我们是不知道的，但从搜索结果看，某关键词下，可能有不同的书籍，而我们是需要精确搜索，下图中实际出现了16本书，但针对SciPy and NumPy，我们要找的是第三个图对应的。因此，我们可以把显示的书名做一个match对照的循环，来实现精确匹配。而另外一方面，网站提供的书名还可能多了冒号，后面附加书名，这样的也符合我们的要求。后来发现用关键词 if xxx in yyy的方式更简便。</p>
<p><img src="http://ocg7i7pt6.bkt.clouddn.com/blog/2016-10-12/093157994.png" alt=""></p>
<p>7、昨天的代码一开始没有考虑到国外网盘下载异常失败的问题，并且有的搜索结果可能有多个网盘地址，而我只取了默认的第一个，考虑到下载的失败可能性，最好把所有下载地址都获取。所以代码需要修改。<br>由于：SciPy and NumPy 对应的网盘当机，选用：《Introduction to Machine Learning with Python》为例</p>
<p>经过对照，在最后的下载界面，是动态的，因此调用selenium+Firefox组合。最后终于得到了完整pdf队中的链接，但速度明显比较慢了，在本例中，是rar后缀的压缩包格式，里面含有pdf。</p>
<pre><code>download link is : [&apos;http://www78.zippyshare.com/v/hBU7JYZp/file.html&apos;, &apos;http://www65.zippyshare.com/v/oFSWQWDk/file.html&apos;]



content: 
book link: http://www78.zippyshare.com/d/hBU7JYZp/2248094/OReilly.Introduction.to.Machine.Learning.with.Python.A.Guide.for.Data.Scientists.1449369413.rar
content: 
book link: http://www65.zippyshare.com/d/oFSWQWDk/1124867/OReilly.Introduction.to.Machine.Learning.with.Python.1449369413_Early.Release.rar

Process finished with exit code 0
</code></pre><p>8、接下来的一个问题，怎么让程序自动下载这2个链接？群里有人推荐了一些别的软件，但是我想来想去因为以后总要面对下载速度的问题，还是选定了迅雷破解版吧，除非将来有其他更好的方案，好在有人共享了一个方案，还特别简单，不过据说只能支持http格式，BT格式的以后再想办法。</p>
<p>9：补充说明，在正文代码的第2个下载地址，是有问题的，差别在于地址点击后，前者可在浏览器或者迅雷直接下载，而后者浏览器没反映，迅雷里下载的是一个html。尽管2个链接的提取方法完全一样，但一个好使，一个异常，由于是同一本书的前后2个小版本，我也不管他了，但为了验证迅雷是否能同时下载5个（代码里设定同时下载的最大值，也是一般默认值） 我用额外的测试脚本加载了一个新的链接，是证明可同时下载的，如图：</p>
<p><img src="http://ocg7i7pt6.bkt.clouddn.com/blog/2016-10-12/203618609.png" alt=""> </p>
<p>9、参考：</p>
<p><a href="http://neue.v2ex.com/t/275703" target="_blank" rel="external">http://neue.v2ex.com/t/275703</a></p>
<p>10、github对应仓库：</p>
<p><a href="https://github.com/vansnowpea/download-pdf-with-Xunlei" target="_blank" rel="external">https://github.com/vansnowpea/download-pdf-with-Xunlei</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/10/10/用户细分精准营销--聚类/" itemprop="url">
                  用户细分精准营销--聚类　以及　机器学习典型应用【Python】
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2016-10-10T13:59:28+08:00" content="2016-10-10">
              2016-10-10
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="用户细分精准营销–聚类"><a href="#用户细分精准营销–聚类" class="headerlink" title="用户细分精准营销–聚类"></a>用户细分精准营销–聚类</h1><p>1、中国移动的各个手机套餐之间的分类为例：</p>
<p><img src="http://ocg7i7pt6.bkt.clouddn.com/blog/2016-10-10/132655994.png" alt=""></p>
<ul>
<li>动感地带： 目标群：在校学生，短信需求旺盛，短信费用低。</li>
</ul>
<ul>
<li>神州大众卡：目标群：更多客户，四川移动推出：最让消费者动心的是没有月租费的日子里接听网内来电20元包完。</li>
</ul>
<ul>
<li>全球通： 目标群：全球飞的高端上午人群，不是很在乎话费，却关心品牌的增值服务，如vip候机厅等。</li>
</ul>
<ul>
<li>神州行： 目标群：普通务工人员，电话需求较多，通话费用低。</li>
</ul>
<p>那么在那个还没有智能机的年代，他们是怎么想到这些分类方法或者区分客户群的呢？回头想来客户群的区分有相当比例是由客户自己定义使用哪个套餐的。当然，以现在的技术用计算机AI可以相对容易的来进行聚类区分了。</p>
<hr>
<p>反垃圾邮件：　    朴素贝叶斯 </p>
<p>信贷风险控制：　  决策树</p>
<p>互联网广告：　    ctr预估-线性逻辑回归 </p>
<p>推荐系统：　     协同过滤 </p>
<p>自然语言处理：　  情感分析，实体识别 </p>
<p>图像识别：　     深度学习 </p>
<p>其他 </p>
<hr>
<p><img src="http://ocg7i7pt6.bkt.clouddn.com/blog/2016-10-10/135019916.png" alt=""></p>
<h1 id="一些常用的算法分类表："><a href="#一些常用的算法分类表：" class="headerlink" title="一些常用的算法分类表："></a>一些常用的算法分类表：</h1><p><img src="http://ocg7i7pt6.bkt.clouddn.com/blog/2016-10-10/143149096.png" alt=""></p>
<ul>
<li>C4.5 是决策树算法，可以解决分类和回归问题，又属于有监督算法</li>
</ul>
<ul>
<li>K-Means 是聚类算法，无监督。</li>
<li>SVM： 基于统计学，有完整理论。</li>
<li>PageRank：谷歌的算法 </li>
</ul>
<h1 id="机器学习的框架"><a href="#机器学习的框架" class="headerlink" title="机器学习的框架"></a>机器学习的框架</h1><h2 id="训练模型："><a href="#训练模型：" class="headerlink" title="训练模型："></a>训练模型：</h2><p>1、定义模型</p>
<p>2、定义损失函数</p>
<p>3、优化算法</p>
<h2 id="模型评估："><a href="#模型评估：" class="headerlink" title="模型评估："></a>模型评估：</h2><p>1、交易验证</p>
<p>2、效果评估</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/10/08/数据分析的参考书集锦/" itemprop="url">
                  数据分析的参考书集锦【Python】+【R】
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2016-10-08T20:59:28+08:00" content="2016-10-08">
              2016-10-08
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="数据分析的参考书集锦，先保存过来再说。"><a href="#数据分析的参考书集锦，先保存过来再说。" class="headerlink" title="数据分析的参考书集锦，先保存过来再说。"></a>数据分析的参考书集锦，先保存过来再说。</h1><p>原文： <a href="http://bbs.pinggu.org/thread-3116701-1-1.html" target="_blank" rel="external">http://bbs.pinggu.org/thread-3116701-1-1.html</a></p>
<h1 id="入门读物："><a href="#入门读物：" class="headerlink" title="入门读物："></a>入门读物：</h1><p>深入浅出数据分析 这书挺简单的，基本的内容都涉及了，说得也比较清楚，最后谈到了 R 是大加分。难易程度：非常易。<br>啤酒与尿布 通过案例来说事情，而且是最经典的例子。难易程度：非常易。<br>数据之美 一本介绍性的书籍，每章都解决一个具体的问题，甚至还有代码，对理解数据分析的应用领域和做法非常有帮助。难易程度：易。<br>数学之美 这本书非常棒啦，入门读起来很不错！<br>      下载地址：深入浅出数据分析、啤酒与尿布、数据之美、数学之美。</p>
<p>数据分析：</p>
<p>SciPy and NumPy 这本书可以归类为数据分析书吧，因为 numpy 和 scipy 真的是非常强大啊。<br>Python for Data Analysis 作者是 Pandas 包的作者，看过他在 Scipy 会议上的演讲，实例非常强！<br>Bad Data Handbook 很好玩的书，作者的角度很不同。<br>       下载地址：SciPy and NumPy、Python for Data Analysis、Bad Data Handbook</p>
<h1 id="适合入门的教程："><a href="#适合入门的教程：" class="headerlink" title="适合入门的教程："></a>适合入门的教程：</h1><p>集体智慧编程 学习数据分析、数据挖掘、机器学习人员应该仔细阅读的第一本书。作者通过实际例子介绍了机器学习和数据挖掘中的算法，浅显易懂，还有可执行的 Python 代码。难易程度：中。<br>Machine Learning in Action 用人话把复杂难懂的机器学习算法解释清楚了，其中有零星的数学公式，但是是以解释清楚为目的的。而且有 Python 代码，大赞！目前中科院的王斌老师（微博： 王斌_ICTIR）已经翻译这本书了 机器学习实战 。这本书本身质量就很高，王老师的翻译质量也很高。难易程度：中。我带的研究生入门必看数目之一！<br>Building Machine Learning Systems with Python 虽然是英文的，但是由于写得很简单，比较理解，又有 Python 代码跟着，辅助理解。<br>数据挖掘导论 最近几年数据挖掘教材中比较好的一本书，被美国诸多大学的数据挖掘课作为教材，没有推荐 Jiawei Han 老师的那本书，因为个人觉得那本书对于初学者来说不太容易读懂。难易程度：中上。<br>Machine Learning for Hackers 也是通过实例讲解机器学习算法，用 R 实现的，可以一边学习机器学习一边学习 R。<br>       下载地址：集体智慧编程+源代码、Machine Learning in Action、Building Machine Learning Systems with Python、                            数据挖掘导论、Machine Learning for Hackers</p>
<h1 id="稍微专业些的："><a href="#稍微专业些的：" class="headerlink" title="稍微专业些的："></a>稍微专业些的：</h1><p>Introduction to Semi-Supervised Learning 半监督学习必读必看的书。<br>Learning to Rank for Information Retrieval 微软亚院刘铁岩老师关于 LTR 的著作，啥都不说了，推荐！<br>Learning to Rank for Information Retrieval and Natural Language Processing 李航老师关于 LTR 的书，也是当时他在微软亚院时候的书，可见微软亚院对 LTR 的研究之深，贡献之大。<br>推荐系统实践 这本书不用说了，研究推荐系统必须要读的书，而且是第一本要读的书。<br>Graphical Models, Exponential Families, and Variational Inference 这个是 Jordan 老爷子和他的得意门徒 Martin J Wainwright 在 Foundation of Machine Learning Research 上的创刊号，可以免费下载，比较难懂，但是一旦读通了，graphical model 的相关内容就可以踏平了。<br>Natural Language Processing with Python NLP 经典，其实主要是讲 NLTK 这个包，但是啊，NLTK 这个包几乎涵盖了 NLP 的很多内容了啊！<br>   下载地址：Introduction to Semi-Supervised Learning、Learning to Rank for Information Retrieval、                           Learning to Rank for Information Retrieval and Natural Language Proces、推荐系统实践</p>
<p>机器学习教材：<br>The Elements of Statistical Learning 这本书有对应的中文版：统计学习基础 。书中配有 R 包，非常赞！可以参照着代码学习算法。<br>统计学习方法 李航老师的扛鼎之作，强烈推荐。难易程度：难。<br>Machine Learning 去年出版的新书，作者 Kevin Murrphy 教授是机器学习领域中年少有为的代表。这书是他的集大成之作，写完之后，就去 Google 了，产学研结合，没有比这个更好的了。<br>Machine Learning 这书和上面的书不是一本！这书叫：Machine Learning: An Algorithmic Perspective 之前做过我带的研究生教材，由于配有代码，所以理解起来比较容易。<br>Pattern Recognition And Machine Learning 经典中的经典。<br>Bayesian Reasoning and Machine Learning 看名字就知道了，彻彻底底的 Bayesian 学派的书，里面的内容非常多，有一张图将机器学习中设计算法的关系总结了一下，很棒。<br>Probabilistic Graphical Models 鸿篇巨制，这书谁要是读完了告诉我一声。<br>Convex Optimization 凸优化中最好的教材，没有之一了。课程也非常棒，Stephen 老师拿着纸一步一步推到，图一点一点画，太棒了。<br>下载地址：The Elements of Statistical Learning、统计学习方法、Machine Learning: An Algorithmic Perspective、<br>                            Pattern Recognition and Machine Learning+答案、Bayesian Reasoning and Machine Learning、                                                   Probabilistic Graphical Models、Convexity and Optimization</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel  sidebar-panel-active ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="Van" />
          <p class="site-author-name" itemprop="name">Van</p>
          <p class="site-description motion-element" itemprop="description"></p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">63</span>
              <span class="site-state-item-name">posts</span>
            </a>
          </div>

          

          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2016</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Van</span>
</div>

<div class="powered-by">
  Powered by <a class="theme-link" href="https://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  



  




  
  

  

  

  

</body>
</html>
